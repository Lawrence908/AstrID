## **ASTR-71: Cloud Storage Integration (P2) - Infrastructure**

### **Context & Current State**
The development environment is set up (ASTR-69), but we need cloud storage infrastructure to handle large astronomical datasets, model artifacts, and processed data. This ticket implements the storage layer using Cloudflare R2 (S3-compatible) with proper content addressing and versioning.

### **Technical Requirements**

**Dependencies**: ASTR-69 (Development Environment Setup) -  Complete
**Domain**: Infrastructure
**Estimated Time**: 2 days

### **Implementation Tasks**

1. **Configure Cloudflare R2 (S3-compatible) Storage**
   - Create `src/infrastructure/storage/r2_client.py`
   - Implement `R2StorageClient` with methods:
     - `upload_file(bucket: str, key: str, data: bytes, content_type: str) -> str`
     - `download_file(bucket: str, key: str) -> bytes`
     - `delete_file(bucket: str, key: str) -> bool`
     - `list_files(bucket: str, prefix: str) -> list[str]`
     - `get_file_metadata(bucket: str, key: str) -> dict`
   - Add authentication with R2 credentials
   - Implement proper error handling and retry logic

2. **Implement Storage Client with Content Addressing**
   - Create `src/infrastructure/storage/content_addressed_storage.py`
   - Implement `ContentAddressedStorage` with methods:
     - `store_data(data: bytes, content_type: str) -> str` (returns content hash)
     - `retrieve_data(content_hash: str) -> bytes`
     - `store_file(file_path: Path, content_type: str) -> str`
     - `get_content_hash(data: bytes) -> str`
   - Use SHA-256 for content addressing
   - Implement deduplication logic
   - Add content verification on retrieval

3. **Set up DVC for Dataset Versioning**
   - Create `src/infrastructure/storage/dvc_client.py`
   - Implement `DVCClient` with methods:
     - `add_dataset(dataset_path: str, remote: str) -> str`
     - `version_dataset(dataset_path: str, message: str) -> str`
     - `pull_dataset(dataset_id: str, target_path: str) -> None`
     - `push_dataset(dataset_id: str) -> None`
     - `list_versions(dataset_path: str) -> list[dict]`
   - Configure DVC remote to use R2 storage
   - Add dataset metadata tracking
   - Implement dataset lineage tracking

4. **Configure MLflow Artifact Storage**
   - Create `src/infrastructure/storage/mlflow_storage.py`
   - Implement `MLflowStorageConfig` with:
     - R2 backend configuration
     - Artifact path structure
     - Access control settings
   - Add `MLflowArtifactStorage` class:
     - `store_model_artifact(model_path: str, run_id: str) -> str`
     - `retrieve_model_artifact(artifact_uri: str) -> bytes`
     - `list_model_artifacts(experiment_id: str) -> list[dict]`
   - Configure MLflow to use R2 as artifact store

### **Integration Points**

- **Domain Models**: Store file references in observation and detection models
- **Processing Pipeline**: Store intermediate and final processing results
- **ML Models**: Store trained models and artifacts
- **Datasets**: Version control for training and validation datasets
- **Configuration**: Environment-based storage configuration

### **Storage Buckets Structure**
```
astrid-storage/
├── raw-observations/          # Original FITS files
├── processed-observations/    # Calibrated and preprocessed data
├── difference-images/         # Image differencing results
├── detections/               # Detection results and metadata
├── models/                   # ML model artifacts
├── datasets/                 # Training and validation datasets
├── artifacts/                # MLflow artifacts
└── temp/                     # Temporary processing files
```

### **Configuration Requirements**
```python
@dataclass
class StorageConfig:
    r2_account_id: str
    r2_access_key_id: str
    r2_secret_access_key: str
    r2_bucket_name: str
    r2_region: str
    dvc_remote_url: str
    mlflow_artifact_root: str
    content_addressing_enabled: bool
    deduplication_enabled: bool
```

### **API Endpoints to Add**
```python
POST /storage/upload
GET /storage/download/{content_hash}
DELETE /storage/{content_hash}
GET /storage/metadata/{content_hash}
POST /storage/datasets/{dataset_id}/version
GET /storage/datasets/{dataset_id}/versions
```

### **Error Handling**
- Network timeout and retry logic
- Storage quota and rate limiting
- Content verification and corruption detection
- Graceful degradation when storage is unavailable
- Comprehensive logging for storage operations

### **Testing Strategy**
- Unit tests for all storage clients
- Integration tests with R2 storage
- Performance tests for large file uploads/downloads
- Content addressing verification tests
- DVC versioning workflow tests

### **Security Considerations**
- Encrypted data at rest (R2 default)
- Secure credential management
- Access control and permissions
- Audit logging for storage operations
- Data retention policies
