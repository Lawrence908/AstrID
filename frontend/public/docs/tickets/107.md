## **ASTR-107: Task Scheduler for Automated Inference Runs (P2) - Workflow & Orchestration**

### **Context & Current State**
Workflow orchestration is complete (ASTR-91 ), providing Prefect-based workflow management with existing cron scheduling configuration. This ticket focuses on setting up automated task scheduling specifically for inference runs, building on the existing Prefect infrastructure while ensuring proper log capture and monitoring for scheduled executions.

### **Technical Requirements**

**Dependencies**: ASTR-91 (Workflow Orchestration) -  Complete
**Domain**: Workflow & Orchestration (Task Scheduling)
**Estimated Time**: 2 days

### **Implementation Tasks**

1. **Review Existing Scheduling Configuration**
   - Audit current Docker setup and Prefect scheduling in `docker-compose.yaml`
   - Review existing Prefect flows in `src/adapters/scheduler/flows/`
   - Examine current cron configurations in `src/adapters/scheduler/config.py`
   - Document current scheduling capabilities and limitations
   - Identify integration points for inference-specific scheduling

2. **Configure Inference-Specific Scheduling**
   - Create dedicated inference scheduling configuration:
     - Add inference-specific cron schedules to `PrefectConfig`
     - Configure different schedules for different inference types:
       - Real-time inference (high frequency)
       - Batch inference (daily/nightly)
       - Model validation inference (weekly)
   - Update `src/adapters/scheduler/config.py` with inference scheduling options
   - Add environment variable support for schedule customization

3. **Set Up Automated Inference Execution**
   - Create inference-specific Prefect flow in `src/adapters/scheduler/flows/inference.py`:
     - `scheduled_inference_flow()` - Main scheduled inference execution
     - `batch_inference_flow()` - Batch processing of multiple observations
     - `model_validation_inference_flow()` - Validation inference runs
   - Integrate with existing U-Net model from ASTR-80
   - Connect to detection pipeline from ASTR-81
   - Add proper error handling and retry logic

4. **Implement Log Capture and Monitoring**
   - Set up comprehensive logging for scheduled runs:
     - Structured logging with timestamps and run IDs
     - Log rotation and archival policies
     - Error logging with stack traces
     - Performance metrics logging
   - Create monitoring dashboard integration:
     - Real-time status of scheduled runs
     - Success/failure rates and trends
     - Performance metrics and resource usage
     - Alert system for failed runs

5. **Configure Docker Integration**
   - Update `docker-compose.yaml` with inference scheduling:
     - Add inference-specific environment variables
     - Configure log volume mounts for persistence
     - Set up health checks for scheduled services
   - Create inference-specific Dockerfile if needed:
     - `Dockerfile.inference` for inference-only containers
     - Optimized for inference workloads
     - GPU support for model inference

6. **Test and Validate Automated Execution**
   - Create test suite for scheduled inference:
     - Unit tests for scheduling configuration
     - Integration tests for Prefect flow execution
     - End-to-end tests for complete inference pipeline
   - Validate log capture and monitoring:
     - Test log generation and storage
     - Verify monitoring dashboard updates
     - Test alert system functionality
   - Performance testing:
     - Load testing with multiple scheduled runs
     - Resource usage monitoring
     - Scalability validation

### **Integration Points**

- **Prefect Integration**: Leverage existing ASTR-91 workflow orchestration
- **Model Integration**: Use U-Net model from ASTR-80
- **Detection Pipeline**: Connect to ASTR-81 detection services
- **MLflow Integration**: Log inference runs and metrics via ASTR-88
- **Storage**: Use existing cloud storage for logs and artifacts
- **Monitoring**: Integrate with existing monitoring infrastructure

### **Scheduling Configuration**
```python
@dataclass
class InferenceSchedulingConfig:
    # Real-time inference (every 15 minutes)
    realtime_schedule: str = "*/15 * * * *"
    realtime_enabled: bool = True
    realtime_max_observations: int = 10
    
    # Batch inference (daily at 2 AM)
    batch_schedule: str = "0 2 * * *"
    batch_enabled: bool = True
    batch_max_observations: int = 100
    
    # Model validation (weekly on Sunday at 4 AM)
    validation_schedule: str = "0 4 * * 0"
    validation_enabled: bool = True
    validation_dataset_size: int = 50
    
    # Logging configuration
    log_retention_days: int = 30
    log_level: str = "INFO"
    log_rotation_size_mb: int = 100
    
    # Monitoring configuration
    monitoring_enabled: bool = True
    alert_on_failure: bool = True
    performance_tracking: bool = True
```

### **Prefect Flow Implementation**
```python
from prefect import flow, task
from prefect.schedules import CronSchedule
from src.domains.ml.services.detection import DetectionService
from src.domains.observations.services import ObservationService

@flow(name="scheduled-inference")
def scheduled_inference_flow(
    max_observations: int = 50,
    model_version: str = "latest",
    confidence_threshold: float = 0.8
) -> dict[str, Any]:
    """Scheduled inference flow for automated anomaly detection."""
    
    # Get observations ready for inference
    observations = await get_observations_for_inference(max_observations)
    
    # Run inference pipeline
    results = await run_inference_batch(
        observations, 
        model_version, 
        confidence_threshold
    )
    
    # Log results and metrics
    await log_inference_results(results)
    
    return {
        "processed_count": len(observations),
        "detections_found": results["detection_count"],
        "processing_time": results["processing_time"],
        "success_rate": results["success_rate"]
    }

@task
async def get_observations_for_inference(max_observations: int) -> list[dict]:
    """Get observations ready for inference processing."""
    observation_service = ObservationService()
    return await observation_service.get_observations_for_processing(
        limit=max_observations,
        status="preprocessed"
    )

@task
async def run_inference_batch(
    observations: list[dict], 
    model_version: str, 
    confidence_threshold: float
) -> dict[str, Any]:
    """Run inference on a batch of observations."""
    detection_service = DetectionService()
    
    results = {
        "detection_count": 0,
        "processing_time": 0,
        "success_rate": 0,
        "errors": []
    }
    
    start_time = time.time()
    successful_runs = 0
    
    for observation in observations:
        try:
            detection_result = await detection_service.process_observation(
                observation_id=observation["id"],
                model_version=model_version,
                confidence_threshold=confidence_threshold
            )
            
            if detection_result["detections"]:
                results["detection_count"] += len(detection_result["detections"])
            
            successful_runs += 1
            
        except Exception as e:
            results["errors"].append({
                "observation_id": observation["id"],
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            })
    
    results["processing_time"] = time.time() - start_time
    results["success_rate"] = successful_runs / len(observations) if observations else 0
    
    return results
```

### **Docker Configuration Updates**
```yaml
# Add to docker-compose.yaml
services:
  # Inference scheduler service
  inference-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.inference
    container_name: astrid-inference-scheduler-dev
    env_file:
      - .env
    environment:
      - PREFECT_API_URL=http://prefect:4200/api
      - REDIS_URL=redis://:${REDIS_PASSWORD:-foo}@redis:6379/0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - AWS_ACCESS_KEY_ID=${CLOUDFLARE_R2_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${CLOUDFLARE_R2_SECRET_ACCESS_KEY}
      - AWS_S3_ENDPOINT_URL=${CLOUDFLARE_R2_ENDPOINT_URL}
      - PYTHONPATH=/app/src
      - DEBUG=true
      # Inference-specific configuration
      - ASTRID_INFERENCE_SCHEDULE_REALTIME="*/15 * * * *"
      - ASTRID_INFERENCE_SCHEDULE_BATCH="0 2 * * *"
      - ASTRID_INFERENCE_SCHEDULE_VALIDATION="0 4 * * 0"
      - ASTRID_INFERENCE_LOG_LEVEL=INFO
      - ASTRID_INFERENCE_MAX_OBSERVATIONS=50
    volumes:
      - .:/app
      - ./src:/app/src
      - ./logs:/app/logs
      - ./certs:/app/certs
    networks:
      - astrid-network
    depends_on:
      prefect:
        condition: service_healthy
      redis:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    command: >
      bash -c "
        python -m src.adapters.scheduler.deploy &&
        python -m src.adapters.scheduler.inference_scheduler
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health/inference')"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
```

### **Logging and Monitoring Setup**
```python
class InferenceLogger:
    """Structured logging for inference runs."""
    
    def __init__(self, log_level: str = "INFO"):
        self.logger = configure_domain_logger("inference.scheduler")
        self.logger.setLevel(getattr(logging, log_level.upper()))
    
    def log_inference_start(self, run_id: str, config: dict) -> None:
        """Log inference run start."""
        self.logger.info(
            "Inference run started",
            extra={
                "run_id": run_id,
                "config": config,
                "timestamp": datetime.utcnow().isoformat(),
                "event_type": "inference_start"
            }
        )
    
    def log_inference_complete(self, run_id: str, results: dict) -> None:
        """Log inference run completion."""
        self.logger.info(
            "Inference run completed",
            extra={
                "run_id": run_id,
                "results": results,
                "timestamp": datetime.utcnow().isoformat(),
                "event_type": "inference_complete"
            }
        )
    
    def log_inference_error(self, run_id: str, error: Exception) -> None:
        """Log inference run error."""
        self.logger.error(
            "Inference run failed",
            extra={
                "run_id": run_id,
                "error": str(error),
                "timestamp": datetime.utcnow().isoformat(),
                "event_type": "inference_error"
            },
            exc_info=True
        )

class InferenceMonitor:
    """Monitoring and alerting for inference runs."""
    
    def __init__(self):
        self.metrics = {}
        self.alerts = []
    
    def track_metrics(self, run_id: str, metrics: dict) -> None:
        """Track inference run metrics."""
        self.metrics[run_id] = {
            **metrics,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def check_health(self) -> dict[str, Any]:
        """Check overall inference system health."""
        recent_runs = self.get_recent_runs(hours=24)
        
        if not recent_runs:
            return {"status": "warning", "message": "No recent inference runs"}
        
        success_rate = sum(1 for run in recent_runs if run["success"]) / len(recent_runs)
        
        if success_rate < 0.9:
            return {"status": "error", "message": f"Low success rate: {success_rate:.2%}"}
        
        return {"status": "healthy", "success_rate": success_rate}
    
    def send_alert(self, message: str, severity: str = "error") -> None:
        """Send alert for inference issues."""
        alert = {
            "message": message,
            "severity": severity,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.alerts.append(alert)
        # TODO: Integrate with notification system
```

### **API Endpoints for Monitoring**
```python
# Add to existing API
@router.get("/scheduler/inference/status")
async def get_inference_scheduler_status():
    """Get current status of inference scheduler."""
    return {
        "scheduler_running": True,
        "last_run": "2025-01-15T02:00:00Z",
        "next_run": "2025-01-15T02:15:00Z",
        "total_runs_today": 96,
        "success_rate": 0.95
    }

@router.get("/scheduler/inference/logs")
async def get_inference_logs(
    hours: int = 24,
    level: str = "INFO"
):
    """Get inference scheduler logs."""
    # Return structured logs for monitoring
    pass

@router.post("/scheduler/inference/trigger")
async def trigger_inference_run(
    run_type: str = "batch",
    max_observations: int = 50
):
    """Manually trigger inference run."""
    # Trigger immediate inference run
    pass
```

### **Error Handling and Recovery**
- Automatic retry logic for failed inference runs
- Graceful handling of model loading failures
- Database connection error recovery
- Resource exhaustion detection and mitigation
- Comprehensive error logging and alerting

### **Testing Strategy**
- Unit tests for scheduling configuration
- Integration tests for Prefect flow execution
- End-to-end tests for complete inference pipeline
- Load testing with multiple concurrent runs
- Log capture and monitoring validation tests

### **Performance Considerations**
- Resource monitoring and optimization
- Concurrent execution limits
- Memory management for large inference batches
- GPU utilization optimization
- Log rotation and storage management

### **Expected Deliverables**
1. **Updated `src/adapters/scheduler/config.py`** - Inference scheduling configuration
2. **New `src/adapters/scheduler/flows/inference.py`** - Inference-specific Prefect flows
3. **Updated `docker-compose.yaml`** - Inference scheduler service
4. **New `Dockerfile.inference`** - Inference-optimized container
5. **Monitoring integration** - Log capture and alerting system
6. **API endpoints** - Monitoring and manual trigger endpoints
7. **Test suite** - Comprehensive testing for scheduled inference

### **Success Criteria**
- Inference runs execute automatically on configured schedules
- All logs are properly captured and stored
- Monitoring dashboard shows real-time status
- Error handling and recovery work correctly
- Performance meets requirements for production use
- Integration with existing AstrID infrastructure is seamless
