# AstrID Midterm Notes: Pipeline Analysis and Next Steps

**Purpose**: Reference document for continuing the project and building the midterm presentation.  
**Based on**: Analysis of [DATA_PIPELINE.md](DATA_PIPELINE.md) vs research sources ([Finding_Supernovae_with_Deep_Learning_Pipelines.txt](../Finding_Supernovae_with_Deep_Learning_Pipelines.txt), [SUPERNOVA_DETECTION_PIPELINE_GUIDE.md](SUPERNOVA_DETECTION_PIPELINE_GUIDE.md)), script verification, and output results assessment.  
**Date**: February 2026  
**Presentation**: ~20 min, 25+ slides; audience: faculty advisors (Prof. Meneses, Prof. Arkos) and peers. Slide deck: [MIDTERM_SLIDES.md](MIDTERM_SLIDES.md), generated by [scripts/generate_slides_pptx.py](../../scripts/generate_slides_pptx.py).

---

## 1. Pipeline Verification Summary

The three core scripts have been verified against the documented pipeline and research methodology.

### Scripts Verified

| Script | Status | Notes |
|--------|--------|--------|
| **generate_difference_images.py** | Correct | Delegates to `SNDifferencingPipeline`; enforces same-mission + same-filter via `find_matching_filter_pair()`; outputs diff/sig/mask FITS with metadata; GC for large FITS. Only `SN_COORDINATES` has 2014J—others get no SN mask position (pipeline handles gracefully). |
| **run_pipeline_from_config.py** | Correct | Orchestrates all 5 stages via subprocess; loads YAML via `PipelineConfig.from_yaml()`; supports `--dry-run`, `--resume`, `--stage`; generates pipeline report; aborts on stage failure. |
| **run_pipeline_per_sn.py** | Correct | Per-SN flow: query → filter → download → organize → differencing; low memory (one SN at a time); async MAST via `query_sn_fits_from_catalog`; in-memory same-mission filter; checkpoint/resume; good for incremental runs. |

### Methodology Alignment

- **Same-mission requirement**: Enforced in filter stage and in differencing (filter matching).
- **WCS alignment**: Implemented via `reproject.reproject_interp()` in [src/domains/differencing/pipeline.py](../../src/domains/differencing/pipeline.py). The script that runs alignment is [scripts/generate_difference_images.py](../../scripts/generate_difference_images.py) (it calls `SNDifferencingPipeline.process()`, which does the alignment). To change or debug alignment, edit the pipeline’s `process()` method in `src/domains/differencing/pipeline.py`.
- **PSF matching**: Gaussian convolution to match broader PSF (simplified ZOGY-style).
- **Significance maps**: `difference / combined_noise` for consistent thresholding.
- **5σ detection**: DAOStarFinder on significance map per research recommendation.
- **Differencing pipeline (9 stages)**: (1) FITS loading (robust HDU handling), (2) WCS alignment (sub-pixel reprojection), (3) background estimation & subtraction, (4) PSF estimation (FWHM from bright stars), (5) PSF matching (Gaussian convolution), (6) flux normalization (robust median), (7) difference image, (8) significance map (SNR per pixel), (9) source detection (DAOStarFinder, 5σ). Background subtraction uses local median (e.g. 50–64 px boxes); SWIFT UVOT PSF typically 3.5–4.0 px FWHM.

**Conclusion**: Scripts are set up correctly; results are methodologically sound for the current phase (data acquisition through differencing).

---

## 2. Research Source Alignment

### Where Implementation Matches the Literature

- **Data acquisition (Phase 1)**: Catalog (6,542 SNe), MAST queries, temporal coverage (pre/post discovery), spatial cone search—all aligned with the guide.
- **Calibration / alignment (Phase 2)**: WCS reprojection for sub-pixel alignment; research states “even sub-pixel misalignment creates massive residuals”—implementation addresses this.
- **Differencing (Phase 3)**: PSF matching (Gaussian approximation), flux normalization, significance map with uniform noise properties (“whitened noise” in Deep Learning source).
- **Source extraction (Phase 4)**: DAOStarFinder, 5σ threshold, shape filters (sharplo/sharphi) to reject cosmic rays and extended sources.
- **Same-mission and filter matching**: Explicitly required in both sources; pipeline enforces both.

### Gaps (Not Yet Implemented)

| Pipeline phase | Research source | Current status |
|----------------|-----------------|----------------|
| **ZOGY algorithm** | Phase 3: “optimal” subtraction in Fourier space | Simplified Gaussian PSF matching; no full ZOGY. |
| **Image triplets** | Phase 4: 63×63 (sci, ref, diff) cutouts | `create_training_triplets.py` exists; only ~57 samples generated. |
| **CNN classifier** | Phase 5: Braai-style CNN on triplets | Not implemented. |
| **Training strategy** | Phase 6: domain shift, cross-survey validation | Config supports it; no training runs yet. |
| **Deployment / inference** | Phase 7: Edge TPU, real-time | Not implemented. |
| **Evaluation** | Phase 8: AUCPR, precision-recall, class imbalance | Not implemented. |

### Key Insight

The pipeline covers **Phases 1–3** of the guide (acquisition → alignment → differencing) and **partial Phase 4** (source detection + triplet generation at small scale). **Phases 5–8** (CNN, training, deployment, evaluation) are the remaining work for the second half of the project.

---

## 3. Current Results

### Key Stats (from slides / pipeline)

- **Survey scale**: Rubin/LSST ~10 million alerts per night — automation is a necessity, not a convenience.
- **Supernovae**: Type Ia are "standard candles" for cosmology; ~1 SN per century per galaxy; detection requires comparing images across epochs.
- **Bogus problem**: At 5σ, 73–629 candidates per image; only ~1 is real. A model that always predicts "bogus" gets 99.99% accuracy but is scientifically useless → need ML and metrics like AUCPR, not accuracy.
- **FITS discovery**: Supporting compressed FITS (.fits.gz) gave a **296% increase** in training pairs (56 → 222).
- **Pilot study**: 19 SNe → only 8 (42%) had usable same-mission pairs; 11 had cross-mission data that produced unusable artifacts.
- **Same-mission impact**: Filter-at-query (vs download-then-filter) → 60–80% download reduction; success rate for complete pairs improved **25% → 75–100%**.
- **Production dataset**: 222 pairs, 2,282 FITS; **99.1% download success rate** for SNe with same-mission observations; **15 YAML configs** for reproducible runs.
- **WCS alignment**: Processed pairs achieve **93.6%–100% overlap**; even 1-pixel misalignment creates large dipole residuals.
- **SN 2014J**: Detected at **2120σ** at pixel (447.9, 555.4); 73 candidates in that image, 1 real. Batch of 5 SNe: significance **412σ–2120σ**, **73–629 candidates** per image.
- **False positive sources**: Cosmic rays, subtraction artifacts, variable stars, host galaxy nuclei (AGN).

### Output Layout (Relevant Paths)

- **output/difference_images/** — Top-level differencing output; processing_summary.json present; some SN dirs empty or missing FITS.
- **output/datasets/sn2014j/** — Most complete: queries, same_mission_pairs, fits_downloads, fits_training, difference_images, training_triplets (multiple variants).
- **output/datasets/best_yield/** — Chunked queries; one SN (2005ai) in differencing summary.
- **output/fits_training/** — Training manifest (e.g., 2005ai).
- **output/same_mission_pairs.json**, **sn_queries_*.json** — Filtered query outputs.

### sn2014j Dataset (Most Complete)

- **5 SNe differenced**: 2014J, 2014ai, 2014bh, 2014bi, 2014cs (all SWIFT uuu).
- **Overlap**: 66–100%.
- **Training triplets**: 57 samples (30 real, 27 bogus) with augmentation (see `training_triplets/summary.json`).
- **Note**: Some difference image FITS referenced in JSON may be missing on disk (cleaned or moved); metadata in processing_summary/pipeline_report is still the source of truth.

### best_yield Dataset

- **1 SN in differencing summary**: 2005ai (GALEX nd).
- **Overlap ~34.75%** — below typical 85% quality threshold; not ideal for training without improving alignment or selection.

### Quality Assessment

- Pipeline **runs correctly** when inputs and same-mission pairs exist.
- **Dataset size**: 57 triplet samples is far below the thousands needed for robust CNN training.
- **Scale needed**: From ~5 fully processed SNe to 200+ with difference images, then triplet generation at scale.
- **Triplet pipeline**: Present and working; needs to be run on the scaled differencing output.

### Triplet cutout size (research default and choice)

**Research default**: **63×63 pixels** is the standard in the literature for real/bogus CNN input. [SUPERNOVA_DETECTION_PIPELINE_GUIDE.md](SUPERNOVA_DETECTION_PIPELINE_GUIDE.md) states: “Typical cutout size: 63×63 pixels centered on detection” and “Input: 3-channel image (63×63×3)” for the Braai/ZTF-style classifier. The ZTF/Braai pipeline uses 63×63 triplets.

**Recommendation**:
- **63×63**: Use for **CNN training** and for the default pipeline triplets stage. Matches the research and our `RealBogusCNN` input.
- **Larger (127, 255, 511)**: Use for **visualization only** when you want more context (galaxy, field) in the PNGs. Run the triplets stage with `--triplets-cutout 255` and `--triplets-visualize`, or run `create_training_triplets.py` manually with `--cutout-size 255 --visualize` and a separate output dir (e.g. `training_triplets_viz_255`). Keep 63×63 NPZ for training; use larger cutouts only for human inspection.

The config runner now includes an optional **triplets** stage (default: 63×63, output under `.../training_triplets`). Use `--triplets-visualize` to get PNGs; use `--triplets-cutout 255` (or similar) only when you want larger viz cutouts.

### WCS alignment: recent fix (for notes / Q&A)

When training triplets showed **crosshairs and sigma values off**, the cause was inconsistent WCS handling. Fix: **revert to a simpler, proven approach** — one shared FITS loader and consistent use of the header’s WCS for both reference and science images so sky→pixel conversion is reliable everywhere. Simpler, proven methods often beat clever ones.

### Pipeline progress / resume (for notes / Q&A)

Long pipeline runtimes were a major hurdle when iterating on WCS and other fixes. **Solution**: `run_pipeline_from_config.py` now writes a per-dataset **pipeline_progress.json** (next to query results) listing SNe that completed differencing. On the next run, download/organize/differencing skip those and only process the remainder, so reruns are much faster. Use `--no-skip-completed` to force a full reprocess.

### Proof-of-concept strategy: 2014 first, then expand

**Recommendation**: Use **sn2014j** (2014 dataset) as the proof-of-concept before scaling.

- **Why 2014 first**: You already have 5 SNe fully through the pipeline (difference images + triplets), same mission (SWIFT uuu), and known-good overlap (66–100%). Ideal for validating triplet quality, visualization, and a first CNN run.
- **Then expand**: Once the PoC looks good (triplet viz interpretable, model trains, metrics make sense), run the same pipeline on the larger dataset (e.g. `output/fits_downloads` → organize → differencing → triplets). That gives 200+ SNe and thousands of triplets for real training.
- **Benefit**: Catches issues (e.g. overlap display bugs, confusing difference-panel scaling) on a small set before investing in full-scale runs.

---

## 4. What’s Ready for the Midterm Presentation

- **Framing**: AstrID is built from scratch (not using existing survey pipelines); emphasis on hands-on experimentation and system design — full workflow from data acquisition through classification.
- **5-stage pipeline**: Query → Filter → Download → Organize → Differencing, documented in DATA_PIPELINE.md and implemented in scripts.
- **Same-mission discovery**: Why cross-mission differencing fails; how filtering and filter matching address it.
- **Differencing pipeline**: WCS alignment, background subtraction, PSF estimation/matching, flux normalization, difference + significance map, 5σ source detection (with reference to [src/domains/differencing/pipeline.py](../../src/domains/differencing/pipeline.py)).
- **Production dataset**: 222 complete pairs, 2,282 FITS (from DATA_PIPELINE.md); mission breakdown (SWIFT, GALEX, PS1).
- **Config-driven runs**: YAML configs ([configs/](../../configs/)), [PipelineConfig](../../src/pipeline/config.py), two runners (full pipeline vs per-SN).
- **Training triplets**: 63×63 cutouts (sci, ref, diff), real/bogus labels, augmentation; small-scale proof of concept (57 samples).
- **Lessons**: Data quality over model complexity; same-mission requirement; modular design and reproducibility. **Additional (from slides)**: Small details matter (compressed FITS → 296% more pairs). Resource constraints: differencing was killed by OOM on large PS1 images; addressed with float32, explicit cleanup, and batch processing. **Debugging ML ≠ debugging software** — emergent behavior from data–parameter interactions. WCS: when crosshairs/sigma were wrong, reverting to a simpler WCS-from-header approach fixed it. Research is non-linear; progress came from revisiting assumptions and iterating.

---

## 5. Detailed Next Steps

### Phase A: Scale the Dataset (Difference Images)

**Goal**: Generate difference images for 200+ same-mission pairs so downstream triplet and ML steps have enough data.

- Run **run_pipeline_from_config.py** or **run_pipeline_per_sn.py** with a config that targets missions with good same-mission yield (e.g. SWIFT, GALEX golden era). Use existing configs (e.g. [configs/best_yield_combined.yaml](../../configs/best_yield_combined.yaml), [configs/galex_golden_era.yaml](../../configs/galex_golden_era.yaml)) or tune from [configs/README.md](../../configs/README.md).
- Ensure **differencing** runs on all organized pairs: `generate_difference_images.py` with appropriate `--input-dir`/`--output-dir` (and `--manifest` if needed). Use `--mission` to run per mission if desired.
- Optionally **expand SN coordinates** in `generate_difference_images.py` (or a separate catalog) so more SNe get SN-position masks for better real/bogus labeling.
- **Quality filter**: Enforce minimum overlap (e.g. ≥85%) in config or post-processing; exclude or flag pairs with very low overlap (e.g. 2005ai-level).
- **Deliverable**: 200+ difference image sets (diff/sig/mask) with processing_summary.json and pipeline_report.json for reproducibility.

### Phase B: Training Triplet Generation at Scale

**Goal**: Build the 63×63 (science, reference, difference) triplet dataset with real/bogus labels for CNN training.

- Run **create_training_triplets.py** on the scaled differencing output (e.g. `output/datasets/<dataset>/difference_images` and corresponding fits_training/manifest). See [scripts/create_training_triplets.py](../../scripts/create_training_triplets.py).
- **Real** = cutouts centered on known SN positions (use mask or catalog); **bogus** = random positions, or detections far from SN, or dedicated artifact examples.
- Target **balanced or controlled class balance** (e.g. ~50% real / 50% bogus) and document in summary.
- Enable **augmentation** (rotation, flip) and save both raw and augmented NPZ (or equivalent) plus a small visualization subset for sanity checks.
- **Deliverable**: Triplet dataset (e.g. train/val split metadata, NPZ or similar, summary.json) with at least a few thousand samples.

### Phase C: CNN Classifier Implementation

**Goal**: Implement the Braai-style CNN that takes 3-channel (sci, ref, diff) input and outputs real/bogus probability.

- **Input**: 63×63×3 (science, reference, difference).
- **Architecture** (from SUPERNOVA_DETECTION_PIPELINE_GUIDE): Conv2D(32)→ReLU→MaxPool, Conv2D(64)→ReLU→MaxPool, Conv2D(128)→ReLU→MaxPool, Flatten, Dense(256)→ReLU→Dropout(0.5), Dense(1)→Sigmoid.
- **Output**: Probability in [0, 1] (bogus–real).
- **Framework**: TensorFlow/Keras or PyTorch; align with existing project style and any future Edge TPU export if desired.
- **Deliverable**: Model definition, training script (data loader from Phase B, loss, optimizer), and checkpoint saving.

### Phase D: Training and Evaluation

**Goal**: Train the CNN and evaluate with the right metrics for severe class imbalance.

- **Splits**: Train/validation/test with temporal or spatial separation (or by SN) to avoid leakage.
- **Metrics**: Precision, recall, F1, **AUCPR**; do not rely on accuracy. Confusion matrix and threshold analysis.
- **Class imbalance**: Weighted loss or oversampling/undersampling so the model does not collapse to “always bogus.”
- **ZTF-style choice**: Prefer minimizing false negatives (permissive threshold) and document the tradeoff (more false positives for human review).
- **Optional**: Cross-mission or cross-survey validation (train on one mission/survey, test on another) to test generalization.
- **Mission-specific training strategy** (config system enables): (1) train separate models per mission (SWIFT, GALEX, PS1), (2) train a combined model for generalization, (3) cross-mission validation (e.g. train on SWIFT+PS1, test on GALEX), (4) filter-specific models. Goal: learn physics of transients, not instrument artifacts; domain shift is a fundamental challenge in astronomical ML.
- **Deliverable**: Trained model, metrics report, and short write-up of threshold and imbalance handling.

### Phase E: Deployment and Active Learning (Stretch)

**Goal**: Make the classifier usable in a pipeline and, if time permits, add a human-in-the-loop loop.

- **Inference API**: Simple service (e.g. FastAPI) that accepts triplet cutouts or paths and returns real/bogus score; integrate with existing [differencing API](../../src/domains/differencing/api/) if appropriate.
- **Batch usage**: Script or job that runs differencing → source detection → cutout extraction → CNN scoring on new data.
- **Active learning** (stretch): Queue uncertain candidates for human review; feed labels back into a retraining or fine-tuning step.
- **Deliverable**: Runnable inference path and, if implemented, a minimal active-learning workflow description.

---

## 6. Presentation Outline (High-Level)

Use this for the midterm presentation; details can be pulled from DATA_PIPELINE.md and [astrid-midterm.md](astrid-midterm.md).

1. **Title / context** — AstrID: supernova detection pipeline; ML + classical astronomy.
2. **Problem** — Scale of modern surveys (e.g. LSST alerts/night); need for automation and the “bogus” problem.
3. **Pipeline overview** — 5 stages: Query → Filter → Download → Organize → Differencing; output = difference images + significance maps + masks.
4. **Same-mission requirement** — Why cross-mission differencing fails; filter matching; impact on dataset size (e.g. 42% viable from query).
5. **Differencing in practice** — WCS alignment, PSF matching, flux normalization, significance map, 5σ detection; show one example (e.g. SN 2014J) and metrics (overlap, max significance).
6. **Current results** — 222 pairs, 2,282 FITS; 5 SNe fully processed in sn2014j; 57 training triplets as proof of concept.
7. **Scripts and config** — One slide on run_pipeline_from_config vs run_pipeline_per_sn; YAML configs; reproducibility.
8. **What’s next** — Scale to 200+ difference images → triplets at scale → CNN → training/evaluation (AUCPR, imbalance) → optional deployment/active learning.
9. **Lessons** — Data quality, same-mission discovery, modular design, iterative debugging.
10. **Conclusion** — Pipeline verified and ready for scaling; ML phase (Phases 5–8) is the next focus.

### Project timeline (from slides)

- **Feb–Mar 2026**: Scale differencing to 200+ pairs; generate triplet dataset.
- **Mar–Apr 2026**: Implement and train CNN classifier; evaluate with AUCPR.
- **Apr 2026**: Cross-mission validation experiments.
- **Apr–May 2026**: (Stretch) Inference API and active learning prototype.
- **May 2026**: Final report and presentation.

---

## Scripts vs src: Where Things Live and How to Align Later

You’re currently driving everything from **scripts/** (especially `run_pipeline_from_config.py`). Alignment and core logic live in **src/**; scripts call into that where it exists.

**Yes, alignment is done when you run the differencing script.** Flow:

1. **run_pipeline_from_config.py** (scripts) runs the stage `differencing` by invoking **generate_difference_images.py** (scripts) as a subprocess.
2. **generate_difference_images.py** imports `SNDifferencingPipeline` from **src/domains/differencing/pipeline.py**, then calls `pipeline.process(ref_path, sci_path, ...)` for each SN.
3. Inside **src/domains/differencing/pipeline.py**, `process()` does WCS alignment (`reproject_interp`), background subtraction, PSF matching, flux normalization, and differencing. So alignment is executed in `src`, but **triggered** by the script.

**Current split:**

| Layer | Location | Role |
|-------|----------|------|
| **Entry / orchestration** | scripts/ | CLI entry points, subprocess chains, config wiring. `run_pipeline_from_config.py`, `run_pipeline_per_sn.py`, `generate_difference_images.py`, `create_training_triplets.py`, `train_real_bogus_cnn.py`. |
| **Config** | src/pipeline/config.py | `PipelineConfig`, YAML loading. Used by the config-based runners in scripts. |
| **Core logic** | src/domains/ | Differencing pipeline (differencing/), detection/CNN (detection/architectures/), etc. Scripts import from here; they don’t reimplement the algorithms. |

**When you move more work into src:** Keep scripts as thin entry points: parse args, load config, call into `src` (services, pipelines, or domain modules). Put new algorithms and reusable logic in `src/domains/` (or `src/pipeline/` for pipeline-wide helpers). That way scripts stay the single place you run from (`run_pipeline_from_config.py` etc.), and “real” implementation stays in src and can be reused by other entry points (e.g. API, notebooks) later.

---

## Key File References

| Role | Path |
|------|------|
| Pipeline implementation | [src/domains/differencing/pipeline.py](../../src/domains/differencing/pipeline.py) (SNDifferencingPipeline) |
| Config system | [src/pipeline/config.py](../../src/pipeline/config.py) (PipelineConfig) |
| Differencing script | [scripts/generate_difference_images.py](../../scripts/generate_difference_images.py) |
| Config runner | [scripts/run_pipeline_from_config.py](../../scripts/run_pipeline_from_config.py) |
| Per-SN runner | [scripts/run_pipeline_per_sn.py](../../scripts/run_pipeline_per_sn.py) |
| Triplet generator | [scripts/create_training_triplets.py](../../scripts/create_training_triplets.py) |
| Example configs | [configs/best_yield_combined.yaml](../../configs/best_yield_combined.yaml), [configs/galex_golden_era.yaml](../../configs/galex_golden_era.yaml) |
| Midterm report | [astrid-midterm.md](astrid-midterm.md) |
| Pipeline doc | [DATA_PIPELINE.md](DATA_PIPELINE.md) |
| Research sources | [Finding_Supernovae_with_Deep_Learning_Pipelines.txt](../Finding_Supernovae_with_Deep_Learning_Pipelines.txt), [SUPERNOVA_DETECTION_PIPELINE_GUIDE.md](SUPERNOVA_DETECTION_PIPELINE_GUIDE.md) |
