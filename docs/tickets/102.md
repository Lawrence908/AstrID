# ASTR-102: Refine Model Performance Tracking

## Summary
Ensure every `ModelRun` persistently records the complete set of model performance metrics defined in our database schema and exposes them consistently via MLflow and API responses.

## Motivation
Current runs do not always populate all desired metrics, creating gaps in evaluation, comparisons, and dashboards. We need a reliable, standardized metrics surface for training and inference to support model governance and decision-making.

## Scope
- Audit `Model` and `ModelRun` metric fields vs. what pipelines actually compute/log
- Implement missing calculations and align naming across DB, MLflow, and API
- Capture latency and throughput for training/inference stages
- Persist confusion matrices and curves as artifacts; summarize key values in DB
- Update Prefect flows to guarantee metrics capture on success and failure

## Acceptance Criteria
- All required metrics stored for each `ModelRun` and queryable via API
- MLflow runs contain matching metrics/tags and key artifacts (curves, confusion)
- Latency and throughput recorded with consistent units and definitions
- Documentation of metric definitions, source of truth, and calculation methods

## Implementation Plan
1) Metrics Inventory & Mapping
- Enumerate DB fields for `Model` and `ModelRun` related to performance
- Map existing pipeline outputs and identify deltas

2) Metric Computation Enhancements
- Add or refine calculations for:
  - Classification: precision, recall, F1 (micro/macro/weighted), accuracy, AUROC, AUPRC, balanced accuracy, MCC
  - Calibration: ECE, Brier score
  - Regression (if applicable): MAE, RMSE, R2
  - Operational: latency (p50/p95), throughput (items/sec)

3) Artifacts & Visualizations
- Log ROC and PR curves, calibration plots, and confusion matrices to MLflow
- Store serialized summaries to `metrics_summary.json` artifact

4) Persistence & API
- Ensure DB persistence for scalar metrics and references to artifacts
- Update API serializers/schemas to expose the full metrics set

5) Workflow Integration
- Instrument Prefect tasks to record timing consistently
- Guarantee metrics logging in both success and failure paths

6) Backfill
- Create a backfill job/notebook to recompute missing metrics for recent runs

## Metrics Dictionary (Authoritative)
- precision, recall, f1 (macro/micro/weighted)
- accuracy, balanced_accuracy, mcc
- auroc, auprc
- ece, brier_score
- latency_ms_p50, latency_ms_p95
- throughput_items_per_s
- confusion_matrix (artifact), roc_curve (artifact), pr_curve (artifact)

## Rollback Plan
- Configuration flag to disable new metrics while retaining legacy fields
- Revert API additions if incompatibilities arise

## References
- `docs/tickets/linear-tickets.md`
- `src/adapters/scheduler/flows/*`
- `src/core/mlflow_*`
- `src/adapters/api/*`