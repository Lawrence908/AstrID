## **ASTR-88: MLflow Integration (P2) - ML Infrastructure**

### **Context & Current State**
Cloud storage integration is complete (ASTR-71 ✅), providing the foundation for storing ML artifacts and datasets. This ticket sets up MLflow for comprehensive experiment tracking, model management, and model versioning to support the machine learning pipeline for anomaly detection.

### **Technical Requirements**

**Dependencies**: ASTR-71 (Cloud Storage Integration) - ✅ Complete
**Domain**: ML Infrastructure
**Estimated Time**: 2 days

### **Implementation Tasks**

1. **Set up MLflow Tracking Server**
   - Create `src/infrastructure/mlflow/mlflow_server.py`
   - Implement `MLflowServer` with methods:
     - `start_tracking_server(host: str, port: int) -> None`
     - `configure_artifact_store(storage_config: StorageConfig) -> None`
     - `setup_database_backend(database_url: str) -> None`
     - `configure_authentication(auth_config: dict) -> None`
   - Add Docker configuration for MLflow server
   - Implement health checks and monitoring
   - Add configuration management for different environments

2. **Implement Experiment Tracking**
   - Create `src/infrastructure/mlflow/experiment_tracker.py`
   - Implement `ExperimentTracker` with methods:
     - `create_experiment(name: str, description: str) -> str`
     - `start_run(experiment_id: str, run_name: str) -> str`
     - `log_parameters(params: dict, run_id: str) -> None`
     - `log_metrics(metrics: dict, step: int, run_id: str) -> None`
     - `log_artifacts(artifacts: dict, run_id: str) -> None`
     - `end_run(run_id: str, status: str) -> None`
   - Add support for nested runs and parent-child relationships
   - Implement automatic parameter and metric logging
   - Add experiment comparison and visualization

3. **Add Model Registry Functionality**
   - Create `src/infrastructure/mlflow/model_registry.py`
   - Implement `ModelRegistry` with methods:
     - `register_model(model_path: str, model_name: str, run_id: str) -> str`
     - `get_model_version(model_name: str, version: str) -> dict`
     - `list_model_versions(model_name: str) -> list[dict]`
     - `transition_model_stage(model_name: str, version: str, stage: str) -> None`
     - `get_latest_model(model_name: str, stage: str) -> dict`
     - `delete_model_version(model_name: str, version: str) -> None`
   - Add model metadata tracking and tagging
   - Implement model lineage and provenance tracking
   - Add model performance comparison tools

4. **Create Model Versioning System**
   - Create `src/infrastructure/mlflow/model_versioning.py`
   - Implement `ModelVersioning` with methods:
     - `create_model_version(model_name: str, model_data: bytes, metadata: dict) -> str`
     - `get_model_version(model_name: str, version: str) -> bytes`
     - `compare_model_versions(model_name: str, version1: str, version2: str) -> dict`
     - `rollback_model_version(model_name: str, target_version: str) -> None`
     - `archive_model_version(model_name: str, version: str) -> None`
   - Add semantic versioning support
   - Implement model deployment tracking
   - Add model performance monitoring

### **Integration Points**

- **Cloud Storage**: Use R2 for MLflow artifact storage
- **Domain Models**: Track model training and evaluation metrics
- **Detection Pipeline**: Log anomaly detection model performance
- **API**: Expose model registry and experiment data via REST API
- **Monitoring**: Integrate with system monitoring and alerting

### **MLflow Configuration**
```python
@dataclass
class MLflowConfig:
    tracking_uri: str
    artifact_root: str
    database_url: str
    authentication_enabled: bool
    model_registry_enabled: bool
    experiment_auto_logging: bool
    artifact_compression: bool
    max_artifact_size: int
```

### **Experiment Structure**
```
experiments/
├── anomaly-detection/          # Main anomaly detection experiments
│   ├── unet-training/         # U-Net model training runs
│   ├── hyperparameter-tuning/ # Hyperparameter optimization
│   └── model-evaluation/      # Model evaluation and testing
├── preprocessing/             # Image preprocessing experiments
│   ├── calibration-optimization/
│   └── quality-assessment/
└── differencing/              # Image differencing experiments
    ├── algorithm-comparison/
    └── parameter-tuning/
```

### **API Endpoints to Add**
```python
GET /mlflow/experiments
POST /mlflow/experiments
GET /mlflow/experiments/{id}/runs
POST /mlflow/experiments/{id}/runs
GET /mlflow/models
POST /mlflow/models/register
GET /mlflow/models/{name}/versions
POST /mlflow/models/{name}/versions/{version}/transition
GET /mlflow/artifacts/{run_id}/{path}
```

### **Model Registry Stages**
```python
class ModelStage(Enum):
    NONE = "None"
    STAGING = "Staging"
    PRODUCTION = "Production"
    ARCHIVED = "Archived"
```

### **Error Handling**
- MLflow server connection failures and retry logic
- Artifact upload/download error handling
- Database connection and transaction management
- Model registry operation error recovery
- Comprehensive logging for ML operations

### **Testing Strategy**
- Unit tests for all MLflow client operations
- Integration tests with MLflow server
- Model registry workflow tests
- Experiment tracking accuracy validation
- Performance tests for large artifact handling

### **Security Considerations**
- Authentication and authorization for MLflow access
- Secure artifact storage and access control
- Model registry access permissions
- Audit logging for model operations
- Data privacy for sensitive model data
