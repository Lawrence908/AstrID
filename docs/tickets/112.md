## **ASTR-112: Data Backup and Disaster Recovery System (P4) - Infrastructure**

### **Context & Current State**
Database setup (ASTR-70 ), cloud storage integration (ASTR-71 ), and MLflow integration (ASTR-88 ) are complete, providing the foundation for a comprehensive backup and disaster recovery system. This ticket implements automated backup procedures and disaster recovery capabilities to protect valuable astronomical data and ensure business continuity.

### **Technical Requirements**

**Dependencies**: ASTR-70 (Database Setup)  Complete, ASTR-71 (Cloud Storage Integration)  Complete, ASTR-88 (MLflow Integration)  Complete
**Domain**: Infrastructure (Data Protection)
**Estimated Time**: 3 days

### **Implementation Tasks**

1. **Set Up Automated Database Backup System**
   - Create `src/infrastructure/backup/database_backup.py`:
     - Implement `DatabaseBackupService` with methods:
       - `create_database_backup(backup_type: str) -> str`
       - `restore_database_backup(backup_id: str, target_database: str) -> bool`
       - `list_available_backups() -> list[dict]`
       - `validate_backup_integrity(backup_id: str) -> bool`
       - `cleanup_old_backups(retention_days: int) -> int`
   - Add backup scheduling configuration:
     - Daily full backups at 2 AM
     - Hourly incremental backups during business hours
     - Weekly compressed archival backups
   - Implement backup verification and integrity checking
   - Add backup encryption for sensitive data

2. **Implement Cloud Storage Redundancy and Versioning**
   - Create `src/infrastructure/backup/storage_backup.py`:
     - Implement `StorageBackupService` with methods:
       - `backup_storage_objects(source_bucket: str, target_bucket: str) -> dict`
       - `enable_versioning(bucket_name: str) -> bool`
       - `create_cross_region_replication(source_region: str, target_region: str) -> bool`
       - `restore_storage_objects(backup_id: str, target_bucket: str) -> bool`
   - Configure Cloudflare R2 with versioning and cross-region replication
   - Implement automated storage backup scheduling
   - Add storage cost optimization and lifecycle policies

3. **Create ML Model and Artifact Backup Procedures**
   - Create `src/infrastructure/backup/ml_backup.py`:
     - Implement `MLBackupService` with methods:
       - `backup_mlflow_artifacts(experiment_id: str) -> str`
       - `backup_model_registry() -> str`
       - `backup_training_data(dataset_id: str) -> str`
       - `restore_ml_artifacts(backup_id: str) -> bool`
   - Integrate with existing MLflow infrastructure from ASTR-88
   - Add model versioning and lineage preservation
   - Implement artifact integrity verification

4. **Add Disaster Recovery Testing and Documentation**
   - Create `src/infrastructure/backup/disaster_recovery.py`:
     - Implement `DisasterRecoveryService` with methods:
       - `test_recovery_procedures() -> dict`
       - `simulate_disaster_scenario(scenario: str) -> dict`
       - `calculate_recovery_time_objective() -> dict`
       - `validate_data_consistency() -> bool`
   - Create comprehensive disaster recovery documentation
   - Implement automated recovery testing procedures
   - Add recovery time and point objectives (RTO/RPO) monitoring

### **Integration Points**

- **Database**: Backup Supabase PostgreSQL data and schemas
- **Storage**: Backup Cloudflare R2 objects and configurations
- **MLflow**: Backup experiment data, models, and artifacts
- **Monitoring**: Integrate with existing monitoring infrastructure
- **Scheduling**: Use existing Prefect workflow system

### **Backup Configuration**
```python
@dataclass
class BackupConfig:
    # Database backup settings
    database_backup_enabled: bool = True
    database_backup_schedule: str = "0 2 * * *"  # Daily at 2 AM
    database_retention_days: int = 30
    database_encryption_enabled: bool = True
    
    # Storage backup settings
    storage_backup_enabled: bool = True
    storage_backup_schedule: str = "0 3 * * *"  # Daily at 3 AM
    storage_versioning_enabled: bool = True
    storage_cross_region_replication: bool = True
    
    # ML backup settings
    ml_backup_enabled: bool = True
    ml_backup_schedule: str = "0 4 * * *"  # Daily at 4 AM
    ml_artifact_retention_days: int = 90
    ml_model_versioning: bool = True
    
    # Disaster recovery settings
    dr_testing_enabled: bool = True
    dr_test_schedule: str = "0 0 * * 0"  # Weekly on Sunday
    rto_target_minutes: int = 60  # Recovery Time Objective
    rpo_target_minutes: int = 15  # Recovery Point Objective
```

### **Database Backup Service**
```python
# src/infrastructure/backup/database_backup.py
import asyncio
import boto3
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging

class DatabaseBackupService:
    """Service for managing database backups and recovery."""
    
    def __init__(
        self,
        database_url: str,
        backup_storage_bucket: str,
        encryption_key: Optional[str] = None
    ):
        self.database_url = database_url
        self.backup_bucket = backup_storage_bucket
        self.encryption_key = encryption_key
        self.s3_client = boto3.client('s3')
        self.logger = logging.getLogger(__name__)
    
    async def create_database_backup(
        self, 
        backup_type: str = "full"
    ) -> Dict[str, any]:
        """Create a database backup."""
        
        backup_id = f"db_backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        backup_path = f"database_backups/{backup_id}.sql"
        
        try:
            # Create database dump
            dump_command = self._build_dump_command(backup_type)
            dump_result = await self._execute_dump_command(dump_command)
            
            if dump_result["success"]:
                # Upload to backup storage
                upload_result = await self._upload_backup(
                    dump_result["file_path"], 
                    backup_path
                )
                
                if upload_result["success"]:
                    # Record backup metadata
                    await self._record_backup_metadata(
                        backup_id, backup_type, backup_path
                    )
                    
                    return {
                        "backup_id": backup_id,
                        "backup_path": backup_path,
                        "backup_type": backup_type,
                        "size_bytes": upload_result["size_bytes"],
                        "created_at": datetime.utcnow().isoformat(),
                        "status": "success"
                    }
                else:
                    raise Exception(f"Upload failed: {upload_result['error']}")
            else:
                raise Exception(f"Dump failed: {dump_result['error']}")
                
        except Exception as e:
            self.logger.error(f"Database backup failed: {e}")
            return {
                "backup_id": backup_id,
                "status": "failed",
                "error": str(e)
            }
    
    async def restore_database_backup(
        self, 
        backup_id: str, 
        target_database: str
    ) -> Dict[str, any]:
        """Restore database from backup."""
        
        try:
            # Download backup from storage
            backup_path = f"database_backups/{backup_id}.sql"
            download_result = await self._download_backup(backup_path)
            
            if not download_result["success"]:
                raise Exception(f"Download failed: {download_result['error']}")
            
            # Restore database
            restore_command = self._build_restore_command(
                download_result["file_path"], 
                target_database
            )
            restore_result = await self._execute_restore_command(restore_command)
            
            if restore_result["success"]:
                return {
                    "backup_id": backup_id,
                    "target_database": target_database,
                    "restored_at": datetime.utcnow().isoformat(),
                    "status": "success"
                }
            else:
                raise Exception(f"Restore failed: {restore_result['error']}")
                
        except Exception as e:
            self.logger.error(f"Database restore failed: {e}")
            return {
                "backup_id": backup_id,
                "status": "failed",
                "error": str(e)
            }
    
    async def list_available_backups(self) -> List[Dict[str, any]]:
        """List all available database backups."""
        
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.backup_bucket,
                Prefix="database_backups/"
            )
            
            backups = []
            for obj in response.get('Contents', []):
                if obj['Key'].endswith('.sql'):
                    backup_id = obj['Key'].split('/')[-1].replace('.sql', '')
                    backups.append({
                        "backup_id": backup_id,
                        "backup_path": obj['Key'],
                        "size_bytes": obj['Size'],
                        "created_at": obj['LastModified'].isoformat(),
                        "storage_class": obj.get('StorageClass', 'STANDARD')
                    })
            
            return sorted(backups, key=lambda x: x['created_at'], reverse=True)
            
        except Exception as e:
            self.logger.error(f"Failed to list backups: {e}")
            return []
    
    async def validate_backup_integrity(self, backup_id: str) -> bool:
        """Validate backup file integrity."""
        
        try:
            backup_path = f"database_backups/{backup_id}.sql"
            
            # Check if backup exists
            response = self.s3_client.head_object(
                Bucket=self.backup_bucket,
                Key=backup_path
            )
            
            # Verify file size is reasonable
            file_size = response['ContentLength']
            if file_size < 1024:  # Less than 1KB is suspicious
                return False
            
            # Download and verify SQL structure
            download_result = await self._download_backup(backup_path)
            if not download_result["success"]:
                return False
            
            # Check for SQL dump markers
            with open(download_result["file_path"], 'r') as f:
                content = f.read(1024)  # Read first 1KB
                if "PostgreSQL database dump" in content:
                    return True
                else:
                    return False
                    
        except Exception as e:
            self.logger.error(f"Backup integrity check failed: {e}")
            return False
    
    def _build_dump_command(self, backup_type: str) -> str:
        """Build pg_dump command based on backup type."""
        
        base_command = "pg_dump"
        
        if backup_type == "full":
            options = "--verbose --clean --no-acl --no-owner"
        elif backup_type == "incremental":
            options = "--verbose --data-only --no-acl --no-owner"
        else:
            options = "--verbose --no-acl --no-owner"
        
        return f"{base_command} {options} {self.database_url}"
    
    def _build_restore_command(self, backup_file: str, target_database: str) -> str:
        """Build psql restore command."""
        
        return f"psql {target_database} < {backup_file}"
```

### **Storage Backup Service**
```python
# src/infrastructure/backup/storage_backup.py
class StorageBackupService:
    """Service for managing cloud storage backups."""
    
    def __init__(self, source_bucket: str, backup_bucket: str):
        self.source_bucket = source_bucket
        self.backup_bucket = backup_bucket
        self.s3_client = boto3.client('s3')
        self.logger = logging.getLogger(__name__)
    
    async def backup_storage_objects(
        self, 
        source_bucket: str, 
        target_bucket: str
    ) -> Dict[str, any]:
        """Backup storage objects to backup bucket."""
        
        backup_id = f"storage_backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        backed_up_objects = 0
        total_size = 0
        
        try:
            # List all objects in source bucket
            paginator = self.s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=source_bucket)
            
            for page in pages:
                for obj in page.get('Contents', []):
                    # Copy object to backup bucket
                    copy_source = {
                        'Bucket': source_bucket,
                        'Key': obj['Key']
                    }
                    
                    backup_key = f"backups/{backup_id}/{obj['Key']}"
                    
                    self.s3_client.copy_object(
                        CopySource=copy_source,
                        Bucket=target_bucket,
                        Key=backup_key,
                        Metadata={
                            'original_bucket': source_bucket,
                            'backup_date': datetime.utcnow().isoformat(),
                            'original_size': str(obj['Size'])
                        }
                    )
                    
                    backed_up_objects += 1
                    total_size += obj['Size']
            
            return {
                "backup_id": backup_id,
                "backed_up_objects": backed_up_objects,
                "total_size_bytes": total_size,
                "source_bucket": source_bucket,
                "target_bucket": target_bucket,
                "status": "success"
            }
            
        except Exception as e:
            self.logger.error(f"Storage backup failed: {e}")
            return {
                "backup_id": backup_id,
                "status": "failed",
                "error": str(e)
            }
    
    async def enable_versioning(self, bucket_name: str) -> bool:
        """Enable versioning for storage bucket."""
        
        try:
            self.s3_client.put_bucket_versioning(
                Bucket=bucket_name,
                VersioningConfiguration={'Status': 'Enabled'}
            )
            return True
        except Exception as e:
            self.logger.error(f"Failed to enable versioning: {e}")
            return False
```

### **ML Backup Service**
```python
# src/infrastructure/backup/ml_backup.py
class MLBackupService:
    """Service for managing ML model and artifact backups."""
    
    def __init__(self, mlflow_tracking_uri: str, backup_bucket: str):
        self.mlflow_uri = mlflow_tracking_uri
        self.backup_bucket = backup_bucket
        self.s3_client = boto3.client('s3')
        self.logger = logging.getLogger(__name__)
    
    async def backup_mlflow_artifacts(self, experiment_id: str) -> str:
        """Backup MLflow artifacts for an experiment."""
        
        backup_id = f"ml_backup_{experiment_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            # Get experiment runs
            runs = mlflow.search_runs(experiment_ids=[experiment_id])
            
            for _, run in runs.iterrows():
                run_id = run['run_id']
                
                # Download artifacts
                artifacts = mlflow.artifacts.list_artifacts(run_id)
                
                for artifact in artifacts:
                    # Download artifact
                    local_path = mlflow.artifacts.download_artifacts(
                        run_id=run_id,
                        path=artifact.path
                    )
                    
                    # Upload to backup storage
                    backup_key = f"ml_backups/{backup_id}/{run_id}/{artifact.path}"
                    self.s3_client.upload_file(
                        local_path,
                        self.backup_bucket,
                        backup_key
                    )
            
            return backup_id
            
        except Exception as e:
            self.logger.error(f"ML backup failed: {e}")
            raise
```

### **Disaster Recovery Service**
```python
# src/infrastructure/backup/disaster_recovery.py
class DisasterRecoveryService:
    """Service for disaster recovery testing and procedures."""
    
    def __init__(self, backup_service: DatabaseBackupService):
        self.backup_service = backup_service
        self.logger = logging.getLogger(__name__)
    
    async def test_recovery_procedures(self) -> Dict[str, any]:
        """Test disaster recovery procedures."""
        
        test_results = {
            "database_recovery": False,
            "storage_recovery": False,
            "ml_recovery": False,
            "overall_success": False,
            "recovery_time_minutes": 0,
            "errors": []
        }
        
        start_time = datetime.utcnow()
        
        try:
            # Test database recovery
            latest_backup = await self._get_latest_backup()
            if latest_backup:
                recovery_result = await self.backup_service.restore_database_backup(
                    latest_backup["backup_id"],
                    "test_recovery_db"
                )
                test_results["database_recovery"] = recovery_result["status"] == "success"
            
            # Test storage recovery
            storage_test = await self._test_storage_recovery()
            test_results["storage_recovery"] = storage_test["success"]
            
            # Test ML recovery
            ml_test = await self._test_ml_recovery()
            test_results["ml_recovery"] = ml_test["success"]
            
            # Calculate recovery time
            end_time = datetime.utcnow()
            test_results["recovery_time_minutes"] = (end_time - start_time).total_seconds() / 60
            
            # Overall success
            test_results["overall_success"] = all([
                test_results["database_recovery"],
                test_results["storage_recovery"],
                test_results["ml_recovery"]
            ])
            
        except Exception as e:
            test_results["errors"].append(str(e))
            self.logger.error(f"Recovery test failed: {e}")
        
        return test_results
```

### **API Endpoints**
```python
# Add to existing API
@router.post("/backup/database")
async def create_database_backup(
    backup_type: str = "full",
    current_user: User = Depends(get_current_admin_user)
):
    """Create a database backup."""
    backup_service = DatabaseBackupService(
        database_url=settings.DATABASE_URL,
        backup_storage_bucket=settings.BACKUP_BUCKET
    )
    
    result = await backup_service.create_database_backup(backup_type)
    return result

@router.get("/backup/database")
async def list_database_backups(
    current_user: User = Depends(get_current_admin_user)
):
    """List available database backups."""
    backup_service = DatabaseBackupService(
        database_url=settings.DATABASE_URL,
        backup_storage_bucket=settings.BACKUP_BUCKET
    )
    
    backups = await backup_service.list_available_backups()
    return {"backups": backups}

@router.post("/backup/test-recovery")
async def test_disaster_recovery(
    current_user: User = Depends(get_current_admin_user)
):
    """Test disaster recovery procedures."""
    dr_service = DisasterRecoveryService(backup_service)
    results = await dr_service.test_recovery_procedures()
    return results
```

### **Environment Configuration**
```bash
# Add to .env file
# Backup settings
BACKUP_ENABLED=true
BACKUP_STORAGE_BUCKET=astrid-backups
BACKUP_ENCRYPTION_KEY=your_encryption_key_here

# Database backup
DATABASE_BACKUP_SCHEDULE="0 2 * * *"
DATABASE_RETENTION_DAYS=30

# Storage backup
STORAGE_BACKUP_SCHEDULE="0 3 * * *"
STORAGE_VERSIONING_ENABLED=true

# ML backup
ML_BACKUP_SCHEDULE="0 4 * * *"
ML_ARTIFACT_RETENTION_DAYS=90

# Disaster recovery
DR_TESTING_ENABLED=true
DR_TEST_SCHEDULE="0 0 * * 0"
RTO_TARGET_MINUTES=60
RPO_TARGET_MINUTES=15
```

### **Error Handling and Monitoring**
- Backup failure detection and alerting
- Recovery procedure validation and testing
- Data integrity verification and checksums
- Comprehensive audit logging for all backup operations
- Performance monitoring for backup and recovery times

### **Testing Strategy**
- Unit tests for all backup services
- Integration tests for backup and restore procedures
- Disaster recovery simulation tests
- Performance tests for large dataset backups
- Data integrity validation tests

### **Performance Considerations**
- Incremental backup strategies to minimize storage usage
- Parallel backup processing for large datasets
- Compression and deduplication for storage optimization
- Backup scheduling to minimize impact on production systems
- Recovery time optimization and testing

### **Expected Deliverables**
1. **Database Backup System** - Automated PostgreSQL backup and restore
2. **Storage Backup System** - Cloud storage redundancy and versioning
3. **ML Backup System** - MLflow artifacts and model backup
4. **Disaster Recovery System** - Testing and recovery procedures
5. **API Endpoints** - Backup management and monitoring
6. **Documentation** - Comprehensive backup and recovery procedures
7. **Monitoring** - Backup status and health monitoring

### **Success Criteria**
- Automated backups run successfully on schedule
- Database can be restored from backup within RTO target
- Storage redundancy protects against data loss
- ML artifacts are preserved and recoverable
- Disaster recovery procedures are tested and documented
- System maintains data integrity throughout backup/restore cycles

This backup and disaster recovery system will provide essential data protection for the AstrID project, ensuring that valuable astronomical data and research results are preserved and can be recovered in case of system failures or disasters.
