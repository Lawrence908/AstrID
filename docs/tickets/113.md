## **ASTR-113: Real Data Loading Integration for Training Pipeline (P2) - ML Infrastructure**

### **Context & Current State**
The training notebook (ASTR-106 ✅) currently uses synthetic data generation for model training. This ticket implements real data loading functions that integrate with the complete AstrID workflow pipeline, enabling training on validated astronomical observations and human-labeled detections. This provides authentic training data that flows through the full observation → preprocessing → differencing → detection → validation pipeline.

### **Technical Requirements**

**Dependencies**: ASTR-106 (Training Notebook) ✅, ASTR-88 (MLflow Integration) ✅, ASTR-81 (Detection Pipeline) ✅, ASTR-76 (Preprocessing) ✅, ASTR-73 (Observation Models) ✅
**Domain**: ML Infrastructure (Data Pipeline Integration)
**Estimated Time**: 3 days

### **Implementation Tasks**

1. **Create Training Data Collection Service**
   - Implement `TrainingDataCollector` service for harvesting validated detections
   - Create database queries to fetch observations with associated preprocessing and detection data
   - Add data quality filtering and validation logic
   - Implement temporal and spatial data selection criteria
   - Add data collection metrics and monitoring

2. **Implement Real Data Loading Pipeline**
   - Create `RealDataLoader` class to replace synthetic data generation
   - Integrate with existing `AstronomicalImageProcessor` from ASTR-76
   - Implement FITS file loading and processing from R2 storage
   - Add support for different image formats and survey sources
   - Create data preprocessing pipeline that matches inference pipeline

3. **Add Training Dataset Management**
   - Implement `TrainingDataset` entity for dataset versioning and tracking
   - Create database schema extensions for training data storage
   - Add dataset quality metrics and validation
   - Implement dataset splitting and sampling strategies
   - Add dataset metadata and lineage tracking

4. **Integrate with Human Validation Labels**
   - Connect to validation events from ASTR-82 (Human Validation System)
   - Implement label extraction from human review process
   - Add support for different anomaly types and confidence levels
   - Create ground truth mask generation from validation labels
   - Implement label quality assessment and filtering

5. **Add Data Pipeline Orchestration**
   - Create Dramatiq worker for automated training data collection
   - Implement Prefect flow for training data pipeline orchestration
   - Add data collection scheduling and triggers
   - Implement data freshness monitoring and alerts
   - Add data collection performance metrics

6. **Enhance MLflow Integration for Real Data**
   - Extend MLflow logging to include dataset metadata and lineage
   - Add data quality metrics to experiment tracking
   - Implement dataset versioning in MLflow
   - Add data distribution logging and visualization
   - Create dataset comparison and analysis tools

### **Integration Points**

- **Observation Domain**: Fetch observations and survey data from ASTR-73
- **Preprocessing Domain**: Use `AstronomicalImageProcessor` from ASTR-76
- **Detection Domain**: Access detection results and confidence scores from ASTR-81
- **Curation Domain**: Integrate with human validation labels from ASTR-82
- **Storage**: Load FITS files from Cloudflare R2 storage
- **Database**: Query and store training dataset metadata
- **MLflow**: Enhanced experiment tracking with real data metrics

### **Database Schema Extensions**

```sql
-- Training dataset storage
CREATE TABLE training_datasets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    collection_params JSONB NOT NULL,
    total_samples INTEGER NOT NULL,
    anomaly_ratio FLOAT NOT NULL,
    quality_score FLOAT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_by VARCHAR(255) NOT NULL,
    status VARCHAR(50) DEFAULT 'active'
);

-- Training samples linking
CREATE TABLE training_samples (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    dataset_id UUID REFERENCES training_datasets(id),
    observation_id UUID REFERENCES observations(id),
    detection_id UUID REFERENCES detections(id),
    image_path VARCHAR(500) NOT NULL,
    mask_path VARCHAR(500) NOT NULL,
    labels JSONB NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Training run tracking
CREATE TABLE training_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    dataset_id UUID REFERENCES training_datasets(id),
    model_id UUID REFERENCES models(id),
    mlflow_run_id VARCHAR(255),
    training_params JSONB NOT NULL,
    performance_metrics JSONB,
    status VARCHAR(50) DEFAULT 'running',
    started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    completed_at TIMESTAMP WITH TIME ZONE
);
```

### **Core Implementation Classes**

```python
@dataclass
class TrainingDataCollectionParams:
    """Parameters for training data collection."""
    survey_ids: List[str]
    date_range: Tuple[datetime, datetime]
    confidence_threshold: float = 0.7
    anomaly_types: List[str] = None
    quality_score_threshold: float = 0.8
    max_samples: int = 10000
    validation_status: str = 'validated'

class TrainingDataCollector:
    """Service for collecting training data from validated detections."""
    
    def __init__(self, db_session: Session, r2_client: R2Client):
        self.db = db_session
        self.r2 = r2_client
        self.preprocessor = AstronomicalImageProcessor()
    
    def collect_training_data(self, params: TrainingDataCollectionParams) -> List[TrainingSample]:
        """Collect training samples from validated detections."""
        pass
    
    def fetch_observations(self, params: TrainingDataCollectionParams) -> List[Observation]:
        """Fetch observations matching collection criteria."""
        pass
    
    def load_associated_data(self, observation: Observation) -> Dict[str, Any]:
        """Load all associated data for an observation."""
        pass
    
    def create_training_sample(self, observation: Observation, detection: Detection) -> TrainingSample:
        """Create training sample from observation and detection."""
        pass

class RealDataLoader:
    """Real data loader for training pipeline."""
    
    def __init__(self, collector: TrainingDataCollector):
        self.collector = collector
    
    def load_training_dataset(self, dataset_id: str) -> AstrIDTrainingDataset:
        """Load real training dataset from database."""
        pass
    
    def create_data_splits(self, samples: List[TrainingSample]) -> Dict[str, List[TrainingSample]]:
        """Create train/validation/test splits."""
        pass
    
    def preprocess_samples(self, samples: List[TrainingSample]) -> List[TrainingSample]:
        """Apply preprocessing pipeline to samples."""
        pass
    
    def validate_data_quality(self, samples: List[TrainingSample]) -> DataQualityReport:
        """Validate data quality and return report."""
        pass

class TrainingDatasetManager:
    """Manager for training dataset operations."""
    
    def create_dataset(self, samples: List[TrainingSample], name: str) -> str:
        """Create and store training dataset."""
        pass
    
    def get_dataset(self, dataset_id: str) -> TrainingDataset:
        """Retrieve training dataset."""
        pass
    
    def update_dataset_metrics(self, dataset_id: str, metrics: Dict[str, Any]) -> None:
        """Update dataset quality metrics."""
        pass
    
    def list_datasets(self, filters: Dict[str, Any] = None) -> List[TrainingDataset]:
        """List available training datasets."""
        pass
```

### **Data Collection Pipeline**

```python
@dramatiq.actor(queue_name='training_data')
def collect_training_data_worker(collection_params: dict):
    """Worker to collect and prepare training data from validated detections."""
    
    collector = TrainingDataCollector(db_session, r2_client)
    
    # Collect validated detections
    samples = collector.collect_training_data(
        TrainingDataCollectionParams(**collection_params)
    )
    
    # Validate data quality
    quality_report = collector.validate_data_quality(samples)
    
    if quality_report.quality_score < 0.8:
        raise ValueError(f"Data quality below threshold: {quality_report.quality_score}")
    
    # Create training dataset
    dataset_id = collector.create_dataset(samples, collection_params['name'])
    
    # Log to MLflow
    log_training_dataset_info(dataset_id, samples, quality_report)
    
    # Trigger training pipeline
    trigger_training_pipeline.send(dataset_id)

@flow
def training_data_collection_flow(collection_params: dict):
    """Prefect flow for training data collection."""
    
    # Collect detections
    detections = collect_validated_detections(collection_params)
    
    # Prepare samples
    samples = prepare_training_samples(detections)
    
    # Validate quality
    quality_report = validate_training_data(samples)
    
    # Create dataset
    dataset_id = create_training_dataset(samples, collection_params['name'])
    
    # Trigger training
    trigger_training_pipeline.send(dataset_id)
    
    return dataset_id
```

### **Enhanced Training Notebook Integration**

```python
# Updated training notebook data loading
def load_real_training_data(dataset_id: str = None, collection_params: dict = None):
    """Load real training data from the pipeline."""
    
    if dataset_id:
        # Load existing dataset
        loader = RealDataLoader(TrainingDataCollector(db_session, r2_client))
        return loader.load_training_dataset(dataset_id)
    
    elif collection_params:
        # Collect new data
        collector = TrainingDataCollector(db_session, r2_client)
        samples = collector.collect_training_data(
            TrainingDataCollectionParams(**collection_params)
        )
        
        # Create dataset
        dataset_id = collector.create_dataset(samples, collection_params['name'])
        
        # Load for training
        loader = RealDataLoader(collector)
        return loader.load_training_dataset(dataset_id)
    
    else:
        raise ValueError("Either dataset_id or collection_params must be provided")

# Example usage in training notebook
collection_params = {
    'survey_ids': ['hst', 'jwst'],
    'date_range': (datetime(2024, 1, 1), datetime(2024, 12, 31)),
    'confidence_threshold': 0.7,
    'anomaly_types': ['supernova', 'asteroid', 'variable_star'],
    'quality_score_threshold': 0.8,
    'max_samples': 5000,
    'name': 'hst_jwst_2024_anomalies'
}

train_dataset, val_dataset, test_dataset = load_real_training_data(
    collection_params=collection_params
)
```

### **Data Quality Validation**

```python
@dataclass
class DataQualityReport:
    """Report on training data quality."""
    total_samples: int
    anomaly_ratio: float
    image_quality_score: float
    label_consistency: float
    temporal_distribution: Dict[str, Any]
    survey_coverage: Dict[str, int]
    quality_score: float
    issues: List[str]

class DataQualityValidator:
    """Validator for training data quality."""
    
    def validate_training_data(self, samples: List[TrainingSample]) -> DataQualityReport:
        """Validate training data quality."""
        pass
    
    def check_anomaly_ratio(self, samples: List[TrainingSample]) -> float:
        """Check anomaly to normal ratio."""
        pass
    
    def assess_image_quality(self, samples: List[TrainingSample]) -> float:
        """Assess overall image quality."""
        pass
    
    def validate_label_consistency(self, samples: List[TrainingSample]) -> float:
        """Validate label consistency across samples."""
        pass
```

### **MLflow Integration Enhancements**

```python
def log_training_dataset_info(dataset_id: str, samples: List[TrainingSample], quality_report: DataQualityReport):
    """Log training dataset information to MLflow."""
    
    with mlflow.start_run(run_name=f"training_data_{dataset_id}"):
        # Log dataset statistics
        mlflow.log_metric("total_samples", len(samples))
        mlflow.log_metric("anomaly_ratio", quality_report.anomaly_ratio)
        mlflow.log_metric("quality_score", quality_report.quality_score)
        mlflow.log_metric("image_quality_score", quality_report.image_quality_score)
        mlflow.log_metric("label_consistency", quality_report.label_consistency)
        
        # Log data distribution
        log_data_distribution(samples)
        
        # Log sample images
        log_sample_images(samples, max_samples=10)
        
        # Log dataset metadata
        mlflow.log_params({
            "collection_date": datetime.now().isoformat(),
            "validation_threshold": 0.7,
            "time_range_days": 180,
            "survey_sources": list(quality_report.survey_coverage.keys()),
            "anomaly_types": list(set(sample.labels.get('anomaly_type') for sample in samples))
        })
```

### **API Endpoints**

```python
# New API endpoints for training data management
@router.post("/training/datasets/collect")
async def collect_training_data(params: TrainingDataCollectionParams):
    """Trigger training data collection."""
    pass

@router.get("/training/datasets")
async def list_training_datasets(filters: dict = None):
    """List available training datasets."""
    pass

@router.get("/training/datasets/{dataset_id}")
async def get_training_dataset(dataset_id: str):
    """Get training dataset details."""
    pass

@router.get("/training/datasets/{dataset_id}/quality")
async def get_dataset_quality(dataset_id: str):
    """Get dataset quality report."""
    pass

@router.post("/training/datasets/{dataset_id}/load")
async def load_dataset_for_training(dataset_id: str):
    """Load dataset for training."""
    pass
```

### **Error Handling and Validation**

- Data collection failure detection and recovery
- FITS file loading error handling
- Database connection error handling
- Data quality validation and filtering
- Memory management for large datasets
- Comprehensive error logging and reporting

### **Testing Strategy**

- Unit tests for data collection and loading functions
- Integration tests with real database and storage
- Data quality validation tests
- MLflow integration tests
- End-to-end pipeline tests
- Performance tests with large datasets

### **Documentation Requirements**

- Data collection parameter documentation
- Dataset quality metrics explanation
- Integration guide with existing pipeline
- Troubleshooting guide for data issues
- Best practices for data collection
- Performance optimization guidelines

### **Performance Considerations**

- Efficient database queries for large datasets
- Streaming data loading for memory efficiency
- Parallel data processing where possible
- Caching for frequently accessed data
- Progress tracking for long data collection runs

### **Expected Deliverables**

1. **`src/domains/ml/training_data/`** - Training data collection services
2. **`src/domains/ml/training_data/models.py`** - Database models for training datasets
3. **`src/domains/ml/training_data/services.py`** - Training data collection services
4. **`src/domains/ml/training_data/workers.py`** - Dramatiq workers for data collection
5. **`src/domains/ml/training_data/flows.py`** - Prefect flows for orchestration
6. **`notebooks/training/real_data_loading.ipynb`** - Example notebook for real data usage
7. **Database migrations** - Schema extensions for training data
8. **API endpoints** - REST API for training data management
9. **Integration tests** - Comprehensive testing of data pipeline

### **Success Criteria**

- Training data collection from validated detections works correctly
- Real data loading integrates seamlessly with existing training notebook
- Data quality validation ensures high-quality training datasets
- MLflow integration logs comprehensive dataset metrics
- Database schema supports training dataset management
- API endpoints provide full training data management capabilities
- Performance is acceptable for large datasets (10k+ samples)
- Documentation is clear and comprehensive

### **Future Enhancements**

- Automated data collection scheduling
- Advanced data augmentation strategies
- Multi-survey data fusion
- Real-time data quality monitoring
- Automated retraining triggers based on new data
- Advanced dataset versioning and lineage tracking
