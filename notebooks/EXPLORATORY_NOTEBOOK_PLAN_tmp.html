<!DOCTYPE html>
<html>
<head>
<title>EXPLORATORY_NOTEBOOK_PLAN.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="exploratory-notebook-implementation-plan">Exploratory Notebook Implementation Plan</h1>
<h2 id="overview">Overview</h2>
<p>This document outlines the implementation plan for an initial exploratory notebook that:</p>
<ol>
<li>Fetches source FITS files from available APIs/integrations</li>
<li>Computes differences from previous versions (reference images)</li>
<li>Prepares data for anomaly detection in subsequent notebooks</li>
<li>Explores options for sourcing ZTF anomalies or creating synthetic anomalies</li>
</ol>
<h2 id="notebook-structure">Notebook Structure</h2>
<h3 id="phase-1-data-acquisition">Phase 1: Data Acquisition</h3>
<p><strong>Goal</strong>: Fetch FITS files from multiple sources and establish a baseline dataset</p>
<h4 id="11-source-fits-file-acquisition">1.1 Source FITS File Acquisition</h4>
<ul>
<li>
<p><strong>MAST API Integration</strong> (<code>src/adapters/external/mast.py</code>)</p>
<ul>
<li>Use <code>MASTClient.query_observations_by_position()</code> to find observations</li>
<li>Target sky regions:
<ul>
<li>Known transient regions (e.g., SN2011fe field, M101)</li>
<li>Deep field regions (COSMOS, GOODS-N)</li>
<li>Avoid galactic plane for cleaner images</li>
</ul>
</li>
<li>Fetch observations from:
<ul>
<li>Pan-STARRS1 (PS1) - deep reference images</li>
<li>HST/JWST - high-quality deep images</li>
<li>TESS - time-series data</li>
</ul>
</li>
<li>Download FITS files using <code>MASTClient.download_files()</code></li>
<li>Store locally with metadata tracking</li>
</ul>
</li>
<li>
<p><strong>SkyView Integration</strong> (<code>src/adapters/external/skyview.py</code>)</p>
<ul>
<li>Use for on-demand image cutouts</li>
<li>Surveys: DSS2, SDSS, GALEX, 2MASS, WISE</li>
<li>Standardize image size (e.g., 240x240 pixels) and pixel scale</li>
<li>Create reference image library from multiple surveys</li>
</ul>
</li>
<li>
<p><strong>PS1 Cutout Helper</strong> (<code>MASTClient.fetch_ps1_cutout()</code>)</p>
<ul>
<li>Use for quick reference image fetching</li>
<li>Already implemented with FITS/JPEG fallback</li>
<li>Good for establishing baseline reference images</li>
</ul>
</li>
</ul>
<h4 id="12-data-organization">1.2 Data Organization</h4>
<ul>
<li>
<p>Create directory structure:</p>
<pre class="hljs"><code><div>notebooks/data/exploratory/
├── source_fits/          # Original FITS files
│   ├── science/          # Current observations
│   └── reference/        # Historical reference images
├── processed/            # Processed images
│   ├── aligned/          # WCS-aligned images
│   └── normalized/       # Normalized images
├── differences/          # Difference images
│   ├── zogy/             # ZOGY algorithm results
│   └── classic/          # Simple subtraction
└── metadata/             # JSON metadata files
</div></code></pre>
</li>
<li>
<p>Track metadata for each observation:</p>
<ul>
<li>Observation ID, RA/Dec, observation time</li>
<li>Survey/instrument, filter band</li>
<li>File paths, image dimensions</li>
<li>Processing status flags</li>
</ul>
</li>
</ul>
<h3 id="phase-2-image-differencing">Phase 2: Image Differencing</h3>
<p><strong>Goal</strong>: Compute differences between current and reference images</p>
<h4 id="21-image-preprocessing">2.1 Image Preprocessing</h4>
<ul>
<li>
<p><strong>FITS I/O</strong> (<code>src/adapters/imaging/fits_io.py</code>)</p>
<ul>
<li>Use <code>FITSProcessor.read_fits()</code> to load images</li>
<li>Extract WCS information for alignment</li>
<li>Extract metadata from headers</li>
</ul>
</li>
<li>
<p><strong>Image Alignment</strong></p>
<ul>
<li>Use WCS information from <code>src/domains/preprocessing/processors/wcs_processor.py</code></li>
<li>Align images to common pixel grid</li>
<li>Handle different pixel scales and orientations</li>
<li>Use <code>astropy</code> coordinate transformations</li>
</ul>
</li>
<li>
<p><strong>Normalization</strong></p>
<ul>
<li>Background subtraction</li>
<li>Flux normalization</li>
<li>Handle different exposure times and zero points</li>
<li>Store normalization parameters</li>
</ul>
</li>
</ul>
<h4 id="22-difference-computation">2.2 Difference Computation</h4>
<ul>
<li>
<p><strong>ZOGY Algorithm</strong> (<code>src/domains/preprocessing/processors/astronomical_image_processing.py</code>)</p>
<ul>
<li>Use <code>ImageDifferencingProcessor.zogy_differencing()</code></li>
<li>Input: science image, reference image</li>
<li>Optional: PSF models, noise maps</li>
<li>Output: difference image + metrics (significance map, SNR)</li>
</ul>
</li>
<li>
<p><strong>Classic Differencing</strong> (baseline)</p>
<ul>
<li>Simple subtraction for comparison</li>
<li>Use <code>ImageDifferencingProcessor.classic_differencing()</code></li>
</ul>
</li>
<li>
<p><strong>Difference Image Quality Assessment</strong></p>
<ul>
<li>Calculate noise properties</li>
<li>Identify artifacts (cosmic rays, bad pixels)</li>
<li>Compute quality metrics:
<ul>
<li>Noise level (std dev of background)</li>
<li>Dynamic range</li>
<li>Detection threshold (SNR)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="23-difference-image-storage">2.3 Difference Image Storage</h4>
<ul>
<li>Save difference images as FITS files</li>
<li>Include metadata:
<ul>
<li>Original observation IDs</li>
<li>Processing parameters (PSF, noise estimates)</li>
<li>Quality metrics</li>
<li>Timestamps</li>
</ul>
</li>
</ul>
<h3 id="phase-3-anomaly-preparation">Phase 3: Anomaly Preparation</h3>
<p><strong>Goal</strong>: Prepare difference images for anomaly detection</p>
<h4 id="31-source-extraction">3.1 Source Extraction</h4>
<ul>
<li>
<p>Use <code>SEP</code> or <code>photutils</code> for source detection on difference images</p>
</li>
<li>
<p>Extract candidate sources:</p>
<ul>
<li>Position (x, y and RA/Dec)</li>
<li>Flux and significance (SNR)</li>
<li>Shape parameters (ellipticity, size)</li>
<li>Quality flags</li>
</ul>
</li>
<li>
<p>Filter candidates:</p>
<ul>
<li>Minimum SNR threshold (e.g., 5σ)</li>
<li>Exclude known artifacts (bad pixels, diffraction spikes)</li>
<li>Size constraints (point sources vs extended)</li>
</ul>
</li>
</ul>
<h4 id="32-image-cutouts">3.2 Image Cutouts</h4>
<ul>
<li>
<p>Extract cutouts around each candidate:</p>
<ul>
<li>Science image cutout</li>
<li>Reference image cutout</li>
<li>Difference image cutout</li>
<li>Standard size (e.g., 64x64 or 128x128 pixels)</li>
<li>Include padding for context</li>
</ul>
</li>
<li>
<p>Store cutouts:</p>
<ul>
<li>As individual FITS files or</li>
<li>In a multi-extension FITS file</li>
<li>Include metadata (position, SNR, timestamp)</li>
</ul>
</li>
</ul>
<h4 id="33-data-format-for-next-notebook">3.3 Data Format for Next Notebook</h4>
<ul>
<li>
<p>Create structured dataset:</p>
<ul>
<li>NumPy arrays or PyTorch tensors</li>
<li>Image triplets: (science, reference, difference)</li>
<li>Labels: (anomaly flag, source type, confidence)</li>
<li>Metadata: (coordinates, timestamps, quality metrics)</li>
</ul>
</li>
<li>
<p>Export formats:</p>
<ul>
<li>HDF5 file for large datasets</li>
<li>Pickle/numpy format for smaller sets</li>
<li>CSV metadata file</li>
<li>JSON configuration file</li>
</ul>
</li>
</ul>
<h3 id="phase-4-anomaly-data-sources">Phase 4: Anomaly Data Sources</h3>
<p><strong>Goal</strong>: Explore options for real and synthetic anomalies</p>
<h4 id="41-ztf-database-options">4.1 ZTF Database Options</h4>
<p><strong>Option A: ZTF Public Data Release</strong></p>
<ul>
<li>
<p><strong>ZTF Public Data Portal</strong> (IRSA)</p>
<ul>
<li>Access: https://irsa.ipac.caltech.edu/Missions/ztf.html</li>
<li>Public data releases with 6-month delay</li>
<li>Difference images and alert packets</li>
<li>Light curves for confirmed transients</li>
</ul>
</li>
<li>
<p><strong>ZTF Alert Stream</strong> (via brokers)</p>
<ul>
<li>Kowalski database (MongoDB) - query interface</li>
<li>ZTF alert schema with image cutouts</li>
<li>Requires: API access, database connection</li>
<li>Filter by: object type, magnitude, location</li>
</ul>
</li>
<li>
<p><strong>ZTF Data Products</strong></p>
<ul>
<li>Difference images: science - reference</li>
<li>Science image stamps</li>
<li>Reference image stamps</li>
<li>Metadata: RA/Dec, mag, filter, MJD</li>
</ul>
</li>
</ul>
<p><strong>Option B: ZTF via API</strong></p>
<ul>
<li>Query ZTF public catalogs:
<ul>
<li>Use <code>astroquery</code> or direct API calls</li>
<li>Filter confirmed transients/supernovae</li>
<li>Download associated image stamps</li>
<li>Cross-match with known object catalogs</li>
</ul>
</li>
</ul>
<p><strong>Option C: ZTF Simulation/Reconstruction</strong></p>
<ul>
<li>Use ZTF-like parameters:
<ul>
<li>Instrument characteristics (PSF, pixel scale)</li>
<li>Survey strategy (cadence, depth)</li>
<li>Generate synthetic ZTF-like images</li>
</ul>
</li>
</ul>
<h4 id="42-synthetic-anomaly-generation">4.2 Synthetic Anomaly Generation</h4>
<p><strong>Leverage existing code</strong> (<code>standalone_training.py</code>)</p>
<ul>
<li>
<p><strong>SyntheticAnomalicalDataset</strong> (lines 359-508)</p>
<ul>
<li>Already generates synthetic astronomical images</li>
<li>Anomaly types: transient, variable, supernova, asteroid</li>
<li>Configurable anomaly ratio and noise levels</li>
</ul>
</li>
<li>
<p><strong>Enhancement for Difference Images</strong>:</p>
<ul>
<li>Generate reference image (steady state)</li>
<li>Generate science image with injected anomaly</li>
<li>Compute difference image</li>
<li>Vary anomaly parameters:
<ul>
<li>Brightness (magnitude range)</li>
<li>Size (point source vs extended)</li>
<li>Position (random vs known locations)</li>
<li>Type (SN Ia, SN II, nova, etc.)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Realistic Anomaly Injection</strong>:</p>
<ul>
<li>Add real transient light curves</li>
<li>Use PSF models from real surveys</li>
<li>Include realistic noise characteristics</li>
<li>Match ZTF-like image properties</li>
</ul>
</li>
</ul>
<h4 id="43-hybrid-approach">4.3 Hybrid Approach</h4>
<ul>
<li>Start with synthetic anomalies for validation</li>
<li>Gradually incorporate real ZTF data as available</li>
<li>Use synthetic data for:
<ul>
<li>Training anomaly detection models</li>
<li>Testing pipeline robustness</li>
<li>Handling edge cases</li>
</ul>
</li>
<li>Use real ZTF data for:
<ul>
<li>Validation and benchmarking</li>
<li>Understanding real-world challenges</li>
<li>Model generalization testing</li>
</ul>
</li>
</ul>
<h3 id="phase-5-data-validation-and-quality-control">Phase 5: Data Validation and Quality Control</h3>
<p><strong>Goal</strong>: Ensure data quality before anomaly detection</p>
<h4 id="51-image-quality-checks">5.1 Image Quality Checks</h4>
<ul>
<li>Verify FITS file integrity</li>
<li>Check image dimensions and data types</li>
<li>Validate WCS solutions</li>
<li>Check for NaN/inf values</li>
</ul>
<h4 id="52-difference-image-validation">5.2 Difference Image Validation</h4>
<ul>
<li>Verify alignment quality (residuals)</li>
<li>Check noise characteristics (should be Gaussian)</li>
<li>Identify and flag artifacts</li>
<li>Validate source extraction results</li>
</ul>
<h4 id="53-metadata-completeness">5.3 Metadata Completeness</h4>
<ul>
<li>Ensure all required metadata present</li>
<li>Check coordinate consistency</li>
<li>Validate timestamps</li>
<li>Verify cross-references (science ↔ reference)</li>
</ul>
<h3 id="phase-6-visualization-and-exploration">Phase 6: Visualization and Exploration</h3>
<p><strong>Goal</strong>: Visualize results and understand data characteristics</p>
<h4 id="61-image-visualization">6.1 Image Visualization</h4>
<ul>
<li>Display science, reference, and difference images</li>
<li>Show source extraction results (overlay)</li>
<li>Create comparison plots (ZOGY vs classic)</li>
<li>Generate quality assessment plots</li>
</ul>
<h4 id="62-statistical-analysis">6.2 Statistical Analysis</h4>
<ul>
<li>Noise distribution analysis</li>
<li>Source detection statistics</li>
<li>Anomaly candidate distribution</li>
<li>Quality metrics distribution</li>
</ul>
<h4 id="63-data-summary">6.3 Data Summary</h4>
<ul>
<li>Generate summary statistics</li>
<li>Create data quality report</li>
<li>Document known issues/limitations</li>
<li>Prepare data catalog for next notebook</li>
</ul>
<h2 id="implementation-details">Implementation Details</h2>
<h3 id="notebook-sections">Notebook Sections</h3>
<ol>
<li>
<p><strong>Setup and Configuration</strong></p>
<ul>
<li>Import libraries</li>
<li>Set up paths and directories</li>
<li>Configure API clients</li>
<li>Set random seeds for reproducibility</li>
</ul>
</li>
<li>
<p><strong>Data Acquisition</strong></p>
<ul>
<li>Define target sky regions</li>
<li>Fetch observations from MAST/SkyView</li>
<li>Download FITS files</li>
<li>Organize and catalog files</li>
</ul>
</li>
<li>
<p><strong>Image Processing</strong></p>
<ul>
<li>Load FITS files</li>
<li>Align images using WCS</li>
<li>Normalize images</li>
<li>Quality assessment</li>
</ul>
</li>
<li>
<p><strong>Image Differencing</strong></p>
<ul>
<li>Compute ZOGY differences</li>
<li>Compute classic differences (baseline)</li>
<li>Calculate quality metrics</li>
<li>Save difference images</li>
</ul>
</li>
<li>
<p><strong>Source Extraction</strong></p>
<ul>
<li>Detect sources in difference images</li>
<li>Filter candidates</li>
<li>Extract cutouts</li>
<li>Create candidate catalog</li>
</ul>
</li>
<li>
<p><strong>Anomaly Data Preparation</strong></p>
<ul>
<li>Option 1: Query ZTF database (if available)</li>
<li>Option 2: Generate synthetic anomalies</li>
<li>Option 3: Hybrid approach</li>
<li>Prepare training/validation dataset</li>
</ul>
</li>
<li>
<p><strong>Data Export</strong></p>
<ul>
<li>Save processed images</li>
<li>Export metadata</li>
<li>Create dataset for next notebook</li>
<li>Generate summary report</li>
</ul>
</li>
<li>
<p><strong>Visualization</strong></p>
<ul>
<li>Display example images</li>
<li>Show difference images</li>
<li>Plot source detection results</li>
<li>Create quality assessment plots</li>
</ul>
</li>
</ol>
<h2 id="technical-requirements">Technical Requirements</h2>
<h3 id="dependencies">Dependencies</h3>
<ul>
<li>Existing AstrID modules:
<ul>
<li><code>src/adapters/external/mast.py</code> - MAST API client</li>
<li><code>src/adapters/external/skyview.py</code> - SkyView client</li>
<li><code>src/adapters/imaging/fits_io.py</code> - FITS I/O</li>
<li><code>src/domains/preprocessing/processors/astronomical_image_processing.py</code> - ZOGY differencing</li>
<li><code>src/domains/preprocessing/processors/fits_processing.py</code> - FITS processing</li>
</ul>
</li>
<li>External packages:
<ul>
<li><code>astropy</code> - FITS, WCS, coordinates</li>
<li><code>astroquery</code> - MAST queries</li>
<li><code>numpy</code>, <code>matplotlib</code> - Data handling, visualization</li>
<li><code>sep</code> or <code>photutils</code> - Source extraction</li>
<li><code>scipy</code> - Image processing</li>
</ul>
</li>
</ul>
<h3 id="configuration-options">Configuration Options</h3>
<ul>
<li>Target sky regions (RA/Dec, radius)</li>
<li>Survey selection (MAST missions, SkyView surveys)</li>
<li>Image size and pixel scale</li>
<li>Differencing algorithm parameters</li>
<li>Source detection thresholds</li>
<li>Anomaly generation parameters (if synthetic)</li>
</ul>
<h2 id="deliverables">Deliverables</h2>
<ol>
<li>
<p><strong>Jupyter Notebook</strong> (<code>notebooks/exploratory_data_preparation.ipynb</code>)</p>
<ul>
<li>Complete implementation of all phases</li>
<li>Well-documented with markdown cells</li>
<li>Clear section organization</li>
</ul>
</li>
<li>
<p><strong>Data Directory Structure</strong></p>
<ul>
<li>Organized FITS files</li>
<li>Processed images</li>
<li>Difference images</li>
<li>Metadata files</li>
</ul>
</li>
<li>
<p><strong>Dataset for Next Notebook</strong></p>
<ul>
<li>Processed image triplets (science, reference, difference)</li>
<li>Candidate source catalog</li>
<li>Anomaly labels (if available)</li>
<li>Metadata and configuration files</li>
</ul>
</li>
<li>
<p><strong>Documentation</strong></p>
<ul>
<li>README for the notebook</li>
<li>Data format specification</li>
<li>Known issues and limitations</li>
<li>Next steps documentation</li>
</ul>
</li>
</ol>
<h2 id="next-steps-subsequent-notebook">Next Steps (Subsequent Notebook)</h2>
<p>The output from this exploratory notebook will feed into:</p>
<ul>
<li><strong>Anomaly Detection Notebook</strong>
<ul>
<li>Load prepared dataset</li>
<li>Apply U-Net model for anomaly detection</li>
<li>Evaluate detection performance</li>
<li>Visualize results</li>
<li>Compare with ground truth (if available)</li>
</ul>
</li>
</ul>
<h2 id="notes">Notes</h2>
<ul>
<li>Start small: Begin with 10-20 observations to validate pipeline</li>
<li>Use existing code: Leverage <code>standalone_training.py</code> for synthetic data</li>
<li>API rate limits: Be mindful of MAST/SkyView rate limits</li>
<li>Data storage: Consider disk space for FITS files</li>
<li>Reproducibility: Save all random seeds and configuration</li>
<li>Error handling: Robust error handling for API failures</li>
<li>Progress tracking: Log progress for long-running operations</li>
</ul>

</body>
</html>
