{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /home/chris/github/AstrID\n",
            "‚úÖ Path setup complete\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(\"‚úÖ Path setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Base: http://127.0.0.1:8000\n",
            "Survey: name=dss2, id=2127bdee-056c-4266-b1b3-20eb879cd543\n",
            "Test Count: 10\n",
            "Date Range: 2025-01-01T00:00:00 to 2025-12-31T23:59:59\n",
            "‚úÖ Configuration complete - Ready for REAL DATA training pipeline\n"
          ]
        }
      ],
      "source": [
        "# Configuration - UPDATED FOR REAL DATA\n",
        "API_BASE = \"http://127.0.0.1:8000\"\n",
        "MLFLOW_UI = \"http://localhost:9003\"\n",
        "PREFECT_UI = \"http://localhost:9004\"\n",
        "\n",
        "# Survey selection - USE DSS2 (has real observations)\n",
        "TEST_SURVEY_NAME: Optional[str] = os.getenv(\"ASTRID_SURVEY_NAME\", \"dss2\").lower()\n",
        "ENV_MAP = {\n",
        "    \"hst\": os.getenv(\"ASTRID_HST_SURVEY_ID\", \"05e6090c-bac5-4b78-8d7d-ae15a7dde50f\"),\n",
        "    \"jwst\": os.getenv(\"ASTRID_JWST_SURVEY_ID\", \"3ae172d0-c51a-4dad-8033-9813792ce503\"),\n",
        "    \"dss2\": os.getenv(\"ASTRID_DSS2_SURVEY_ID\", \"2127bdee-056c-4266-b1b3-20eb879cd543\"),\n",
        "    \"tess\": os.getenv(\"ASTRID_TESS_SURVEY_ID\", \"49e8d057-184a-4239-9bff-9be72fbcfd02\"),\n",
        "}\n",
        "TEST_SURVEY_ID: Optional[str] = os.getenv(\"ASTRID_DSS2_SURVEY_ID\", \"\")\n",
        "\n",
        "# Training dataset configuration\n",
        "TEST_COUNT = 10  # Use more observations for training\n",
        "TEST_DATE_RANGE = {\n",
        "    \"start\": \"2025-01-01T00:00:00\",  # Updated to 2025 (when your real data was created)\n",
        "    \"end\": \"2025-12-31T23:59:59\"\n",
        "}\n",
        "\n",
        "# Authentication\n",
        "AUTH_TOKEN: Optional[str] = None\n",
        "AUTH_HEADERS: Dict[str, str] = {}\n",
        "\n",
        "print(f\"API Base: {API_BASE}\")\n",
        "print(f\"Survey: name={TEST_SURVEY_NAME}, id={TEST_SURVEY_ID}\")\n",
        "print(f\"Test Count: {TEST_COUNT}\")\n",
        "print(f\"Date Range: {TEST_DATE_RANGE['start']} to {TEST_DATE_RANGE['end']}\")\n",
        "print(\"‚úÖ Configuration complete - Ready for REAL DATA training pipeline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Authentication Cell (Copy to Other Notebooks)\n",
        "\n",
        "**Copy the cell below to any notebook that needs API authentication:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê Authentication configured for training pipeline\n",
            "API Key: astrid__v2B8H-b6qRBf...\n",
            "‚úÖ Ready to create training datasets from real observations\n"
          ]
        }
      ],
      "source": [
        "from src.core.constants import TRAINING_PIPELINE_API_KEY\n",
        "\n",
        "# Set up authentication for training pipeline\n",
        "global AUTH_HEADERS\n",
        "AUTH_HEADERS = {\n",
        "    \"X-API-Key\": TRAINING_PIPELINE_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "print(\"üîê Authentication configured for training pipeline\")\n",
        "print(f\"API Key: {TRAINING_PIPELINE_API_KEY[:20]}...\")\n",
        "print(\"‚úÖ Ready to create training datasets from real observations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Survey Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Using survey: name=dss2, id=2127bdee-056c-4266-b1b3-20eb879cd543\n"
          ]
        }
      ],
      "source": [
        "def resolve_test_survey() -> str | None:\n",
        "    \"\"\"Resolve existing survey UUID from env; do not fabricate UUIDs.\"\"\"\n",
        "    if not AUTH_HEADERS:\n",
        "        print(\"‚ùå Not authenticated. Please run the API key cell.\")\n",
        "        return None\n",
        "    if not TEST_SURVEY_ID:\n",
        "        print(\"‚ùå No survey UUID configured. Set ASTRID_SURVEY_ID or ASTRID_<NAME>_SURVEY_ID.\")\n",
        "        return None\n",
        "    print(f\"üéØ Using survey: name={TEST_SURVEY_NAME}, id={TEST_SURVEY_ID}\")\n",
        "    return TEST_SURVEY_ID\n",
        "\n",
        "# Set the survey ID\n",
        "TEST_SURVEY_ID = resolve_test_survey()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Health Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API: Healthy\n",
            "‚úÖ MLflow: Healthy\n",
            "‚úÖ Prefect: Healthy\n",
            "\n",
            "üéâ All services are healthy!\n"
          ]
        }
      ],
      "source": [
        "def check_service_health(url: str, service_name: str) -> bool:\n",
        "    \"\"\"Check if a service is healthy.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ {service_name}: Healthy\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå {service_name}: HTTP {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {service_name}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Check all services\n",
        "services = {\n",
        "    \"API\": f\"{API_BASE}/health\",\n",
        "    \"MLflow\": MLFLOW_UI,\n",
        "    \"Prefect\": PREFECT_UI\n",
        "}\n",
        "\n",
        "all_healthy = True\n",
        "for name, url in services.items():\n",
        "    if not check_service_health(url, name):\n",
        "        all_healthy = False\n",
        "\n",
        "if all_healthy:\n",
        "    print(\"\\nüéâ All services are healthy!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some services are not healthy. Please check docker-compose.\")\n",
        "    print(\"Run: docker-compose up -d api worker prefect mlflow redis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Creating training dataset from real observations...\n",
            "üéØ Creating training dataset from REAL observations...\n",
            "Survey: dss2 (2127bdee-056c-4266-b1b3-20eb879cd543)\n",
            "Date range: 2025-01-01T00:00:00 to 2025-12-31T23:59:59\n",
            "Confidence threshold: 0.1\n",
            "Max samples: 50\n",
            "‚ùå Failed to create training dataset: 404 Client Error: Not Found for url: http://127.0.0.1:8000/training/datasets/collect\n",
            "   Error details: {'detail': 'No training samples found with specified criteria'}\n",
            "\n",
            "‚ùå Training dataset creation failed: 404 Client Error: Not Found for url: http://127.0.0.1:8000/training/datasets/collect\n",
            "This might mean:\n",
            "- No observations found in the specified date range\n",
            "- API endpoint not available\n",
            "- Authentication issues\n"
          ]
        }
      ],
      "source": [
        "# CREATE TRAINING DATASET FROM REAL OBSERVATIONS\n",
        "def create_training_dataset_from_real_observations() -> Dict[str, Any]:\n",
        "    \"\"\"Create training dataset from our real observations (not mock data).\"\"\"\n",
        "    url = f\"{API_BASE}/training/datasets/collect\"\n",
        "    payload = {\n",
        "        \"survey_ids\": [TEST_SURVEY_NAME],  # Use DSS2 survey with real data\n",
        "        \"start\": TEST_DATE_RANGE[\"start\"],\n",
        "        \"end\": TEST_DATE_RANGE[\"end\"],\n",
        "        \"confidence_threshold\": 0.1,  # Very low threshold to capture all real observations\n",
        "        \"max_samples\": 50,  # Start with reasonable number\n",
        "        \"name\": f\"real_data_{TEST_SURVEY_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    }\n",
        "    \n",
        "    print(f\"üéØ Creating training dataset from REAL observations...\")\n",
        "    print(f\"Survey: {TEST_SURVEY_NAME} ({TEST_SURVEY_ID})\")\n",
        "    print(f\"Date range: {payload['start']} to {payload['end']}\")\n",
        "    print(f\"Confidence threshold: {payload['confidence_threshold']}\")\n",
        "    print(f\"Max samples: {payload['max_samples']}\")\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=AUTH_HEADERS, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        result = response.json()\n",
        "        dataset_info = result.get(\"data\", {})\n",
        "        \n",
        "        print(f\"‚úÖ Successfully created training dataset from real observations!\")\n",
        "        print(f\"   - Dataset ID: {dataset_info.get('dataset_id')}\")\n",
        "        print(f\"   - Name: {dataset_info.get('name')}\")\n",
        "        print(f\"   - Total samples: {dataset_info.get('total')}\")\n",
        "        print(f\"   - Quality score: {dataset_info.get('quality', {}).get('quality_score', 0):.3f}\")\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"dataset_id\": dataset_info.get(\"dataset_id\"),\n",
        "            \"dataset_info\": dataset_info\n",
        "        }\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Failed to create training dataset: {str(e)}\")\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            try:\n",
        "                error_detail = e.response.json()\n",
        "                print(f\"   Error details: {error_detail}\")\n",
        "            except:\n",
        "                print(f\"   Response: {e.response.text}\")\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Create training dataset from real observations\n",
        "print(\"üöÄ Creating training dataset from real observations...\")\n",
        "dataset_result = create_training_dataset_from_real_observations()\n",
        "\n",
        "if dataset_result[\"success\"]:\n",
        "    dataset_id = dataset_result[\"dataset_id\"]\n",
        "    print(f\"\\nüéâ Training dataset created successfully!\")\n",
        "    print(f\"Dataset ID: {dataset_id}\")\n",
        "    print(f\"\\nNext steps:\")\n",
        "    print(f\"1. ‚úÖ Real observations collected into training dataset\")\n",
        "    print(f\"2. üîÑ Run preprocessing pipeline on observations\")\n",
        "    print(f\"3. üîÑ Run differencing pipeline to create difference images\")\n",
        "    print(f\"4. üîÑ Run detection pipeline with U-Net model\")\n",
        "    print(f\"5. üéØ Train ML models on processed data\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training dataset creation failed: {dataset_result['error']}\")\n",
        "    print(f\"This might mean:\")\n",
        "    print(f\"- No observations found in the specified date range\")\n",
        "    print(f\"- API endpoint not available\")\n",
        "    print(f\"- Authentication issues\")\n",
        "    dataset_id = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consume real_data_manifest.json and print available samples\\nimport json\\nfrom pathlib import Path\\n\\nmanifest_path = Path.cwd().parent / \"ml_training_data\" / \"real_data_loading.ipynb\"  # notebook path\\n# Manifest sits next to the real_data_loading notebook\\nman_path = (Path.cwd().parent / \"ml_training_data\" / \"real_data_manifest.json\")\\nif not man_path.exists():\\n    raise FileNotFoundError(f\"Manifest not found: {man_path}\")\\nmanifest = json.loads(man_path.read_text())\\nprint(f\"Found {len(manifest)} entries in manifest\")\\nprint(manifest[:3])\\n\\n# TODO: Hand off to training service or create TrainingDataset via API from manifest entries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Manifest not found: /home/chris/github/AstrID/ml_training_data/real_data_manifest.json",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m man_path = (Path.cwd().parent / \u001b[33m\"\u001b[39m\u001b[33mml_training_data\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mreal_data_manifest.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m man_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mManifest not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mman_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m manifest = json.loads(man_path.read_text())\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# naive split 80/20\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Manifest not found: /home/chris/github/AstrID/ml_training_data/real_data_manifest.json"
          ]
        }
      ],
      "source": [
        "# Build simple training list from manifest entries\n",
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "\n",
        "train_items: List[Dict] = []\n",
        "val_items: List[Dict] = []\n",
        "\n",
        "manifest_path = Path.cwd().parent / \"ml_training_data\" / \"real_data_loading.ipynb\"  # notebook path\n",
        "man_path = (Path.cwd().parent / \"ml_training_data\" / \"real_data_manifest.json\")\n",
        "if not man_path.exists():\n",
        "    raise FileNotFoundError(f\"Manifest not found: {man_path}\")\n",
        "manifest = json.loads(man_path.read_text())\n",
        "\n",
        "# naive split 80/20\n",
        "cut = max(1, int(0.8 * len(manifest)))\n",
        "train_items = manifest[:cut]\n",
        "val_items = manifest[cut:]\n",
        "\n",
        "print(f\"Train: {len(train_items)} | Val: {len(val_items)}\")\n",
        "\n",
        "# Preview a few items with full R2 URL for convenience\n",
        "endpoint = os.getenv(\"MLFLOW_S3_ENDPOINT_URL\", \"\")\n",
        "def to_url(item: Dict) -> str:\n",
        "    if endpoint:\n",
        "        return f\"{endpoint}/{item['bucket']}/{item['key']}\"\n",
        "    return f\"s3://{item['bucket']}/{item['key']}\"\n",
        "\n",
        "# Preview a few training samples with their R2 URLs\n",
        "for sample in train_items[:3]:\n",
        "    print(to_url(sample))\n",
        "\n",
        "# Prepare minimal dataset payload from manifest entries\n",
        "from datetime import datetime\n",
        "\n",
        "items = manifest  # manifest is already loaded above\n",
        "\n",
        "dataset_payload = {\n",
        "    \"name\": f\"skyview_manifest_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "    \"description\": \"SkyView cutouts uploaded to R2 for training\",\n",
        "    \"total\": len(items),\n",
        "    \"items\": items[:10],  # keep it small for a dry run\n",
        "}\n",
        "\n",
        "print(\"Dataset payload preview (first 1 item):\")\n",
        "print(json.dumps({**dataset_payload, \"items\": dataset_payload[\"items\"][:1]}, indent=2)[:800])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Ingest Test Observations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_test_observations() -> Dict[str, Any]:\n",
        "    \"\"\"Ingest test observations using the API.\"\"\"\n",
        "    # AUTH_HEADERS should already be set globally in cell 6\n",
        "    if not AUTH_HEADERS:\n",
        "        print(\"‚ùå AUTH_HEADERS not set. Please run the authentication cell first.\")\n",
        "        return {\"success\": False, \"error\": \"AUTH_HEADERS not set. Please run the authentication cell first.\"}\n",
        "    \n",
        "    url = f\"{API_BASE}/observations/ingest/batch-random\"\n",
        "    payload = {\n",
        "        \"count\": TEST_COUNT,\n",
        "        \"survey_id\": TEST_SURVEY_ID\n",
        "    }\n",
        "    \n",
        "    print(f\"Ingesting {TEST_COUNT} test observations for survey {TEST_SURVEY_ID}...\")\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=AUTH_HEADERS, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        result = response.json()\n",
        "        observations = result.get(\"data\", [])\n",
        "        \n",
        "        print(f\"‚úÖ Successfully ingested {len(observations)} observations\")\n",
        "        \n",
        "        # Store observation IDs for later use\n",
        "        observation_ids = [obs[\"id\"] for obs in observations]\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"count\": len(observations),\n",
        "            \"observation_ids\": observation_ids,\n",
        "            \"observations\": observations\n",
        "        }\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Failed to ingest observations: {str(e)}\")\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            try:\n",
        "                error_detail = e.response.json()\n",
        "                print(f\"   Error details: {error_detail}\")\n",
        "            except:\n",
        "                print(f\"   Response: {e.response.text}\")\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Run ingestion\n",
        "ingestion_result = ingest_test_observations()\n",
        "\n",
        "if ingestion_result[\"success\"]:\n",
        "    print(f\"\\nObservation IDs: {ingestion_result['observation_ids']}\")\n",
        "    # Store for later steps\n",
        "    observation_ids = ingestion_result[\"observation_ids\"]\n",
        "else:\n",
        "    print(\"\\n‚ùå Cannot proceed without observations\")\n",
        "    observation_ids = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_dataset() -> Dict[str, Any]:\n",
        "    \"\"\"Create training dataset from the processed detections.\"\"\"\n",
        "    url = f\"{API_BASE}/training/datasets/collect\"\n",
        "    payload = {\n",
        "        \"survey_ids\": [TEST_SURVEY_ID],\n",
        "        \"start\": TEST_DATE_RANGE[\"start\"],\n",
        "        \"end\": TEST_DATE_RANGE[\"end\"],\n",
        "        \"confidence_threshold\": 0.3,  # Lower threshold for testing\n",
        "        \"max_samples\": 100,\n",
        "        \"name\": f\"test_{TEST_SURVEY_ID}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    }\n",
        "    \n",
        "    print(f\"Creating training dataset for survey {TEST_SURVEY_ID}...\")\n",
        "    print(f\"Date range: {payload['start']} to {payload['end']}\")\n",
        "    print(f\"Confidence threshold: {payload['confidence_threshold']}\")\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=AUTH_HEADERS, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        result = response.json()\n",
        "        dataset_info = result.get(\"data\", {})\n",
        "        \n",
        "        print(f\"‚úÖ Successfully created training dataset\")\n",
        "        print(f\"   - Dataset ID: {dataset_info.get('dataset_id')}\")\n",
        "        print(f\"   - Name: {dataset_info.get('name')}\")\n",
        "        print(f\"   - Total samples: {dataset_info.get('total')}\")\n",
        "        print(f\"   - Quality score: {dataset_info.get('quality', {}).get('quality_score', 0):.3f}\")\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"dataset_id\": dataset_info.get(\"dataset_id\"),\n",
        "            \"dataset_info\": dataset_info\n",
        "        }\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Failed to create training dataset: {str(e)}\")\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Create training dataset\n",
        "dataset_result = create_training_dataset()\n",
        "\n",
        "if dataset_result[\"success\"]:\n",
        "    dataset_id = dataset_result[\"dataset_id\"]\n",
        "    print(f\"\\nüéâ Training dataset created successfully!\")\n",
        "    print(f\"Dataset ID: {dataset_id}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training dataset creation failed: {dataset_result['error']}\")\n",
        "    dataset_id = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_training_datasets() -> Dict[str, Any]:\n",
        "    \"\"\"List all available training datasets.\"\"\"\n",
        "    url = f\"{API_BASE}/training/datasets\"\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url, headers=AUTH_HEADERS, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        result = response.json()\n",
        "        datasets = result.get(\"data\", [])\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(datasets)} training datasets\")\n",
        "        \n",
        "        for i, dataset in enumerate(datasets, 1):\n",
        "            print(f\"\\n{i}. {dataset.get('name')}\")\n",
        "            print(f\"   - ID: {dataset.get('id')}\")\n",
        "            print(f\"   - Samples: {dataset.get('total_samples')}\")\n",
        "            print(f\"   - Quality: {dataset.get('quality_score', 0):.3f}\")\n",
        "            print(f\"   - Status: {dataset.get('status')}\")\n",
        "            print(f\"   - Created: {dataset.get('created_at')}\")\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"datasets\": datasets\n",
        "        }\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Failed to list datasets: {str(e)}\")\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# List all datasets\n",
        "list_result = list_training_datasets()\n",
        "\n",
        "if list_result[\"success\"]:\n",
        "    datasets = list_result[\"datasets\"]\n",
        "    \n",
        "    # Find datasets with samples\n",
        "    datasets_with_samples = [d for d in datasets if d.get(\"total_samples\", 0) > 0]\n",
        "    \n",
        "    if datasets_with_samples:\n",
        "        print(f\"\\nüéâ Found {len(datasets_with_samples)} datasets with samples!\")\n",
        "        print(f\"\\nReady for training:\")\n",
        "        for dataset in datasets_with_samples:\n",
        "            print(f\"- {dataset['name']} (ID: {dataset['id']}) - {dataset['total_samples']} samples\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  No datasets have samples yet.\")\n",
        "        print(f\"This suggests the data pipeline needs to be run first.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Failed to list datasets: {list_result['error']}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
