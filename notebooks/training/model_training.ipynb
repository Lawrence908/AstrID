{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AstrID Model Training Notebook\n",
        "\n",
        "## U-Net Anomaly Detection with MLflow Integration\n",
        "\n",
        "This notebook provides a comprehensive training pipeline for the U-Net anomaly detection model with:\n",
        "- Complete MLflow experiment tracking\n",
        "- GPU energy monitoring (ASTR-101)\n",
        "- Comprehensive performance metrics (ASTR-102)\n",
        "- Data preprocessing integration\n",
        "- Visualization and debugging tools\n",
        "\n",
        "**Project**: ASTR-106 - Training Notebook for Model Training and MLflow Logging  \n",
        "**Dependencies**: ASTR-88 (MLflow Integration) ‚úÖ, ASTR-80 (U-Net Model) ‚úÖ, ASTR-76 (Preprocessing) ‚úÖ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Environment Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-23 23:53:36.453398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Project root: /home/chris/github/AstrID/notebooks\n",
            "‚úÖ Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import sys\n",
        "import asyncio\n",
        "import logging\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from dataclasses import dataclass, field\n",
        "from uuid import uuid4\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Scientific computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix,\n",
        "    classification_report, matthews_corrcoef\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "# MLflow and tracking\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# AstrID imports\n",
        "from src.infrastructure.mlflow import MLflowConfig, ExperimentTracker, ModelRegistry\n",
        "from src.core.gpu_monitoring import GPUPowerMonitor, EnergyConsumption\n",
        "from src.core.mlflow_energy import MLflowEnergyTracker\n",
        "from src.core.energy_analysis import EnergyAnalyzer\n",
        "from src.domains.preprocessing.processors.astronomical_image_processing import AstronomicalImageProcessor\n",
        "from src.adapters.ml.unet import UNetModel\n",
        "from src.domains.detection.models import Model, ModelRun\n",
        "from src.domains.detection.metrics.detection_metrics import DetectionMetrics\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "print(f\"‚úÖ Project root: {project_root}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.core.constants import TRAINING_PIPELINE_API_KEY\n",
        "\n",
        "global AUTH_HEADERS\n",
        "AUTH_HEADERS = {\n",
        "    \"X-API-Key\": TRAINING_PIPELINE_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç MLflow tracking URI: postgresql+asyncpg://postgres.piqpfeytatilqmzgpaei:SPXQgzx4xwuBVFmJ@aws-1-us-west-1.pooler.supabase.com/postgres\n",
            "üîç MLflow environment variables:\n",
            "   MLFLOW_SUPABASE_HOST: aws-1-us-west-1.pooler.supabase.com\n",
            "   MLFLOW_SUPABASE_PROJECT_REF: piqpfeytatilqmzgpaei\n",
            "   MLFLOW_SUPABASE_PASSWORD: Set\n"
          ]
        }
      ],
      "source": [
        "# Debug: Check MLflow configuration\n",
        "import os\n",
        "from src.core.constants import get_mlflow_tracking_uri\n",
        "\n",
        "print(f\"üîç MLflow tracking URI: {get_mlflow_tracking_uri()}\")\n",
        "print(f\"üîç MLflow environment variables:\")\n",
        "print(f\"   MLFLOW_SUPABASE_HOST: {os.getenv('MLFLOW_SUPABASE_HOST', 'Not set')}\")\n",
        "print(f\"   MLFLOW_SUPABASE_PROJECT_REF: {os.getenv('MLFLOW_SUPABASE_PROJECT_REF', 'Not set')}\")\n",
        "print(f\"   MLFLOW_SUPABASE_PASSWORD: {'Set' if os.getenv('MLFLOW_SUPABASE_PASSWORD') else 'Not set'}\")\n",
        "\n",
        "# Check if we should use a different approach\n",
        "if not get_mlflow_tracking_uri() or get_mlflow_tracking_uri() == \"postgresql+asyncpg://postgres:None@aws-0-us-west-1.pooler.supabase.com:5432/postgres\":\n",
        "    print(\"‚ö†Ô∏è  MLflow environment variables not set, using local SQLite backend\")\n",
        "    print(\"üí° Consider setting MLFLOW_SUPABASE_* environment variables for production\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Current file location: /home/chris/github/AstrID/notebooks/training\n",
            "‚úÖ Added to Python path: /home/chris/github/AstrID\n",
            "‚úÖ Added project root to Python path: /home/chris/github\n",
            "‚úÖ Current working directory: /home/chris/github/AstrID/notebooks/training\n",
            "‚úÖ Python path includes notebooks: ['/home/chris/github/AstrID/notebooks', '/home/chris/github/AstrID/notebooks']\n",
            "‚úÖ Successfully imported notebooks module\n"
          ]
        }
      ],
      "source": [
        "# Add notebooks directory to Python path for imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the absolute path to the notebooks directory\n",
        "# This works regardless of where the notebook is run from\n",
        "current_file = Path(__file__) if '__file__' in globals() else Path.cwd()\n",
        "notebooks_dir = current_file.parent.parent  # Go up two levels to get to notebooks/\n",
        "\n",
        "# Add both the notebooks directory and the project root to Python path\n",
        "sys.path.insert(0, str(notebooks_dir))\n",
        "sys.path.insert(0, str(notebooks_dir.parent))  # Also add project root\n",
        "\n",
        "print(f\"‚úÖ Current file location: {current_file}\")\n",
        "print(f\"‚úÖ Added to Python path: {notebooks_dir}\")\n",
        "print(f\"‚úÖ Added project root to Python path: {notebooks_dir.parent}\")\n",
        "print(f\"‚úÖ Current working directory: {Path.cwd()}\")\n",
        "print(f\"‚úÖ Python path includes notebooks: {[p for p in sys.path if 'notebooks' in p]}\")\n",
        "\n",
        "# Test the import\n",
        "try:\n",
        "    import notebooks\n",
        "    print(\"‚úÖ Successfully imported notebooks module\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import notebooks: {e}\")\n",
        "    print(\"üí° Trying alternative approach...\")\n",
        "    \n",
        "    # Alternative: Add the specific path\n",
        "    training_utils_path = notebooks_dir / \"training\" / \"utils\"\n",
        "    sys.path.insert(0, str(training_utils_path))\n",
        "    print(f\"‚úÖ Added training utils path: {training_utils_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Training configuration initialized: training_run_20250923_235339\n"
          ]
        }
      ],
      "source": [
        "from src.core.constants import get_mlflow_tracking_uri, MLFLOW_S3_ENDPOINT_URL, MLFLOW_BUCKET_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Comprehensive training configuration.\"\"\"\n",
        "    \n",
        "    # Experiment settings\n",
        "    experiment_name: str = \"unet_anomaly_detection\"\n",
        "    experiment_id: str = \"\"\n",
        "    run_name: str = f\"training_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    # Model architecture\n",
        "    model_name: str = \"unet_anomaly_detector\"\n",
        "    input_channels: int = 1\n",
        "    output_channels: int = 1\n",
        "    input_size: Tuple[int, int] = (512, 512)\n",
        "    initial_filters: int = 64\n",
        "    depth: int = 4\n",
        "    \n",
        "    # Training parameters\n",
        "    batch_size: int = 2\n",
        "    learning_rate: float = 0.001\n",
        "    num_epochs: int = 100\n",
        "    weight_decay: float = 1e-4\n",
        "    gradient_clip_norm: float = 1.0\n",
        "    \n",
        "    # Data parameters\n",
        "    validation_split: float = 0.2\n",
        "    test_split: float = 0.1\n",
        "    \n",
        "    # Training strategy\n",
        "    early_stopping_patience: int = 10\n",
        "    checkpoint_frequency: int = 5\n",
        "    \n",
        "    # MLflow parameters\n",
        "    mlflow_tracking_uri: str = \"http://localhost:5000\"\n",
        "    database_url: str = get_mlflow_tracking_uri()\n",
        "    mlflow_bucket_name: str = (MLFLOW_BUCKET_NAME or \"astrid-models\")\n",
        "    mlflow_endpoint_url: str = (MLFLOW_S3_ENDPOINT_URL or \"\")\n",
        "    mlflow_access_key_id: str = (AWS_ACCESS_KEY_ID or \"\")\n",
        "    mlflow_secret_access_key: str = (AWS_SECRET_ACCESS_KEY or \"\")\n",
        "    mlflow_region: str = (AWS_DEFAULT_REGION or \"auto\")\n",
        "    \n",
        "    # Artifact root\n",
        "    artifact_root: str = f\"s3://{mlflow_bucket_name}\"\n",
        "    \n",
        "    \n",
        "    # Energy tracking\n",
        "    enable_energy_tracking: bool = True\n",
        "    gpu_power_sampling_hz: float = 1.0\n",
        "    carbon_intensity_kg_per_kwh: float = 0.233\n",
        "    \n",
        "    # Performance metrics\n",
        "    confidence_threshold: float = 0.5\n",
        "    \n",
        "    # Tags for MLflow\n",
        "    tags: Dict[str, str] = field(default_factory=lambda: {\n",
        "        \"model_type\": \"unet\",\n",
        "        \"task\": \"anomaly_detection\",\n",
        "        \"dataset\": \"astronomical_images\",\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"gpu_tracking\": \"enabled\"\n",
        "    })\n",
        "\n",
        "# Initialize configuration\n",
        "config = TrainingConfig()\n",
        "print(f\"üìã Training configuration initialized: {config.run_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MLflow Setup and Experiment Tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Python paths configured for imports\n"
          ]
        }
      ],
      "source": [
        "# Set up Python path for imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add paths for imports\n",
        "sys.path.insert(0, str(Path.cwd() / \"utils\"))  # For utility files\n",
        "sys.path.insert(0, str(Path.cwd().parent.parent))  # For src modules\n",
        "print(\"‚úÖ Python paths configured for imports\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç MLflow configuration: MLflowConfig(tracking_uri='http://localhost:5000', artifact_root='s3://astrid-models', database_url='postgresql+asyncpg://postgres.piqpfeytatilqmzgpaei:SPXQgzx4xwuBVFmJ@aws-1-us-west-1.pooler.supabase.com/postgres', authentication_enabled=False, model_registry_enabled=True, experiment_auto_logging=True, artifact_compression=True, max_artifact_size=104857600, server_host='0.0.0.0', server_port=5000, server_workers=4, server_timeout=120, auth_config={}, storage_config=None)\n",
            "üîç MLflow client: <mlflow.tracking.client.MlflowClient object at 0x7f568ef7aed0>\n",
            "üîç MLflow tracking URI: http://localhost:5000\n",
            "‚úÖ Created new experiment: unet_anomaly_detection\n",
            "üî¨ Experiment ID: 2\n"
          ]
        }
      ],
      "source": [
        "# Initialize MLflow configuration\n",
        "mlflow_config = MLflowConfig(\n",
        "    tracking_uri=config.mlflow_tracking_uri,\n",
        "    artifact_root=config.artifact_root,\n",
        "    database_url=config.database_url\n",
        ")\n",
        "\n",
        "print(f\"üîç MLflow configuration: {mlflow_config}\")\n",
        "\n",
        "# Initialize MLflow components\n",
        "experiment_tracker = ExperimentTracker(mlflow_config)\n",
        "model_registry = ModelRegistry(mlflow_config)\n",
        "mlflow_client = MlflowClient(tracking_uri=config.mlflow_tracking_uri)\n",
        "\n",
        "print(f\"üîç MLflow client: {mlflow_client}\")\n",
        "\n",
        "# Set MLflow tracking URI\n",
        "mlflow.set_tracking_uri(config.mlflow_tracking_uri)\n",
        "\n",
        "print(f\"üîç MLflow tracking URI: {config.mlflow_tracking_uri}\")\n",
        "\n",
        "# Create or get experiment\n",
        "try:\n",
        "    experiment_id = experiment_tracker.create_experiment(\n",
        "        name=config.experiment_name,\n",
        "        description=\"U-Net anomaly detection training experiments\"\n",
        "    )\n",
        "    print(f\"‚úÖ Created new experiment: {config.experiment_name}\")\n",
        "except Exception as e:\n",
        "    # Get existing experiment\n",
        "    experiment = mlflow_client.get_experiment_by_name(config.experiment_name)\n",
        "    if experiment:\n",
        "        experiment_id = experiment.experiment_id\n",
        "        print(f\"‚úÖ Using existing experiment: {config.experiment_name}\")\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "print(f\"üî¨ Experiment ID: {experiment_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ REAL DATA INTEGRATION ENABLED!\n",
            "   Now training with actual astronomical observations and validated detections!\n",
            "   This enables meaningful GPU utilization and energy tracking.\n",
            "\n",
            "üìä Real Data Configuration:\n",
            "   Survey IDs: ['hst', 'jwst', 'skyview']\n",
            "   Confidence threshold: 0.6\n",
            "   Max samples: 100\n",
            "   Date range: 365 days\n",
            "\n",
            "üîç Attempting to load real astronomical data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:notebooks.training.utils.real_data_utils:Error loading real data: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SUCCESS: Real data loaded successfully!\n",
            "   üìä Data splits: Train=70, Val=15, Test=15\n",
            "   üî¨ Sample batch shape: torch.Size([2, 1, 64, 64])\n",
            "   üìà Value range: [-1.000, 2.095]\n",
            "   üéØ Mask coverage: -8162 positive pixels\n",
            "\n",
            "‚úÖ Data loading complete!\n",
            "üéâ Training will use REAL astronomical observations!\n",
            "   - GPU utilization should reach ~80-100%\n",
            "   - Energy tracking will show meaningful consumption\n",
            "   - Training on validated astronomical detections\n"
          ]
        }
      ],
      "source": [
        "## üåü REAL DATA INTEGRATION (ASTR-113) üåü\n",
        "print(\"üöÄ REAL DATA INTEGRATION ENABLED!\")\n",
        "print(\"   Now training with actual astronomical observations and validated detections!\")\n",
        "print(\"   This enables meaningful GPU utilization and energy tracking.\")\n",
        "print()\n",
        "\n",
        "# Import both real data utilities and fallback synthetic data\n",
        "from notebooks.training.utils.real_data_utils import (\n",
        "    RealDataConfig, \n",
        "    load_real_training_data, \n",
        "    create_real_data_loaders,\n",
        "    get_real_training_data\n",
        ")\n",
        "from notebooks.training.utils.training_utils import AstronomicalDataset, create_data_transforms, load_sample_data\n",
        "\n",
        "# Configure real data collection\n",
        "real_data_config = RealDataConfig(\n",
        "    survey_ids=[\"hst\", \"jwst\", \"skyview\"],  # Multiple survey sources\n",
        "    confidence_threshold=0.6,  # Lower threshold to get more samples\n",
        "    max_samples=config.batch_size * 50,  # Reasonable size for demo\n",
        "    date_range_days=365,  # Last year of data\n",
        "    validation_status=\"validated\",  # Prefer validated detections\n",
        "    anomaly_types=None,  # Include all anomaly types\n",
        ")\n",
        "\n",
        "print(f\"üìä Real Data Configuration:\")\n",
        "print(f\"   Survey IDs: {real_data_config.survey_ids}\")\n",
        "print(f\"   Confidence threshold: {real_data_config.confidence_threshold}\")\n",
        "print(f\"   Max samples: {real_data_config.max_samples}\")\n",
        "print(f\"   Date range: {real_data_config.date_range_days} days\")\n",
        "print()\n",
        "\n",
        "# Try to load real astronomical data\n",
        "try:\n",
        "    print(\"üîç Attempting to load real astronomical data...\")\n",
        "    \n",
        "    # Load real datasets\n",
        "    train_dataset, val_dataset, test_dataset = await load_real_training_data(\n",
        "        config=real_data_config,\n",
        "        dataset_name=f\"real_training_{config.run_name}\",\n",
        "        created_by=\"training_notebook\"\n",
        "    )\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader, val_loader, test_loader = create_real_data_loaders(\n",
        "        train_dataset, val_dataset, test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ SUCCESS: Real data loaded successfully!\")\n",
        "    print(f\"   üìä Data splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "    \n",
        "    # Verify real data by sampling a batch\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    sample_images, sample_masks = sample_batch\n",
        "    print(f\"   üî¨ Sample batch shape: {sample_images.shape}\")\n",
        "    print(f\"   üìà Value range: [{sample_images.min():.3f}, {sample_images.max():.3f}]\")\n",
        "    print(f\"   üéØ Mask coverage: {sample_masks.sum().item():.0f} positive pixels\")\n",
        "    \n",
        "    # Set flag for real data usage\n",
        "    USING_REAL_DATA = True\n",
        "    real_data_info = {\n",
        "        \"dataset_config\": real_data_config.__dict__,\n",
        "        \"train_samples\": len(train_dataset),\n",
        "        \"val_samples\": len(val_dataset),\n",
        "        \"test_samples\": len(test_dataset),\n",
        "    }\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Real data loading failed: {e}\")\n",
        "    print(\"üìã Falling back to synthetic data generation...\")\n",
        "    \n",
        "    # Fallback to synthetic data\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    sample_images, sample_masks = load_sample_data(\n",
        "        num_samples=200, \n",
        "        image_size=config.input_size\n",
        "    )\n",
        "    \n",
        "    train_transform, val_transform = create_data_transforms()\n",
        "    \n",
        "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "        sample_images, sample_masks, \n",
        "        test_size=config.validation_split + config.test_split, \n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    val_images, test_images, val_masks, test_masks = train_test_split(\n",
        "        val_images, val_masks,\n",
        "        test_size=config.test_split / (config.validation_split + config.test_split),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = AstronomicalDataset(train_images, train_masks, transform=train_transform)\n",
        "    val_dataset = AstronomicalDataset(val_images, val_masks, transform=val_transform)\n",
        "    test_dataset = AstronomicalDataset(test_images, test_masks, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"üìä Synthetic data splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "    \n",
        "    USING_REAL_DATA = False\n",
        "    real_data_info = None\n",
        "\n",
        "print()\n",
        "print(\"‚úÖ Data loading complete!\")\n",
        "if USING_REAL_DATA:\n",
        "    print(\"üéâ Training will use REAL astronomical observations!\")\n",
        "    print(\"   - GPU utilization should reach ~80-100%\")\n",
        "    print(\"   - Energy tracking will show meaningful consumption\")\n",
        "    print(\"   - Training on validated astronomical detections\")\n",
        "else:\n",
        "    print(\"üîÑ Training will use synthetic data (fallback mode)\")\n",
        "    print(\"   - Consider checking database connectivity\")\n",
        "    print(\"   - Or add some real observations to the database\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GPU Energy Tracking Setup (ASTR-101)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîã GPU energy tracking initialized\n",
            "üéÆ GPU available: NVIDIA GeForce RTX 3080 (Count: 1)\n"
          ]
        }
      ],
      "source": [
        "# Initialize GPU energy tracking\n",
        "if config.enable_energy_tracking:\n",
        "    gpu_monitor = GPUPowerMonitor(\n",
        "        sampling_interval=1.0 / config.gpu_power_sampling_hz,\n",
        "        carbon_intensity_kg_per_kwh=config.carbon_intensity_kg_per_kwh\n",
        "    )\n",
        "    \n",
        "    energy_tracker = MLflowEnergyTracker(\n",
        "        experiment_name=config.experiment_name\n",
        "    )\n",
        "    \n",
        "    energy_analyzer = EnergyAnalyzer()\n",
        "    \n",
        "    print(\"üîã GPU energy tracking initialized\")\n",
        "    \n",
        "    # Check GPU availability\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"üéÆ GPU available: {gpu_name} (Count: {gpu_count})\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No GPU available - energy tracking will use simulation mode\")\n",
        "else:\n",
        "    gpu_monitor = None\n",
        "    energy_tracker = None\n",
        "    energy_analyzer = None\n",
        "    print(\"‚ö° Energy tracking disabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Loading and Preprocessing Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training utilities imported\n",
            "‚úÖ Comprehensive metrics calculator imported\n",
            "‚úÖ Training manager imported\n",
            "üìä Loading sample astronomical data...\n",
            "‚úÖ Loaded 200 samples\n",
            "‚úÖ Data transforms created\n",
            "üìä Data splits: Train=139, Val=40, Test=21\n",
            "‚úÖ Data loading complete\n"
          ]
        }
      ],
      "source": [
        "# Import training utilities\n",
        "from notebooks.training.utils.training_utils import (\n",
        "    AstronomicalDataset, create_data_transforms, load_sample_data\n",
        ")\n",
        "print(\"‚úÖ Training utilities imported\")\n",
        "from notebooks.training.utils.performance_metrics import ComprehensiveMetricsCalculator\n",
        "print(\"‚úÖ Comprehensive metrics calculator imported\")\n",
        "from notebooks.training.utils.training_manager import TrainingManager\n",
        "print(\"‚úÖ Training manager imported\")\n",
        "\n",
        "# Load sample data\n",
        "print(\"üìä Loading sample astronomical data...\")\n",
        "sample_images, sample_masks = load_sample_data(\n",
        "    num_samples=200, \n",
        "    image_size=config.input_size\n",
        ")\n",
        "print(f\"‚úÖ Loaded {len(sample_images)} samples\")\n",
        "\n",
        "# Create data transforms\n",
        "train_transform, val_transform = create_data_transforms()\n",
        "print(\"‚úÖ Data transforms created\")\n",
        "\n",
        "# Create datasets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    sample_images, sample_masks, \n",
        "    test_size=config.validation_split + config.test_split, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(\n",
        "    val_images, val_masks,\n",
        "    test_size=config.test_split / (config.validation_split + config.test_split),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = AstronomicalDataset(\n",
        "    train_images, train_masks, \n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = AstronomicalDataset(\n",
        "    val_images, val_masks, \n",
        "    transform=val_transform\n",
        ")\n",
        "test_dataset = AstronomicalDataset(\n",
        "    test_images, test_masks, \n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"üìä Data splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "print(\"‚úÖ Data loading complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Architecture and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è  Model Architecture:\n",
            "   Total parameters: 22,388,033\n",
            "   Trainable parameters: 22,388,033\n",
            "   Model size: 85.40 MB\n",
            "‚úÖ Model, loss function, and optimizer initialized\n"
          ]
        }
      ],
      "source": [
        "# Define U-Net model architecture\n",
        "from src.domains.detection.architectures.unet_torch import UNet\n",
        "\n",
        "model = UNet(\n",
        "    in_channels=config.input_channels,\n",
        "    out_channels=config.output_channels,\n",
        "    initial_filters=config.initial_filters,\n",
        "    depth=config.depth,\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"üèóÔ∏è  Model Architecture:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr=config.learning_rate, \n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    mode='min', \n",
        "    factor=0.5, \n",
        "    patience=5\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model, loss function, and optimizer initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comprehensive Training with MLflow and Energy Tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-23 23:55:53,033 - root - WARNING - Sentry DSN not provided, error tracking disabled\n",
            "2025-09-23 23:55:53,036 - root - INFO - Logging initialized for development environment\n",
            "2025-09-23 23:55:53,037 - astrid.domains.detection.metrics - INFO - Domain logger initialized for detection.metrics\n",
            "2025-09-23 23:55:53,038 - notebooks.training.utils.training_manager - INFO - Using device: cuda\n",
            "2025-09-23 23:55:53,093 - notebooks.training.utils.training_manager - INFO - üîï Disabled MLflow auto-logging to prevent duplicate runs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting comprehensive training with full tracking...\n",
            "   - MLflow experiment tracking: ‚úÖ\n",
            "   - GPU energy monitoring: ‚úÖ\n",
            "   - Performance metrics (ASTR-102): ‚úÖ\n",
            "   - Model checkpointing: ‚úÖ\n",
            "   - Real data integration: ‚úÖ\n",
            "   üåü REAL DATA FEATURES ENABLED:\n",
            "      ‚Ä¢ Training on validated astronomical detections\n",
            "      ‚Ä¢ Meaningful GPU utilization expected\n",
            "      ‚Ä¢ Real energy consumption tracking\n",
            "      ‚Ä¢ Authentic astronomical image patches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-23 23:58:01,100 - urllib3.connectionpool - WARNING - Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=5000): Read timed out. (read timeout=120)\")': /api/2.0/mlflow/experiments/get-by-name?experiment_name=2\n",
            "2025-09-23 23:58:02,144 - src.infrastructure.mlflow.experiment_tracker - INFO - Started run 'training_run_20250923_235339' with ID: b9edf1cb18304c9a9622b0ab32129e6a\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÉ View run training_run_20250923_235339 at: http://localhost:5000/#/experiments/3/runs/b9edf1cb18304c9a9622b0ab32129e6a\n",
            "üß™ View experiment at: http://localhost:5000/#/experiments/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-23 23:58:09,339 - src.core.gpu_monitoring - INFO - Starting GPU power monitoring for 1 GPUs\n",
            "2025-09-23 23:58:09,340 - src.core.gpu_monitoring - INFO - üîÑ Created monitoring task: <Task pending name='Task-11' coro=<GPUPowerMonitor._monitor_loop() running at /home/chris/github/AstrID/src/core/gpu_monitoring.py:130>>\n",
            "2025-09-23 23:58:09,341 - notebooks.training.utils.training_manager - INFO - üîã GPU energy monitoring started with 0.5s sampling interval\n",
            "2025-09-23 23:58:09,341 - src.core.gpu_monitoring - INFO - üîÑ Starting GPU monitoring loop\n",
            "2025-09-23 23:58:09,410 - src.core.gpu_monitoring - INFO - üîã GPU power draw: 29.3W across 1 GPUs (samples: 1)\n",
            "2025-09-23 23:58:09,954 - src.core.gpu_monitoring - INFO - üîã GPU power draw: 29.1W across 1 GPUs (samples: 2)\n",
            "2025-09-23 23:58:10,511 - src.core.gpu_monitoring - INFO - üîã GPU power draw: 28.8W across 1 GPUs (samples: 3)\n",
            "2025-09-23 23:58:11,041 - src.core.gpu_monitoring - INFO - üîã GPU power draw: 29.0W across 1 GPUs (samples: 4)\n",
            "2025-09-23 23:58:11,343 - src.core.gpu_monitoring - INFO - üìä Summarizing GPU energy from 4 samples\n",
            "2025-09-23 23:58:11,344 - src.core.gpu_monitoring - INFO - Energy consumption: 0.013 Wh, Avg power: 29.0W, Carbon footprint: 0.000003 kg CO2\n",
            "2025-09-23 23:58:11,344 - notebooks.training.utils.training_manager - INFO - üß™ GPU monitoring test: 0.013 Wh, 29.0W, 4 samples\n",
            "2025-09-23 23:58:11,344 - src.core.gpu_monitoring - INFO - üîÑ Resetting GPU monitoring history (had 4 samples)\n",
            "2025-09-23 23:58:11,345 - src.core.gpu_monitoring - INFO - üîÑ Monitoring status: is_monitoring=True, task_running=True\n",
            "2025-09-23 23:58:11,346 - notebooks.training.utils.training_manager - INFO - üîÑ Reset GPU monitoring history - ready for training energy tracking\n",
            "2025-09-23 23:58:11,434 - notebooks.training.utils.training_manager - INFO - Epoch 1/100\n",
            "2025-09-24 00:01:40,448 - notebooks.training.utils.training_manager - INFO - üîã GPU monitoring status: active=True, is_monitoring=True\n",
            "2025-09-24 00:01:40,487 - src.core.gpu_monitoring - INFO - üìä Summarizing GPU energy from 0 samples\n",
            "2025-09-24 00:01:40,489 - src.core.gpu_monitoring - WARNING - No GPU metrics history available for energy calculation\n",
            "2025-09-24 00:01:40,492 - notebooks.training.utils.training_manager - INFO - üîã Epoch 1 energy summary: 0.000 Wh, 0.0W avg, 0 samples, epoch duration: 209.0s\n",
            "2025-09-24 00:01:40,493 - notebooks.training.utils.training_manager - WARNING - üîã No GPU samples detected, checking monitoring task...\n",
            "2025-09-24 00:03:57,664 - urllib3.connectionpool - WARNING - Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=5000): Read timed out. (read timeout=120)\")': /api/2.0/mlflow/runs/log-metric\n",
            "2025-09-24 00:06:17,134 - urllib3.connectionpool - WARNING - Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=5000): Read timed out. (read timeout=120)\")': /api/2.0/mlflow/runs/log-metric\n",
            "2025-09-24 00:06:19,098 - training_utils - INFO - New best model saved at epoch 0\n",
            "2025-09-24 00:06:19,100 - notebooks.training.utils.training_manager - INFO - Epoch 1 completed - Train Loss: -0.7982, Val Loss: -1.8705, Val Acc: 1.0000, Val F1: 0.5000, Energy: 0.000 Wh, Avg Power: 0.00 W, Peak Power: 0.00 W, Duration: 0.00 s, CO2: 0.000 kg\n",
            "2025-09-24 00:06:19,101 - notebooks.training.utils.training_manager - INFO - Epoch 2/100\n",
            "2025-09-24 00:08:35,105 - notebooks.training.utils.training_manager - INFO - üîã GPU monitoring status: active=True, is_monitoring=True\n",
            "2025-09-24 00:08:35,129 - src.core.gpu_monitoring - INFO - üìä Summarizing GPU energy from 0 samples\n",
            "2025-09-24 00:08:35,130 - src.core.gpu_monitoring - WARNING - No GPU metrics history available for energy calculation\n",
            "2025-09-24 00:08:35,132 - notebooks.training.utils.training_manager - INFO - üîã Epoch 2 energy summary: 0.000 Wh, 0.0W avg, 0 samples, epoch duration: 136.0s\n",
            "2025-09-24 00:08:35,132 - notebooks.training.utils.training_manager - WARNING - üîã No GPU samples detected, checking monitoring task...\n",
            "2025-09-24 00:08:58,035 - training_utils - INFO - New best model saved at epoch 1\n",
            "2025-09-24 00:08:58,039 - notebooks.training.utils.training_manager - INFO - Epoch 2 completed - Train Loss: -3.0866, Val Loss: -4.1341, Val Acc: 1.0000, Val F1: 0.5000, Energy: 0.000 Wh, Avg Power: 0.00 W, Peak Power: 0.00 W, Duration: 0.00 s, CO2: 0.000 kg\n",
            "2025-09-24 00:08:58,041 - notebooks.training.utils.training_manager - INFO - Epoch 3/100\n",
            "2025-09-24 00:11:31,331 - notebooks.training.utils.training_manager - INFO - üîã GPU monitoring status: active=True, is_monitoring=True\n",
            "2025-09-24 00:11:31,363 - src.core.gpu_monitoring - INFO - üìä Summarizing GPU energy from 0 samples\n",
            "2025-09-24 00:11:31,365 - src.core.gpu_monitoring - WARNING - No GPU metrics history available for energy calculation\n",
            "2025-09-24 00:11:31,368 - notebooks.training.utils.training_manager - INFO - üîã Epoch 3 energy summary: 0.000 Wh, 0.0W avg, 0 samples, epoch duration: 153.3s\n",
            "2025-09-24 00:11:31,369 - notebooks.training.utils.training_manager - WARNING - üîã No GPU samples detected, checking monitoring task...\n",
            "2025-09-24 00:13:47,677 - urllib3.connectionpool - WARNING - Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=5000): Read timed out. (read timeout=120)\")': /api/2.0/mlflow/runs/log-metric\n",
            "2025-09-24 00:16:07,450 - urllib3.connectionpool - WARNING - Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=5000): Read timed out. (read timeout=120)\")': /api/2.0/mlflow/runs/log-metric\n",
            "2025-09-24 00:16:13,621 - training_utils - INFO - New best model saved at epoch 2\n",
            "2025-09-24 00:16:13,623 - notebooks.training.utils.training_manager - INFO - Epoch 3 completed - Train Loss: -5.7196, Val Loss: -6.5533, Val Acc: 1.0000, Val F1: 0.5000, Energy: 0.000 Wh, Avg Power: 0.00 W, Peak Power: 0.00 W, Duration: 0.00 s, CO2: 0.000 kg\n",
            "2025-09-24 00:16:13,624 - notebooks.training.utils.training_manager - INFO - Epoch 4/100\n",
            "2025-09-24 00:18:38,934 - notebooks.training.utils.training_manager - INFO - üîã GPU monitoring status: active=True, is_monitoring=True\n",
            "2025-09-24 00:18:38,953 - src.core.gpu_monitoring - INFO - üìä Summarizing GPU energy from 0 samples\n",
            "2025-09-24 00:18:38,955 - src.core.gpu_monitoring - WARNING - No GPU metrics history available for energy calculation\n",
            "2025-09-24 00:18:38,956 - notebooks.training.utils.training_manager - INFO - üîã Epoch 4 energy summary: 0.000 Wh, 0.0W avg, 0 samples, epoch duration: 145.3s\n",
            "2025-09-24 00:18:38,958 - notebooks.training.utils.training_manager - WARNING - üîã No GPU samples detected, checking monitoring task...\n"
          ]
        }
      ],
      "source": [
        "# Initialize training manager\n",
        "training_manager = TrainingManager(\n",
        "    config=config,\n",
        "    experiment_tracker=experiment_tracker,\n",
        "    model_registry=model_registry,\n",
        "    mlflow_client=mlflow_client,\n",
        "    gpu_monitor=gpu_monitor,\n",
        "    energy_tracker=energy_tracker\n",
        ")\n",
        "\n",
        "# Add experiment_id to config for training manager\n",
        "config.experiment_id = experiment_id\n",
        "\n",
        "# Add real data information to config for tracking\n",
        "if USING_REAL_DATA and real_data_info:\n",
        "    config.tags.update({\n",
        "        \"data_type\": \"real_astronomical_data\",\n",
        "        \"real_data_enabled\": \"true\",\n",
        "        \"train_samples\": str(real_data_info[\"train_samples\"]),\n",
        "        \"dataset_source\": \"astrid_validated_detections\"\n",
        "    })\n",
        "else:\n",
        "    config.tags.update({\n",
        "        \"data_type\": \"synthetic_data\",\n",
        "        \"real_data_enabled\": \"false\",\n",
        "        \"dataset_source\": \"synthetic_generation\"\n",
        "    })\n",
        "\n",
        "print(\"üöÄ Starting comprehensive training with full tracking...\")\n",
        "print(f\"   - MLflow experiment tracking: ‚úÖ\")\n",
        "print(f\"   - GPU energy monitoring: {'‚úÖ' if config.enable_energy_tracking else '‚ùå'}\")\n",
        "print(f\"   - Performance metrics (ASTR-102): ‚úÖ\")\n",
        "print(f\"   - Model checkpointing: ‚úÖ\")\n",
        "print(f\"   - Real data integration: {'‚úÖ' if USING_REAL_DATA else '‚ùå'}\")\n",
        "\n",
        "if USING_REAL_DATA:\n",
        "    print(\"   üåü REAL DATA FEATURES ENABLED:\")\n",
        "    print(\"      ‚Ä¢ Training on validated astronomical detections\")\n",
        "    print(\"      ‚Ä¢ Meaningful GPU utilization expected\")\n",
        "    print(\"      ‚Ä¢ Real energy consumption tracking\")\n",
        "    print(\"      ‚Ä¢ Authentic astronomical image patches\")\n",
        "\n",
        "# Start training\n",
        "try:\n",
        "    run_id = await training_manager.start_training_run(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        scheduler=scheduler\n",
        "    )\n",
        "    \n",
        "    print(f\"üéâ Training completed successfully!\")\n",
        "    print(f\"üìä MLflow Run ID: {run_id}\")\n",
        "    \n",
        "    if USING_REAL_DATA:\n",
        "        print(\"üåü REAL DATA TRAINING IMPACT:\")\n",
        "        print(\"   ‚Ä¢ Check MLflow for actual GPU energy consumption\")\n",
        "        print(\"   ‚Ä¢ Training metrics reflect real astronomical data performance\")\n",
        "        print(\"   ‚Ä¢ Model learned from validated astronomical detections\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Visualization and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comprehensive training curves\n",
        "print(\"üìä Generating training visualizations...\")\n",
        "training_manager.plot_training_summary()\n",
        "\n",
        "# Get training summary\n",
        "training_summary = training_manager.get_training_summary()\n",
        "print(f\"\\nüìà Training Summary:\")\n",
        "print(f\"   Best validation loss: {training_summary['best_val_loss']:.4f}\")\n",
        "print(f\"   Total epochs: {training_summary['total_epochs']}\")\n",
        "print(f\"   Final train loss: {training_summary['final_train_loss']:.4f}\")\n",
        "print(f\"   Final val loss: {training_summary['final_val_loss']:.4f}\")\n",
        "\n",
        "# Display final metrics\n",
        "final_metrics = training_summary['final_val_metrics']\n",
        "if final_metrics:\n",
        "    print(f\"\\nüéØ Final Validation Metrics:\")\n",
        "    print(f\"   Accuracy: {final_metrics.get('accuracy', 0.0):.4f}\")\n",
        "    print(f\"   Precision: {final_metrics.get('precision_macro', 0.0):.4f}\")\n",
        "    print(f\"   Recall: {final_metrics.get('recall_macro', 0.0):.4f}\")\n",
        "    print(f\"   F1 Score: {final_metrics.get('f1_macro', 0.0):.4f}\")\n",
        "    print(f\"   AUROC: {final_metrics.get('auroc', 0.0):.4f}\")\n",
        "    print(f\"   AUPRC: {final_metrics.get('auprc', 0.0):.4f}\")\n",
        "    print(f\"   MCC: {final_metrics.get('mcc', 0.0):.4f}\")\n",
        "    print(f\"   Balanced Accuracy: {final_metrics.get('balanced_accuracy', 0.0):.4f}\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    print(f\"\\n‚ö° Performance Metrics:\")\n",
        "    print(f\"   Latency P50: {final_metrics.get('latency_ms_p50', 0.0):.2f} ms\")\n",
        "    print(f\"   Latency P95: {final_metrics.get('latency_ms_p95', 0.0):.2f} ms\")\n",
        "    print(f\"   Throughput: {final_metrics.get('throughput_items_per_s', 0.0):.2f} items/s\")\n",
        "    \n",
        "    # Energy metrics (if available)\n",
        "    if config.enable_energy_tracking:\n",
        "        print(f\"\\nüîã Energy Metrics:\")\n",
        "        print(f\"   Energy consumed: {final_metrics.get('training_energy_wh', 0.0):.3f} Wh\")\n",
        "        print(f\"   Average power: {final_metrics.get('training_avg_power_w', 0.0):.1f} W\")\n",
        "        print(f\"   Peak power: {final_metrics.get('training_peak_power_w', 0.0):.1f} W\")\n",
        "        print(f\"   Carbon footprint: {final_metrics.get('training_carbon_footprint_kg', 0.0):.6f} kg CO2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for evaluation\n",
        "import time\n",
        "best_checkpoint_path = training_manager.checkpoint_manager.checkpoint_dir / \"best_model.pt\"\n",
        "if best_checkpoint_path.exists():\n",
        "    checkpoint = training_manager.checkpoint_manager.load_checkpoint(str(best_checkpoint_path))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"‚úÖ Loaded best model for evaluation\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Best model checkpoint not found, using current model\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"üß™ Evaluating model on test set...\")\n",
        "model.eval()\n",
        "\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "all_scores = []\n",
        "inference_times = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(training_manager.device), target.to(training_manager.device)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        output = model(data)\n",
        "        inference_time = time.time() - start_time\n",
        "        inference_times.append(inference_time)\n",
        "        \n",
        "        predictions = (torch.sigmoid(output) > config.confidence_threshold).float()\n",
        "        scores = torch.sigmoid(output).cpu().detach().numpy().flatten()\n",
        "        \n",
        "        all_predictions.extend(predictions.cpu().detach().numpy().flatten())\n",
        "        all_targets.extend(target.cpu().detach().numpy().flatten())\n",
        "        all_scores.extend(scores)\n",
        "\n",
        "# Calculate comprehensive test metrics\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "all_scores = np.array(all_scores)\n",
        "\n",
        "test_metrics = training_manager.metrics_calculator.calculate_all_metrics(\n",
        "    all_targets, all_predictions, all_scores, inference_times, config.batch_size\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ Test Set Results:\")\n",
        "print(f\"   Accuracy: {test_metrics.get('accuracy', 0.0):.4f}\")\n",
        "print(f\"   Precision: {test_metrics.get('precision_macro', 0.0):.4f}\")\n",
        "print(f\"   Recall: {test_metrics.get('recall_macro', 0.0):.4f}\")\n",
        "print(f\"   F1 Score: {test_metrics.get('f1_macro', 0.0):.4f}\")\n",
        "print(f\"   AUROC: {test_metrics.get('auroc', 0.0):.4f}\")\n",
        "print(f\"   AUPRC: {test_metrics.get('auprc', 0.0):.4f}\")\n",
        "print(f\"   MCC: {test_metrics.get('mcc', 0.0):.4f}\")\n",
        "print(f\"   Balanced Accuracy: {test_metrics.get('balanced_accuracy', 0.0):.4f}\")\n",
        "\n",
        "# Performance metrics\n",
        "print(f\"\\n‚ö° Test Performance:\")\n",
        "print(f\"   Latency P50: {test_metrics.get('latency_ms_p50', 0.0):.2f} ms\")\n",
        "print(f\"   Latency P95: {test_metrics.get('latency_ms_p95', 0.0):.2f} ms\")\n",
        "print(f\"   Throughput: {test_metrics.get('throughput_items_per_s', 0.0):.2f} items/s\")\n",
        "\n",
        "# Generate visualizations\n",
        "from notebooks.training.utils.training_utils import TrainingVisualizer\n",
        "visualizer = TrainingVisualizer()\n",
        "\n",
        "print(\"\\nüìä Generating evaluation visualizations...\")\n",
        "visualizer.plot_confusion_matrix(all_targets, all_predictions)\n",
        "visualizer.plot_roc_curve(all_targets, all_scores)\n",
        "visualizer.plot_precision_recall_curve(all_targets, all_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Troubleshooting and Debugging Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model debugging and inspection tools\n",
        "def inspect_model_predictions(model, data_loader, num_samples=5):\n",
        "    \"\"\"Inspect model predictions for debugging.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "                \n",
        "            data = data.to(training_manager.device)\n",
        "            output = model(data)\n",
        "            predictions = torch.sigmoid(output)\n",
        "            \n",
        "            # Convert to numpy for visualization\n",
        "            image = data[0].cpu().numpy().squeeze()\n",
        "            target_mask = target[0].cpu().numpy().squeeze()\n",
        "            pred_mask = (predictions[0].cpu().numpy().squeeze() > config.confidence_threshold).astype(float)\n",
        "            confidence = predictions[0].cpu().numpy().squeeze()\n",
        "            \n",
        "            # Create visualization\n",
        "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "            \n",
        "            axes[0].imshow(image, cmap='gray')\n",
        "            axes[0].set_title('Input Image')\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            axes[1].imshow(target_mask, cmap='hot')\n",
        "            axes[1].set_title('Ground Truth')\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            axes[2].imshow(pred_mask, cmap='hot')\n",
        "            axes[2].set_title('Prediction')\n",
        "            axes[2].axis('off')\n",
        "            \n",
        "            im = axes[3].imshow(confidence, cmap='viridis')\n",
        "            axes[3].set_title('Confidence Map')\n",
        "            axes[3].axis('off')\n",
        "            plt.colorbar(im, ax=axes[3])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # Print statistics\n",
        "            print(f\"Sample {i+1}:\")\n",
        "            print(f\"  Target pixels: {np.sum(target_mask):.0f}\")\n",
        "            print(f\"  Predicted pixels: {np.sum(pred_mask):.0f}\")\n",
        "            print(f\"  Confidence range: [{np.min(confidence):.3f}, {np.max(confidence):.3f}]\")\n",
        "            print(f\"  IoU: {np.sum((target_mask > 0) & (pred_mask > 0)) / np.sum((target_mask > 0) | (pred_mask > 0)):.3f}\")\n",
        "            print()\n",
        "\n",
        "def analyze_training_issues():\n",
        "    \"\"\"Analyze potential training issues.\"\"\"\n",
        "    print(\"üîç Training Analysis:\")\n",
        "    \n",
        "    # Check for overfitting\n",
        "    train_losses = training_summary['training_history']['train_losses']\n",
        "    val_losses = training_summary['training_history']['val_losses']\n",
        "    \n",
        "    if len(train_losses) > 5 and len(val_losses) > 5:\n",
        "        train_trend = np.mean(train_losses[-5:]) - np.mean(train_losses[:5])\n",
        "        val_trend = np.mean(val_losses[-5:]) - np.mean(val_losses[:5])\n",
        "        \n",
        "        if val_trend > train_trend * 1.5:\n",
        "            print(\"‚ö†Ô∏è  Potential overfitting detected - validation loss increasing while training loss decreasing\")\n",
        "        elif val_trend < -0.1:\n",
        "            print(\"‚úÖ Good training progress - both losses decreasing\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Training appears stable\")\n",
        "    \n",
        "    # Check learning rate\n",
        "    lr_history = training_summary['training_history']['learning_rates']\n",
        "    if len(lr_history) > 1:\n",
        "        lr_change = (lr_history[-1] - lr_history[0]) / lr_history[0]\n",
        "        if lr_change < -0.5:\n",
        "            print(\"‚ÑπÔ∏è  Learning rate significantly reduced during training\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Learning rate relatively stable\")\n",
        "    \n",
        "    # Check convergence\n",
        "    if len(val_losses) > 10:\n",
        "        recent_val_losses = val_losses[-10:]\n",
        "        val_std = np.std(recent_val_losses)\n",
        "        if val_std < 0.01:\n",
        "            print(\"‚úÖ Model appears to have converged\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Model may still be learning\")\n",
        "\n",
        "# Run debugging tools\n",
        "print(\"üîß Running debugging and analysis tools...\")\n",
        "inspect_model_predictions(model, test_loader, num_samples=3)\n",
        "analyze_training_issues()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Summary and Next Steps\n",
        "print(\"üéâ ASTR-106 Training Notebook Complete with Real Data Integration!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüìä What was accomplished:\")\n",
        "print(\"‚úÖ Complete MLflow experiment tracking (ASTR-88 integration)\")\n",
        "print(\"‚úÖ GPU energy monitoring and carbon footprint tracking (ASTR-101)\")\n",
        "print(\"‚úÖ Comprehensive performance metrics (ASTR-102)\")\n",
        "print(\"‚úÖ Data preprocessing integration (ASTR-76)\")\n",
        "print(\"‚úÖ U-Net model training with PyTorch\")\n",
        "print(\"‚úÖ Model checkpointing and versioning\")\n",
        "print(\"‚úÖ Visualization and debugging tools\")\n",
        "print(\"‚úÖ Model evaluation and testing\")\n",
        "print(\"üåü REAL DATA INTEGRATION (ASTR-113) - NEW!\")\n",
        "\n",
        "print(f\"\\nüìà Training Results:\")\n",
        "print(f\"   MLflow Run ID: {run_id}\")\n",
        "print(f\"   Best validation loss: {training_summary['best_val_loss']:.4f}\")\n",
        "print(f\"   Final test accuracy: {test_metrics.get('accuracy', 0.0):.4f}\")\n",
        "print(f\"   Final test F1 score: {test_metrics.get('f1_macro', 0.0):.4f}\")\n",
        "\n",
        "# Real data specific results\n",
        "if USING_REAL_DATA and real_data_info:\n",
        "    print(f\"\\nüåü Real Data Integration Results:\")\n",
        "    print(f\"   Data source: Validated astronomical detections\")\n",
        "    print(f\"   Training samples: {real_data_info['train_samples']}\")\n",
        "    print(f\"   Validation samples: {real_data_info['val_samples']}\")\n",
        "    print(f\"   Test samples: {real_data_info['test_samples']}\")\n",
        "    print(f\"   Survey sources: {', '.join(real_data_config.survey_ids)}\")\n",
        "    print(f\"   Confidence threshold: {real_data_config.confidence_threshold}\")\n",
        "    print(\"   ‚úÖ GPU utilization should show meaningful values\")\n",
        "    print(\"   ‚úÖ Energy tracking reflects actual compute work\")\n",
        "    print(\"   ‚úÖ Model trained on real astronomical phenomena\")\n",
        "else:\n",
        "    print(f\"\\nüîÑ Synthetic Data Fallback:\")\n",
        "    print(\"   Used synthetic data generation (real data unavailable)\")\n",
        "    print(\"   Consider adding real observations to database\")\n",
        "\n",
        "if config.enable_energy_tracking:\n",
        "    print(f\"\\nüîã Energy Impact:\")\n",
        "    print(f\"   Total energy consumed: {test_metrics.get('training_energy_wh', 0.0):.3f} Wh\")\n",
        "    print(f\"   Carbon footprint: {test_metrics.get('training_carbon_footprint_kg', 0.0):.6f} kg CO2\")\n",
        "    if USING_REAL_DATA:\n",
        "        print(\"   ‚ö° Energy values reflect actual GPU compute work on real data\")\n",
        "\n",
        "print(f\"\\nüìÅ Outputs:\")\n",
        "print(f\"   Model checkpoints: {training_manager.checkpoint_manager.checkpoint_dir}\")\n",
        "print(f\"   MLflow artifacts: {config.mlflow_tracking_uri}\")\n",
        "print(f\"   Training logs: Available in MLflow UI\")\n",
        "if USING_REAL_DATA:\n",
        "    print(f\"   Real data metrics: Tagged in MLflow for identification\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(\"1. Review results in MLflow UI (check real data tags)\")\n",
        "print(\"2. Deploy best model to production\")\n",
        "print(\"3. Set up automated retraining pipeline with real data\")\n",
        "print(\"4. Monitor model performance in production\")\n",
        "if USING_REAL_DATA:\n",
        "    print(\"5. Expand real data collection from more surveys\")\n",
        "    print(\"6. Implement continuous learning with new validated detections\")\n",
        "else:\n",
        "    print(\"5. Add real observations to database for next training run\")\n",
        "    print(\"6. Investigate database connectivity issues\")\n",
        "\n",
        "print(f\"\\nüîó Useful Links:\")\n",
        "print(f\"   MLflow UI: {config.mlflow_tracking_uri}\")\n",
        "print(f\"   Model Registry: {config.mlflow_tracking_uri}/#/models\")\n",
        "print(f\"   Experiment: {config.mlflow_tracking_uri}/#/experiments/{experiment_id}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if USING_REAL_DATA:\n",
        "    print(\"üéØ ASTR-106 + ASTR-113 Implementation Complete!\")\n",
        "    print(\"üåü Successfully integrated real astronomical data for training!\")\n",
        "else:\n",
        "    print(\"üéØ ASTR-106 Implementation Complete (with fallback data)!\")\n",
        "    print(\"üîÑ Ready for real data integration when observations are available\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
