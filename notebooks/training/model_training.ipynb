{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AstrID Model Training Notebook\n",
        "\n",
        "## U-Net Anomaly Detection with MLflow Integration\n",
        "\n",
        "This notebook provides a comprehensive training pipeline for the U-Net anomaly detection model with:\n",
        "- Complete MLflow experiment tracking\n",
        "- GPU energy monitoring (ASTR-101)\n",
        "- Comprehensive performance metrics (ASTR-102)\n",
        "- Data preprocessing integration\n",
        "- Visualization and debugging tools\n",
        "\n",
        "**Project**: ASTR-106 - Training Notebook for Model Training and MLflow Logging  \n",
        "**Dependencies**: ASTR-88 (MLflow Integration) ‚úÖ, ASTR-80 (U-Net Model) ‚úÖ, ASTR-76 (Preprocessing) ‚úÖ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Environment Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import sys\n",
        "import asyncio\n",
        "import logging\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from dataclasses import dataclass, field\n",
        "from uuid import uuid4\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Scientific computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix,\n",
        "    classification_report, matthews_corrcoef\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "# MLflow and tracking\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# AstrID imports\n",
        "from src.infrastructure.mlflow import MLflowConfig, ExperimentTracker, ModelRegistry\n",
        "from src.core.gpu_monitoring import GPUPowerMonitor, EnergyConsumption\n",
        "from src.core.mlflow_energy import MLflowEnergyTracker\n",
        "from src.core.energy_analysis import EnergyAnalyzer\n",
        "from src.domains.preprocessing.processors.astronomical_image_processing import AstronomicalImageProcessor\n",
        "from src.adapters.ml.unet import UNetModel\n",
        "from src.domains.detection.models import Model, ModelRun\n",
        "from src.domains.detection.metrics.detection_metrics import DetectionMetrics\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "print(f\"‚úÖ Project root: {project_root}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.core.constants import TRAINING_PIPELINE_API_KEY\n",
        "\n",
        "global AUTH_HEADERS\n",
        "AUTH_HEADERS = {\n",
        "    \"X-API-Key\": TRAINING_PIPELINE_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check MLflow configuration\n",
        "import os\n",
        "from src.core.constants import get_mlflow_tracking_uri\n",
        "\n",
        "print(f\"üîç MLflow tracking URI: {get_mlflow_tracking_uri()}\")\n",
        "print(f\"üîç MLflow environment variables:\")\n",
        "print(f\"   MLFLOW_SUPABASE_HOST: {os.getenv('MLFLOW_SUPABASE_HOST', 'Not set')}\")\n",
        "print(f\"   MLFLOW_SUPABASE_PROJECT_REF: {os.getenv('MLFLOW_SUPABASE_PROJECT_REF', 'Not set')}\")\n",
        "print(f\"   MLFLOW_SUPABASE_PASSWORD: {'Set' if os.getenv('MLFLOW_SUPABASE_PASSWORD') else 'Not set'}\")\n",
        "\n",
        "# Check if we should use a different approach\n",
        "if not get_mlflow_tracking_uri() or get_mlflow_tracking_uri() == \"postgresql+asyncpg://postgres:None@aws-0-us-west-1.pooler.supabase.com:5432/postgres\":\n",
        "    print(\"‚ö†Ô∏è  MLflow environment variables not set, using local SQLite backend\")\n",
        "    print(\"üí° Consider setting MLFLOW_SUPABASE_* environment variables for production\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add notebooks directory to Python path for imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the absolute path to the notebooks directory\n",
        "# This works regardless of where the notebook is run from\n",
        "current_file = Path(__file__) if '__file__' in globals() else Path.cwd()\n",
        "notebooks_dir = current_file.parent.parent  # Go up two levels to get to notebooks/\n",
        "\n",
        "# Add both the notebooks directory and the project root to Python path\n",
        "sys.path.insert(0, str(notebooks_dir))\n",
        "sys.path.insert(0, str(notebooks_dir.parent))  # Also add project root\n",
        "\n",
        "print(f\"‚úÖ Current file location: {current_file}\")\n",
        "print(f\"‚úÖ Added to Python path: {notebooks_dir}\")\n",
        "print(f\"‚úÖ Added project root to Python path: {notebooks_dir.parent}\")\n",
        "print(f\"‚úÖ Current working directory: {Path.cwd()}\")\n",
        "print(f\"‚úÖ Python path includes notebooks: {[p for p in sys.path if 'notebooks' in p]}\")\n",
        "\n",
        "# Test the import\n",
        "try:\n",
        "    import notebooks\n",
        "    print(\"‚úÖ Successfully imported notebooks module\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import notebooks: {e}\")\n",
        "    print(\"üí° Trying alternative approach...\")\n",
        "    \n",
        "    # Alternative: Add the specific path\n",
        "    training_utils_path = notebooks_dir / \"training\" / \"utils\"\n",
        "    sys.path.insert(0, str(training_utils_path))\n",
        "    print(f\"‚úÖ Added training utils path: {training_utils_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.core.constants import get_mlflow_tracking_uri, MLFLOW_S3_ENDPOINT_URL, MLFLOW_BUCKET_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Comprehensive training configuration.\"\"\"\n",
        "    \n",
        "    # Experiment settings\n",
        "    experiment_name: str = \"unet_anomaly_detection\"\n",
        "    experiment_id: str = \"\"\n",
        "    run_name: str = f\"training_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    # Model architecture\n",
        "    model_name: str = \"unet_anomaly_detector\"\n",
        "    input_channels: int = 1\n",
        "    output_channels: int = 1\n",
        "    input_size: Tuple[int, int] = (512, 512)\n",
        "    initial_filters: int = 64\n",
        "    depth: int = 4\n",
        "    \n",
        "    # Training parameters\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 0.001\n",
        "    num_epochs: int = 100\n",
        "    weight_decay: float = 1e-4\n",
        "    gradient_clip_norm: float = 1.0\n",
        "    \n",
        "    # Data parameters\n",
        "    validation_split: float = 0.2\n",
        "    test_split: float = 0.1\n",
        "    \n",
        "    # Training strategy\n",
        "    early_stopping_patience: int = 10\n",
        "    checkpoint_frequency: int = 5\n",
        "    \n",
        "    # MLflow parameters\n",
        "    mlflow_tracking_uri: str = \"http://localhost:5000\"\n",
        "    database_url: str = get_mlflow_tracking_uri()\n",
        "    mlflow_bucket_name: str = (MLFLOW_BUCKET_NAME or \"astrid-models\")\n",
        "    mlflow_endpoint_url: str = (MLFLOW_S3_ENDPOINT_URL or \"\")\n",
        "    mlflow_access_key_id: str = (AWS_ACCESS_KEY_ID or \"\")\n",
        "    mlflow_secret_access_key: str = (AWS_SECRET_ACCESS_KEY or \"\")\n",
        "    mlflow_region: str = (AWS_DEFAULT_REGION or \"auto\")\n",
        "    \n",
        "    # Artifact root\n",
        "    artifact_root: str = f\"s3://{mlflow_bucket_name}\"\n",
        "    \n",
        "    \n",
        "    # Energy tracking\n",
        "    enable_energy_tracking: bool = True\n",
        "    gpu_power_sampling_hz: float = 1.0\n",
        "    carbon_intensity_kg_per_kwh: float = 0.233\n",
        "    \n",
        "    # Performance metrics\n",
        "    confidence_threshold: float = 0.5\n",
        "    \n",
        "    # Tags for MLflow\n",
        "    tags: Dict[str, str] = field(default_factory=lambda: {\n",
        "        \"model_type\": \"unet\",\n",
        "        \"task\": \"anomaly_detection\",\n",
        "        \"dataset\": \"astronomical_images\",\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"gpu_tracking\": \"enabled\"\n",
        "    })\n",
        "\n",
        "# Initialize configuration\n",
        "config = TrainingConfig()\n",
        "print(f\"üìã Training configuration initialized: {config.run_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MLflow Setup and Experiment Tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Python path for imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add paths for imports\n",
        "sys.path.insert(0, str(Path.cwd() / \"utils\"))  # For utility files\n",
        "sys.path.insert(0, str(Path.cwd().parent.parent))  # For src modules\n",
        "print(\"‚úÖ Python paths configured for imports\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize MLflow configuration\n",
        "mlflow_config = MLflowConfig(\n",
        "    tracking_uri=config.mlflow_tracking_uri,\n",
        "    artifact_root=config.artifact_root,\n",
        "    database_url=config.database_url\n",
        ")\n",
        "\n",
        "print(f\"üîç MLflow configuration: {mlflow_config}\")\n",
        "\n",
        "# Initialize MLflow components\n",
        "experiment_tracker = ExperimentTracker(mlflow_config)\n",
        "model_registry = ModelRegistry(mlflow_config)\n",
        "mlflow_client = MlflowClient(tracking_uri=config.mlflow_tracking_uri)\n",
        "\n",
        "print(f\"üîç MLflow client: {mlflow_client}\")\n",
        "\n",
        "# Set MLflow tracking URI\n",
        "mlflow.set_tracking_uri(config.mlflow_tracking_uri)\n",
        "\n",
        "print(f\"üîç MLflow tracking URI: {config.mlflow_tracking_uri}\")\n",
        "\n",
        "# Create or get experiment\n",
        "try:\n",
        "    experiment_id = experiment_tracker.create_experiment(\n",
        "        name=config.experiment_name,\n",
        "        description=\"U-Net anomaly detection training experiments\"\n",
        "    )\n",
        "    print(f\"‚úÖ Created new experiment: {config.experiment_name}\")\n",
        "except Exception as e:\n",
        "    # Get existing experiment\n",
        "    experiment = mlflow_client.get_experiment_by_name(config.experiment_name)\n",
        "    if experiment:\n",
        "        experiment_id = experiment.experiment_id\n",
        "        print(f\"‚úÖ Using existing experiment: {config.experiment_name}\")\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "print(f\"üî¨ Experiment ID: {experiment_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import data loading utilities\n",
        "from notebooks.training.utils.training_utils import AstronomicalDataset, create_data_transforms, load_sample_data\n",
        "\n",
        "# Load sample data\n",
        "print(\"üìä Loading sample astronomical data...\")\n",
        "sample_images, sample_masks = load_sample_data(\n",
        "    num_samples=200, \n",
        "    image_size=config.input_size\n",
        ")\n",
        "print(f\"‚úÖ Loaded {len(sample_images)} samples\")\n",
        "\n",
        "# Create data transforms\n",
        "train_transform, val_transform = create_data_transforms()\n",
        "print(\"‚úÖ Data transforms created\")\n",
        "\n",
        "# Create datasets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    sample_images, sample_masks, \n",
        "    test_size=config.validation_split + config.test_split, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(\n",
        "    val_images, val_masks,\n",
        "    test_size=config.test_split / (config.validation_split + config.test_split),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = AstronomicalDataset(\n",
        "    train_images, train_masks, \n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = AstronomicalDataset(\n",
        "    val_images, val_masks, \n",
        "    transform=val_transform\n",
        ")\n",
        "test_dataset = AstronomicalDataset(\n",
        "    test_images, test_masks, \n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"üìä Data splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "print(\"‚úÖ Data loading complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GPU Energy Tracking Setup (ASTR-101)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize GPU energy tracking\n",
        "if config.enable_energy_tracking:\n",
        "    gpu_monitor = GPUPowerMonitor(\n",
        "        sampling_interval=1.0 / config.gpu_power_sampling_hz,\n",
        "        carbon_intensity_kg_per_kwh=config.carbon_intensity_kg_per_kwh\n",
        "    )\n",
        "    \n",
        "    energy_tracker = MLflowEnergyTracker(\n",
        "        experiment_name=config.experiment_name\n",
        "    )\n",
        "    \n",
        "    energy_analyzer = EnergyAnalyzer()\n",
        "    \n",
        "    print(\"üîã GPU energy tracking initialized\")\n",
        "    \n",
        "    # Check GPU availability\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"üéÆ GPU available: {gpu_name} (Count: {gpu_count})\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No GPU available - energy tracking will use simulation mode\")\n",
        "else:\n",
        "    gpu_monitor = None\n",
        "    energy_tracker = None\n",
        "    energy_analyzer = None\n",
        "    print(\"‚ö° Energy tracking disabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Loading and Preprocessing Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import training utilities\n",
        "from notebooks.training.utils.training_utils import (\n",
        "    AstronomicalDataset, create_data_transforms, load_sample_data\n",
        ")\n",
        "print(\"‚úÖ Training utilities imported\")\n",
        "from notebooks.training.utils.performance_metrics import ComprehensiveMetricsCalculator\n",
        "print(\"‚úÖ Comprehensive metrics calculator imported\")\n",
        "from notebooks.training.utils.training_manager import TrainingManager\n",
        "print(\"‚úÖ Training manager imported\")\n",
        "\n",
        "# Load sample data\n",
        "print(\"üìä Loading sample astronomical data...\")\n",
        "sample_images, sample_masks = load_sample_data(\n",
        "    num_samples=200, \n",
        "    image_size=config.input_size\n",
        ")\n",
        "print(f\"‚úÖ Loaded {len(sample_images)} samples\")\n",
        "\n",
        "# Create data transforms\n",
        "train_transform, val_transform = create_data_transforms()\n",
        "print(\"‚úÖ Data transforms created\")\n",
        "\n",
        "# Create datasets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    sample_images, sample_masks, \n",
        "    test_size=config.validation_split + config.test_split, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(\n",
        "    val_images, val_masks,\n",
        "    test_size=config.test_split / (config.validation_split + config.test_split),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = AstronomicalDataset(\n",
        "    train_images, train_masks, \n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = AstronomicalDataset(\n",
        "    val_images, val_masks, \n",
        "    transform=val_transform\n",
        ")\n",
        "test_dataset = AstronomicalDataset(\n",
        "    test_images, test_masks, \n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"üìä Data splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "print(\"‚úÖ Data loading complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Architecture and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define U-Net model architecture\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"U-Net architecture for astronomical anomaly detection.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=1, out_channels=1, initial_filters=64, depth=4):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        self.depth = depth\n",
        "        self.initial_filters = initial_filters\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.pools = nn.ModuleList()\n",
        "        \n",
        "        in_ch = in_channels\n",
        "        for i in range(depth):\n",
        "            out_ch = initial_filters * (2 ** i)\n",
        "            self.encoders.append(self._conv_block(in_ch, out_ch))\n",
        "            if i < depth - 1:  # No pooling after last encoder\n",
        "                self.pools.append(nn.MaxPool2d(2))\n",
        "            in_ch = out_ch\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._conv_block(\n",
        "            initial_filters * (2 ** (depth - 1)), \n",
        "            initial_filters * (2 ** depth)\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.upsamples = nn.ModuleList()\n",
        "        \n",
        "        for i in range(depth - 1, 0, -1):\n",
        "            in_ch = initial_filters * (2 ** i) + initial_filters * (2 ** (i - 1))\n",
        "            out_ch = initial_filters * (2 ** (i - 1))\n",
        "            self.upsamples.append(nn.ConvTranspose2d(\n",
        "                initial_filters * (2 ** i), \n",
        "                initial_filters * (2 ** (i - 1)), \n",
        "                kernel_size=2, \n",
        "                stride=2\n",
        "            ))\n",
        "            self.decoders.append(self._conv_block(in_ch, out_ch))\n",
        "        \n",
        "        # Final layer\n",
        "        self.final = nn.Conv2d(initial_filters, out_channels, kernel_size=1)\n",
        "        \n",
        "    def _conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder path\n",
        "        encoder_outputs = []\n",
        "        for i, encoder in enumerate(self.encoders):\n",
        "            x = encoder(x)\n",
        "            encoder_outputs.append(x)\n",
        "            if i < len(self.pools):\n",
        "                x = self.pools[i](x)\n",
        "        \n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        \n",
        "        # Decoder path\n",
        "        for i, (upsample, decoder) in enumerate(zip(self.upsamples, self.decoders)):\n",
        "            x = upsample(x)\n",
        "            # Skip connection\n",
        "            skip_idx = len(encoder_outputs) - 2 - i\n",
        "            x = torch.cat([x, encoder_outputs[skip_idx]], dim=1)\n",
        "            x = decoder(x)\n",
        "        \n",
        "        # Final output\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = UNet(\n",
        "    in_channels=config.input_channels,\n",
        "    out_channels=config.output_channels,\n",
        "    initial_filters=config.initial_filters,\n",
        "    depth=config.depth\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"üèóÔ∏è  Model Architecture:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr=config.learning_rate, \n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    mode='min', \n",
        "    factor=0.5, \n",
        "    patience=5\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model, loss function, and optimizer initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix U-Net decoder channel dimensions and re-instantiate model\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"U-Net architecture for astronomical anomaly detection (fixed decoder).\"\"\"\n",
        "    def __init__(self, in_channels=1, out_channels=1, initial_filters=64, depth=4):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.initial_filters = initial_filters\n",
        "\n",
        "        # Encoder\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.pools = nn.ModuleList()\n",
        "        in_ch = in_channels\n",
        "        for i in range(depth):\n",
        "            out_ch = initial_filters * (2 ** i)\n",
        "            self.encoders.append(self._conv_block(in_ch, out_ch))\n",
        "            if i < depth - 1:\n",
        "                self.pools.append(nn.MaxPool2d(2))\n",
        "            in_ch = out_ch\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._conv_block(\n",
        "            initial_filters * (2 ** (depth - 1)),\n",
        "            initial_filters * (2 ** depth),\n",
        "        )\n",
        "\n",
        "        # Decoder (fixed)\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.upsamples = nn.ModuleList()\n",
        "        prev_ch = initial_filters * (2 ** depth)\n",
        "        for i in range(depth - 1, 0, -1):\n",
        "            out_ch = initial_filters * (2 ** (i - 1))\n",
        "            self.upsamples.append(\n",
        "                nn.ConvTranspose2d(prev_ch, out_ch, kernel_size=2, stride=2)\n",
        "            )\n",
        "            # After upsample, concat with encoder skip (out_ch) => 2 * out_ch input to block\n",
        "            self.decoders.append(self._conv_block(out_ch * 2, out_ch))\n",
        "            prev_ch = out_ch\n",
        "\n",
        "        # Final layer\n",
        "        self.final = nn.Conv2d(initial_filters, out_channels, kernel_size=1)\n",
        "\n",
        "    def _conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for i, encoder in enumerate(self.encoders):\n",
        "            x = encoder(x)\n",
        "            encoder_outputs.append(x)\n",
        "            if i < len(self.pools):\n",
        "                x = self.pools[i](x)\n",
        "        x = self.bottleneck(x)\n",
        "        for i, (upsample, decoder) in enumerate(zip(self.upsamples, self.decoders)):\n",
        "            x = upsample(x)\n",
        "            skip_idx = len(encoder_outputs) - 2 - i\n",
        "            x = torch.cat([x, encoder_outputs[skip_idx]], dim=1)\n",
        "            x = decoder(x)\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "# Recreate model with fixed architecture\n",
        "model = UNet(\n",
        "    in_channels=config.input_channels,\n",
        "    out_channels=config.output_channels,\n",
        "    initial_filters=config.initial_filters,\n",
        "    depth=config.depth,\n",
        ")\n",
        "\n",
        "print(\"üîß Rebuilt UNet with corrected decoder channel dimensions.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comprehensive Training with MLflow and Energy Tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize training manager\n",
        "training_manager = TrainingManager(\n",
        "    config=config,\n",
        "    experiment_tracker=experiment_tracker,\n",
        "    model_registry=model_registry,\n",
        "    mlflow_client=mlflow_client,\n",
        "    gpu_monitor=gpu_monitor,\n",
        "    energy_tracker=energy_tracker\n",
        ")\n",
        "\n",
        "# Add experiment_id to config for training manager\n",
        "config.experiment_id = experiment_id\n",
        "\n",
        "print(\"üöÄ Starting comprehensive training with full tracking...\")\n",
        "print(f\"   - MLflow experiment tracking: ‚úÖ\")\n",
        "print(f\"   - GPU energy monitoring: {'‚úÖ' if config.enable_energy_tracking else '‚ùå'}\")\n",
        "print(f\"   - Performance metrics (ASTR-102): ‚úÖ\")\n",
        "print(f\"   - Model checkpointing: ‚úÖ\")\n",
        "\n",
        "# Start training\n",
        "try:\n",
        "    run_id = await training_manager.start_training_run(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        scheduler=scheduler\n",
        "    )\n",
        "    \n",
        "    print(f\"üéâ Training completed successfully!\")\n",
        "    print(f\"üìä MLflow Run ID: {run_id}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Visualization and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comprehensive training curves\n",
        "print(\"üìä Generating training visualizations...\")\n",
        "training_manager.plot_training_summary()\n",
        "\n",
        "# Get training summary\n",
        "training_summary = training_manager.get_training_summary()\n",
        "print(f\"\\nüìà Training Summary:\")\n",
        "print(f\"   Best validation loss: {training_summary['best_val_loss']:.4f}\")\n",
        "print(f\"   Total epochs: {training_summary['total_epochs']}\")\n",
        "print(f\"   Final train loss: {training_summary['final_train_loss']:.4f}\")\n",
        "print(f\"   Final val loss: {training_summary['final_val_loss']:.4f}\")\n",
        "\n",
        "# Display final metrics\n",
        "final_metrics = training_summary['final_val_metrics']\n",
        "if final_metrics:\n",
        "    print(f\"\\nüéØ Final Validation Metrics:\")\n",
        "    print(f\"   Accuracy: {final_metrics.get('accuracy', 0.0):.4f}\")\n",
        "    print(f\"   Precision: {final_metrics.get('precision_macro', 0.0):.4f}\")\n",
        "    print(f\"   Recall: {final_metrics.get('recall_macro', 0.0):.4f}\")\n",
        "    print(f\"   F1 Score: {final_metrics.get('f1_macro', 0.0):.4f}\")\n",
        "    print(f\"   AUROC: {final_metrics.get('auroc', 0.0):.4f}\")\n",
        "    print(f\"   AUPRC: {final_metrics.get('auprc', 0.0):.4f}\")\n",
        "    print(f\"   MCC: {final_metrics.get('mcc', 0.0):.4f}\")\n",
        "    print(f\"   Balanced Accuracy: {final_metrics.get('balanced_accuracy', 0.0):.4f}\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    print(f\"\\n‚ö° Performance Metrics:\")\n",
        "    print(f\"   Latency P50: {final_metrics.get('latency_ms_p50', 0.0):.2f} ms\")\n",
        "    print(f\"   Latency P95: {final_metrics.get('latency_ms_p95', 0.0):.2f} ms\")\n",
        "    print(f\"   Throughput: {final_metrics.get('throughput_items_per_s', 0.0):.2f} items/s\")\n",
        "    \n",
        "    # Energy metrics (if available)\n",
        "    if config.enable_energy_tracking:\n",
        "        print(f\"\\nüîã Energy Metrics:\")\n",
        "        print(f\"   Energy consumed: {final_metrics.get('training_energy_wh', 0.0):.3f} Wh\")\n",
        "        print(f\"   Average power: {final_metrics.get('training_avg_power_w', 0.0):.1f} W\")\n",
        "        print(f\"   Peak power: {final_metrics.get('training_peak_power_w', 0.0):.1f} W\")\n",
        "        print(f\"   Carbon footprint: {final_metrics.get('training_carbon_footprint_kg', 0.0):.6f} kg CO2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for evaluation\n",
        "import time\n",
        "best_checkpoint_path = training_manager.checkpoint_manager.checkpoint_dir / \"best_model.pt\"\n",
        "if best_checkpoint_path.exists():\n",
        "    checkpoint = training_manager.checkpoint_manager.load_checkpoint(str(best_checkpoint_path))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"‚úÖ Loaded best model for evaluation\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Best model checkpoint not found, using current model\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"üß™ Evaluating model on test set...\")\n",
        "model.eval()\n",
        "\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "all_scores = []\n",
        "inference_times = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(training_manager.device), target.to(training_manager.device)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        output = model(data)\n",
        "        inference_time = time.time() - start_time\n",
        "        inference_times.append(inference_time)\n",
        "        \n",
        "        predictions = (torch.sigmoid(output) > config.confidence_threshold).float()\n",
        "        scores = torch.sigmoid(output).cpu().detach().numpy().flatten()\n",
        "        \n",
        "        all_predictions.extend(predictions.cpu().detach().numpy().flatten())\n",
        "        all_targets.extend(target.cpu().detach().numpy().flatten())\n",
        "        all_scores.extend(scores)\n",
        "\n",
        "# Calculate comprehensive test metrics\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "all_scores = np.array(all_scores)\n",
        "\n",
        "test_metrics = training_manager.metrics_calculator.calculate_all_metrics(\n",
        "    all_targets, all_predictions, all_scores, inference_times, config.batch_size\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ Test Set Results:\")\n",
        "print(f\"   Accuracy: {test_metrics.get('accuracy', 0.0):.4f}\")\n",
        "print(f\"   Precision: {test_metrics.get('precision_macro', 0.0):.4f}\")\n",
        "print(f\"   Recall: {test_metrics.get('recall_macro', 0.0):.4f}\")\n",
        "print(f\"   F1 Score: {test_metrics.get('f1_macro', 0.0):.4f}\")\n",
        "print(f\"   AUROC: {test_metrics.get('auroc', 0.0):.4f}\")\n",
        "print(f\"   AUPRC: {test_metrics.get('auprc', 0.0):.4f}\")\n",
        "print(f\"   MCC: {test_metrics.get('mcc', 0.0):.4f}\")\n",
        "print(f\"   Balanced Accuracy: {test_metrics.get('balanced_accuracy', 0.0):.4f}\")\n",
        "\n",
        "# Performance metrics\n",
        "print(f\"\\n‚ö° Test Performance:\")\n",
        "print(f\"   Latency P50: {test_metrics.get('latency_ms_p50', 0.0):.2f} ms\")\n",
        "print(f\"   Latency P95: {test_metrics.get('latency_ms_p95', 0.0):.2f} ms\")\n",
        "print(f\"   Throughput: {test_metrics.get('throughput_items_per_s', 0.0):.2f} items/s\")\n",
        "\n",
        "# Generate visualizations\n",
        "from notebooks.training.utils.training_utils import TrainingVisualizer\n",
        "visualizer = TrainingVisualizer()\n",
        "\n",
        "print(\"\\nüìä Generating evaluation visualizations...\")\n",
        "visualizer.plot_confusion_matrix(all_targets, all_predictions)\n",
        "visualizer.plot_roc_curve(all_targets, all_scores)\n",
        "visualizer.plot_precision_recall_curve(all_targets, all_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Troubleshooting and Debugging Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model debugging and inspection tools\n",
        "def inspect_model_predictions(model, data_loader, num_samples=5):\n",
        "    \"\"\"Inspect model predictions for debugging.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (data, target) in enumerate(data_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "                \n",
        "            data = data.to(training_manager.device)\n",
        "            output = model(data)\n",
        "            predictions = torch.sigmoid(output)\n",
        "            \n",
        "            # Convert to numpy for visualization\n",
        "            image = data[0].cpu().numpy().squeeze()\n",
        "            target_mask = target[0].cpu().numpy().squeeze()\n",
        "            pred_mask = (predictions[0].cpu().numpy().squeeze() > config.confidence_threshold).astype(float)\n",
        "            confidence = predictions[0].cpu().numpy().squeeze()\n",
        "            \n",
        "            # Create visualization\n",
        "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "            \n",
        "            axes[0].imshow(image, cmap='gray')\n",
        "            axes[0].set_title('Input Image')\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            axes[1].imshow(target_mask, cmap='hot')\n",
        "            axes[1].set_title('Ground Truth')\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            axes[2].imshow(pred_mask, cmap='hot')\n",
        "            axes[2].set_title('Prediction')\n",
        "            axes[2].axis('off')\n",
        "            \n",
        "            im = axes[3].imshow(confidence, cmap='viridis')\n",
        "            axes[3].set_title('Confidence Map')\n",
        "            axes[3].axis('off')\n",
        "            plt.colorbar(im, ax=axes[3])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # Print statistics\n",
        "            print(f\"Sample {i+1}:\")\n",
        "            print(f\"  Target pixels: {np.sum(target_mask):.0f}\")\n",
        "            print(f\"  Predicted pixels: {np.sum(pred_mask):.0f}\")\n",
        "            print(f\"  Confidence range: [{np.min(confidence):.3f}, {np.max(confidence):.3f}]\")\n",
        "            print(f\"  IoU: {np.sum((target_mask > 0) & (pred_mask > 0)) / np.sum((target_mask > 0) | (pred_mask > 0)):.3f}\")\n",
        "            print()\n",
        "\n",
        "def analyze_training_issues():\n",
        "    \"\"\"Analyze potential training issues.\"\"\"\n",
        "    print(\"üîç Training Analysis:\")\n",
        "    \n",
        "    # Check for overfitting\n",
        "    train_losses = training_summary['training_history']['train_losses']\n",
        "    val_losses = training_summary['training_history']['val_losses']\n",
        "    \n",
        "    if len(train_losses) > 5 and len(val_losses) > 5:\n",
        "        train_trend = np.mean(train_losses[-5:]) - np.mean(train_losses[:5])\n",
        "        val_trend = np.mean(val_losses[-5:]) - np.mean(val_losses[:5])\n",
        "        \n",
        "        if val_trend > train_trend * 1.5:\n",
        "            print(\"‚ö†Ô∏è  Potential overfitting detected - validation loss increasing while training loss decreasing\")\n",
        "        elif val_trend < -0.1:\n",
        "            print(\"‚úÖ Good training progress - both losses decreasing\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Training appears stable\")\n",
        "    \n",
        "    # Check learning rate\n",
        "    lr_history = training_summary['training_history']['learning_rates']\n",
        "    if len(lr_history) > 1:\n",
        "        lr_change = (lr_history[-1] - lr_history[0]) / lr_history[0]\n",
        "        if lr_change < -0.5:\n",
        "            print(\"‚ÑπÔ∏è  Learning rate significantly reduced during training\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Learning rate relatively stable\")\n",
        "    \n",
        "    # Check convergence\n",
        "    if len(val_losses) > 10:\n",
        "        recent_val_losses = val_losses[-10:]\n",
        "        val_std = np.std(recent_val_losses)\n",
        "        if val_std < 0.01:\n",
        "            print(\"‚úÖ Model appears to have converged\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Model may still be learning\")\n",
        "\n",
        "# Run debugging tools\n",
        "print(\"üîß Running debugging and analysis tools...\")\n",
        "inspect_model_predictions(model, test_loader, num_samples=3)\n",
        "analyze_training_issues()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Summary and Next Steps\n",
        "print(\"üéâ ASTR-106 Training Notebook Complete!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä What was accomplished:\")\n",
        "print(\"‚úÖ Complete MLflow experiment tracking (ASTR-88 integration)\")\n",
        "print(\"‚úÖ GPU energy monitoring and carbon footprint tracking (ASTR-101)\")\n",
        "print(\"‚úÖ Comprehensive performance metrics (ASTR-102)\")\n",
        "print(\"‚úÖ Data preprocessing integration (ASTR-76)\")\n",
        "print(\"‚úÖ U-Net model training with PyTorch\")\n",
        "print(\"‚úÖ Model checkpointing and versioning\")\n",
        "print(\"‚úÖ Visualization and debugging tools\")\n",
        "print(\"‚úÖ Model evaluation and testing\")\n",
        "\n",
        "print(f\"\\nüìà Training Results:\")\n",
        "print(f\"   MLflow Run ID: {run_id}\")\n",
        "print(f\"   Best validation loss: {training_summary['best_val_loss']:.4f}\")\n",
        "print(f\"   Final test accuracy: {test_metrics.get('accuracy', 0.0):.4f}\")\n",
        "print(f\"   Final test F1 score: {test_metrics.get('f1_macro', 0.0):.4f}\")\n",
        "\n",
        "if config.enable_energy_tracking:\n",
        "    print(f\"\\nüîã Energy Impact:\")\n",
        "    print(f\"   Total energy consumed: {test_metrics.get('training_energy_wh', 0.0):.3f} Wh\")\n",
        "    print(f\"   Carbon footprint: {test_metrics.get('training_carbon_footprint_kg', 0.0):.6f} kg CO2\")\n",
        "\n",
        "print(f\"\\nüìÅ Outputs:\")\n",
        "print(f\"   Model checkpoints: {training_manager.checkpoint_manager.checkpoint_dir}\")\n",
        "print(f\"   MLflow artifacts: {config.mlflow_tracking_uri}\")\n",
        "print(f\"   Training logs: Available in MLflow UI\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(\"1. Review results in MLflow UI\")\n",
        "print(\"2. Deploy best model to production\")\n",
        "print(\"3. Set up automated retraining pipeline\")\n",
        "print(\"4. Monitor model performance in production\")\n",
        "print(\"5. Collect more training data for improvement\")\n",
        "\n",
        "print(f\"\\nüîó Useful Links:\")\n",
        "print(f\"   MLflow UI: {config.mlflow_tracking_uri}\")\n",
        "print(f\"   Model Registry: {config.mlflow_tracking_uri}/#/models\")\n",
        "print(f\"   Experiment: {config.mlflow_tracking_uri}/#/experiments/{experiment_id}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéØ ASTR-106 Implementation Complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
