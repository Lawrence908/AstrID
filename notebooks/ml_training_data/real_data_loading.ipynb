{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASTR-113: Real Data Loading Smoke Test\n",
        "\n",
        "This notebook validates the real-data collection and loading pipeline:\n",
        "- Collect validated detections into a `TrainingDataset`\n",
        "- Persist `TrainingSample` rows\n",
        "- Load the dataset and create train/val/test splits\n",
        "\n",
        "Run this before integrating with `notebooks/training/model_training.ipynb`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root added: /home/chris/github/AstrID\n"
          ]
        }
      ],
      "source": [
        "# Setup imports and environment\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path.cwd().parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root added: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'http://localhost:8000'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "API_BASE = os.environ.get(\"ASTRID_API_BASE\", \"http://localhost:8000\")\n",
        "API_BASE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'success',\n",
              " 'data': {'dataset_id': 'b407e1c8-df17-4d03-9e22-df5d404200c0',\n",
              "  'name': 'smoketest_hst_2024',\n",
              "  'total': 0,\n",
              "  'quality': {'anomaly_ratio': 0.0, 'quality_score': 0.0}},\n",
              " 'meta': {},\n",
              " 'error': None}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Collect a small dataset via API\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "params = {\n",
        "    \"survey_ids\": [\"hst\"],\n",
        "    \"start\": \"2024-01-01T00:00:00\",\n",
        "    \"end\": \"2024-12-31T23:59:59\",\n",
        "    \"confidence_threshold\": 0.7,\n",
        "    \"max_samples\": 50,\n",
        "    \"name\": \"smoketest_hst_2024\"\n",
        "}\n",
        "\n",
        "r = requests.post(f\"{API_BASE}/training/datasets/collect\", json=params, timeout=60)\n",
        "r.raise_for_status()\n",
        "resp = r.json()\n",
        "resp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "500 Server Error: Internal Server Error for url: http://localhost:8000/training/datasets",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Verify dataset is listed\u001b[39;00m\n\u001b[32m      2\u001b[39m r = requests.get(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/training/datasets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m datasets = r.json()[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDatasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(datasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/AstrID/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 500 Server Error: Internal Server Error for url: http://localhost:8000/training/datasets"
          ]
        }
      ],
      "source": [
        "# Verify dataset is listed\n",
        "r = requests.get(f\"{API_BASE}/training/datasets\")\n",
        "r.raise_for_status()\n",
        "datasets = r.json()[\"data\"]\n",
        "\n",
        "print(f\"Datasets: {len(datasets)}\")\n",
        "# Show the most recent one\n",
        "sorted(datasets, key=lambda d: d.get(\"created_at\", \"\"), reverse=True)[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect dataset via API (no direct DB access needed in notebook)\n",
        "# Resolve dataset_id from prior response or fallback to latest dataset via API\n",
        "if isinstance(resp, dict) and \"data\" in resp and resp[\"data\"].get(\"dataset_id\"):\n",
        "    dataset_id = resp[\"data\"][\"dataset_id\"]\n",
        "else:\n",
        "    r = requests.get(f\"{API_BASE}/training/datasets\")\n",
        "    r.raise_for_status()\n",
        "    datasets = r.json().get(\"data\", [])\n",
        "    if not datasets:\n",
        "        raise RuntimeError(\"No training datasets available\")\n",
        "    # pick the most recent\n",
        "    datasets_sorted = sorted(datasets, key=lambda d: d.get(\"created_at\", \"\"), reverse=True)\n",
        "    dataset_id = str(datasets_sorted[0][\"id\"])  # ensure string\n",
        "\n",
        "print(\"Dataset ID:\", dataset_id)\n",
        "\n",
        "# Get dataset details\n",
        "detail = requests.get(f\"{API_BASE}/training/datasets/{dataset_id}\")\n",
        "detail.raise_for_status()\n",
        "detail.json()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: preview a few sample image paths from DB via API\n",
        "# (Future enhancement: API could return sample listings)\n",
        "print(\"For now, verify counts above. Integration with training notebook will consume by dataset_id.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
