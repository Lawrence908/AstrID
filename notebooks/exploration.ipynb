{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c1992b",
   "metadata": {},
   "source": [
    "# AstrID Exploratory Data Preparation\n",
    "\n",
    "This notebook implements the exploratory data preparation pipeline for anomaly detection:\n",
    "1. **Data Acquisition**: Fetch FITS files from MAST/SkyView APIs\n",
    "2. **Image Differencing**: Compute differences from reference images using ZOGY algorithm\n",
    "3. **Anomaly Preparation**: Prepare data for anomaly detection in subsequent notebooks\n",
    "4. **Data Sources**: Explore ZTF data and synthetic anomaly generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "Following the plan in `EXPLORATORY_NOTEBOOK_PLAN.md`, this notebook will:\n",
    "- Fetch source FITS files from multiple astronomical surveys\n",
    "- Process and align images using WCS\n",
    "- Compute difference images (ZOGY and classic methods)\n",
    "- Extract source candidates from difference images\n",
    "- Prepare datasets for anomaly detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5693bfb",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Set up paths, imports, and configuration for the exploratory pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/chris/github/AstrID\n",
      "Current working directory: /home/chris/github/AstrID/notebooks\n",
      "   Cleared SUPABASE_SSL_CERT_PATH (pointed to non-existent file)\n",
      "   Cleared REQUESTS_CA_BUNDLE (pointed to non-existent file)\n",
      "‚úÖ SSL configured to use system certificates: /home/chris/github/AstrID/.venv/lib/python3.12/site-packages/certifi/cacert.pem\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import ssl\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# SSL Configuration - Use system default certificates for exploration\n",
    "# This fixes the \"invalid path: certs/prod-ca-2021.crt\" error\n",
    "# The issue is that some code might be looking for a cert file that doesn't exist\n",
    "\n",
    "# Clear any existing cert paths that point to non-existent files\n",
    "cert_path = project_root / \"certs\" / \"prod-ca-2021.crt\"\n",
    "if not cert_path.exists():\n",
    "    # Remove environment variables that point to non-existent cert\n",
    "    for env_var in ['SUPABASE_SSL_CERT_PATH', 'REQUESTS_CA_BUNDLE', 'SSL_CERT_FILE']:\n",
    "        if env_var in os.environ:\n",
    "            old_path = os.environ[env_var]\n",
    "            if 'certs/prod-ca-2021.crt' in old_path or not os.path.exists(old_path):\n",
    "                del os.environ[env_var]\n",
    "                print(f\"   Cleared {env_var} (pointed to non-existent file)\")\n",
    "\n",
    "# Use certifi's CA bundle (system certificates)\n",
    "try:\n",
    "    import certifi\n",
    "    # Use certifi's CA bundle for requests and SSL\n",
    "    os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "    os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "    print(f\"‚úÖ SSL configured to use system certificates: {certifi.where()}\")\n",
    "except ImportError:\n",
    "    # If certifi not available, use default SSL context (will use system certs)\n",
    "    print(\"‚ö†Ô∏è  certifi not available, using default SSL context\")\n",
    "    # For requests specifically, we can disable verification in exploratory mode\n",
    "    import warnings\n",
    "    try:\n",
    "        import urllib3\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        print(\"‚ö†Ô∏è  SSL verification warnings disabled for exploration\")\n",
    "    except ImportError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f21a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core scientific libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.wcs import WCS\n",
    "import astropy.time as time\n",
    "\n",
    "# Install nest_asyncio if needed (for Jupyter async support)\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio available (async support enabled)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  nest_asyncio not installed - async functions may fail in Jupyter\")\n",
    "    print(\"   Install with: pip install nest-asyncio\")\n",
    "    print(\"   Or run: !pip install nest-asyncio\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 13:12:06,453 - INFO - Using SSL_CERT_FILE bundle at /home/chris/github/AstrID/.venv/lib/python3.12/site-packages/certifi/cacert.pem\n",
      "2025-11-04 13:12:06,471 - INFO - SSL context created using SSL_CERT_FILE\n",
      "2025-11-04 13:12:06,472 - INFO - Creating database engine with URL: postgresql+asyncpg://postgres.vqplumkrlkgrsnnkptqp:****@aws-1-us-west-1.pooler.supabase.com/postgres\n",
      "2025-11-04 13:12:06,505 - INFO - Database engine created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AstrID modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import AstrID modules\n",
    "try:\n",
    "    from src.adapters.external.mast import MASTClient\n",
    "    from src.adapters.external.skyview import SkyViewClient\n",
    "    from src.adapters.imaging.fits_io import FITSProcessor\n",
    "    from src.domains.preprocessing.processors.astronomical_image_processing import ImageDifferencingProcessor\n",
    "    from src.domains.preprocessing.processors.fits_processing import AdvancedFITSProcessor\n",
    "    print(\"‚úÖ AstrID modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Some AstrID modules not available: {e}\")\n",
    "    print(\"Some functionality may be limited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad6c576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits/science\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits/reference\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/processed\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/processed/aligned\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/processed/normalized\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/differences\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/differences/zogy\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/differences/classic\n",
      "‚úÖ Created directory: /home/chris/github/AstrID/notebooks/data/exploratory/metadata\n",
      "\n",
      "üìÅ Data directory structure ready at: /home/chris/github/AstrID/notebooks/data/exploratory\n"
     ]
    }
   ],
   "source": [
    "# Create data directory structure\n",
    "data_dir = project_root / \"notebooks\" / \"data\" / \"exploratory\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subdirectories\n",
    "dirs = {\n",
    "    'source_fits': data_dir / \"source_fits\",\n",
    "    'science': data_dir / \"source_fits\" / \"science\",\n",
    "    'reference': data_dir / \"source_fits\" / \"reference\",\n",
    "    'processed': data_dir / \"processed\",\n",
    "    'aligned': data_dir / \"processed\" / \"aligned\",\n",
    "    'normalized': data_dir / \"processed\" / \"normalized\",\n",
    "    'differences': data_dir / \"differences\",\n",
    "    'zogy': data_dir / \"differences\" / \"zogy\",\n",
    "    'classic': data_dir / \"differences\" / \"classic\",\n",
    "    'metadata': data_dir / \"metadata\",\n",
    "}\n",
    "\n",
    "# Create all directories\n",
    "for name, path in dirs.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Created directory: {path}\")\n",
    "\n",
    "print(f\"\\nüìÅ Data directory structure ready at: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950acbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration saved\n",
      "üìã Target regions: 3\n",
      "üìã Max observations: 10\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Target sky regions (RA, Dec in degrees, radius in degrees)\n",
    "    'target_regions': [\n",
    "        {'name': 'M101_field', 'ra': 210.802, 'dec': 54.349, 'radius': 0.1},\n",
    "        {'name': 'SN2011fe_field', 'ra': 165.360, 'dec': 17.419, 'radius': 0.1},\n",
    "        {'name': 'COSMOS_field', 'ra': 150.0, 'dec': 2.0, 'radius': 0.1},\n",
    "    ],\n",
    "    \n",
    "    # Survey selection\n",
    "    'mast_missions': ['PanSTARRS', 'HST', 'JWST'],  # MAST missions to query\n",
    "    'skyview_surveys': ['DSS2 Red', 'SDSS DR12'],  # SkyView surveys\n",
    "    \n",
    "    # Image parameters\n",
    "    'image_size_pixels': 240,  # Standard image size\n",
    "    'pixel_scale_arcsec': 0.25,  # Pixel scale in arcseconds\n",
    "    \n",
    "    # Differencing parameters\n",
    "    'differencing_method': 'zogy',  # 'zogy' or 'classic'\n",
    "    'source_detection_snr_threshold': 5.0,  # Minimum SNR for source detection\n",
    "    \n",
    "    # Processing flags\n",
    "    'max_observations': 10,  # Limit for initial exploration\n",
    "    'save_intermediate': True,  # Save intermediate processing steps\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_file = dirs['metadata'] / 'config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "    \n",
    "print(\"‚úÖ Configuration saved\")\n",
    "print(f\"üìã Target regions: {len(CONFIG['target_regions'])}\")\n",
    "print(f\"üìã Max observations: {CONFIG['max_observations']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0ba47",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "\n",
    "Fetch FITS files from MAST and SkyView APIs. We'll start with a small number of observations to validate the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671731d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MAST client initialized\n",
      "‚úÖ SkyView client initialized\n",
      "‚úÖ FITS processor initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize API clients\n",
    "try:\n",
    "    mast_client = MASTClient(timeout=30)\n",
    "    print(\"‚úÖ MAST client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: MAST client initialization failed: {e}\")\n",
    "    mast_client = None\n",
    "\n",
    "try:\n",
    "    skyview_client = SkyViewClient(timeout=60)\n",
    "    print(\"‚úÖ SkyView client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: SkyView client initialization failed: {e}\")\n",
    "    skyview_client = None\n",
    "\n",
    "# Initialize FITS processor\n",
    "fits_processor = FITSProcessor()\n",
    "print(\"‚úÖ FITS processor initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef93ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper function to save observation metadata\n",
    "def save_observation_metadata(obs_data, filepath):\n",
    "    \"\"\"Save observation metadata to JSON file.\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(obs_data, f, indent=2, default=str)\n",
    "\n",
    "# Helper function to load FITS and extract basic info\n",
    "def get_fits_info(filepath):\n",
    "    \"\"\"Get basic information from a FITS file.\"\"\"\n",
    "    try:\n",
    "        with fits.open(filepath) as hdul:\n",
    "            hdu = hdul[0]\n",
    "            info = {\n",
    "                'filepath': str(filepath),\n",
    "                'shape': hdu.data.shape if hdu.data is not None else None,\n",
    "                'dtype': str(hdu.data.dtype) if hdu.data is not None else None,\n",
    "                'header_keys': list(hdu.header.keys())[:20],  # First 20 keys\n",
    "                'wcs': None,\n",
    "            }\n",
    "            \n",
    "            # Try to extract WCS\n",
    "            try:\n",
    "                wcs = WCS(hdu.header)\n",
    "                if wcs.is_celestial:\n",
    "                    info['wcs'] = {\n",
    "                        'has_celestial': True,\n",
    "                        'naxis': wcs.naxis,\n",
    "                    }\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            return info\n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'filepath': str(filepath)}\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3473a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching reference image for region: M101_field\n",
      "Coordinates: RA=210.802¬∞, Dec=54.349¬∞\n",
      "‚úÖ SSL configured for requests: /home/chris/github/AstrID/.venv/lib/python3.12/site-packages/certifi/cacert.pem\n",
      "‚úÖ Successfully fetched PS1 reference image\n",
      "   Source: ps1\n",
      "   Format: jpeg\n",
      "   Shape: (6302, 6283)\n",
      "   Saved to: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits/reference/reference_ps1_M101_field_g.fits\n"
     ]
    }
   ],
   "source": [
    "# Fetch reference images using PS1 cutout helper (quick start)\n",
    "# This uses the MASTClient.fetch_ps1_cutout() static method\n",
    "\n",
    "reference_images = []\n",
    "target_region = CONFIG['target_regions'][0]  # Start with first region\n",
    "\n",
    "print(f\"Fetching reference image for region: {target_region['name']}\")\n",
    "print(f\"Coordinates: RA={target_region['ra']:.3f}¬∞, Dec={target_region['dec']:.3f}¬∞\")\n",
    "\n",
    "# Ensure SSL is properly configured before making requests\n",
    "# Patch requests to use system certificates if needed\n",
    "try:\n",
    "    import requests\n",
    "    import certifi\n",
    "    \n",
    "    # Override requests to use certifi's CA bundle\n",
    "    if hasattr(requests, 'adapters'):\n",
    "        # This ensures requests uses the system certs\n",
    "        requests.packages.urllib3.util.ssl_.DEFAULT_CA_BUNDLE_PATH = certifi.where()\n",
    "    \n",
    "    # Also set the environment variable (in case it wasn't set earlier)\n",
    "    if 'REQUESTS_CA_BUNDLE' not in os.environ:\n",
    "        os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "    \n",
    "    print(f\"‚úÖ SSL configured for requests: {certifi.where()}\")\n",
    "except Exception as ssl_err:\n",
    "    print(f\"‚ö†Ô∏è  SSL setup warning: {ssl_err}\")\n",
    "    print(\"   Will attempt to use default SSL context\")\n",
    "\n",
    "try:\n",
    "    # Use the static helper method\n",
    "    image_data, info = MASTClient.fetch_ps1_cutout(\n",
    "        ra_deg=target_region['ra'],\n",
    "        dec_deg=target_region['dec'],\n",
    "        size_pixels=CONFIG['image_size_pixels'],\n",
    "        filt='g',  # g-band filter\n",
    "    )\n",
    "    \n",
    "    if image_data is not None:\n",
    "        print(f\"‚úÖ Successfully fetched PS1 reference image\")\n",
    "        print(f\"   Source: {info.get('source', 'unknown')}\")\n",
    "        print(f\"   Format: {info.get('format', 'unknown')}\")\n",
    "        print(f\"   Shape: {image_data.shape}\")\n",
    "        \n",
    "        # Save as reference image\n",
    "        ref_filename = f\"reference_ps1_{target_region['name']}_g.fits\"\n",
    "        ref_path = dirs['reference'] / ref_filename\n",
    "        \n",
    "        # Create FITS file from numpy array\n",
    "        hdu = fits.PrimaryHDU(image_data)\n",
    "        hdu.header['RA'] = target_region['ra']\n",
    "        hdu.header['DEC'] = target_region['dec']\n",
    "        hdu.header['SURVEY'] = 'PS1'\n",
    "        hdu.header['FILTER'] = 'g'\n",
    "        hdu.writeto(ref_path, overwrite=True)\n",
    "        \n",
    "        reference_images.append({\n",
    "            'filename': ref_filename,\n",
    "            'path': str(ref_path),\n",
    "            'region': target_region['name'],\n",
    "            'ra': target_region['ra'],\n",
    "            'dec': target_region['dec'],\n",
    "            'info': info,\n",
    "        })\n",
    "        \n",
    "        print(f\"   Saved to: {ref_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Failed to fetch PS1 image: {info.get('error', 'Unknown error')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching PS1 reference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # If SSL is still the issue, provide helpful message\n",
    "    if \"certificate\" in str(e).lower() or \"ssl\" in str(e).lower() or \"tls\" in str(e).lower():\n",
    "        print(\"\\nüí° SSL Certificate Issue Detected\")\n",
    "        print(\"   The code is looking for a certificate file that doesn't exist.\")\n",
    "        print(\"   Options:\")\n",
    "        print(\"   1. Install certifi: pip install certifi\")\n",
    "        print(\"   2. Or use system certificates (configured above)\")\n",
    "        print(\"   3. For exploration only, you can disable SSL verification (not recommended for production)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Event loop issue detected\n",
      "   Installing nest_asyncio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-11-04 13:23:19,715 - INFO - Querying MAST for position (210.802, 54.349) with radius 0.1¬∞\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio installed and applied\n",
      "\n",
      "üîç Querying MAST for region: M101_field\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 13:23:31,675 - INFO - MAST query returned 4291 raw observations\n",
      "2025-11-04 13:23:31,836 - INFO - Found 2413 observations from MAST (filtered 1878 total)\n",
      "2025-11-04 13:23:31,837 - INFO - Filtered for missions: ['PanSTARRS', 'HST', 'JWST']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 2413 observations\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: MESSIER-101\n",
      "   - JWST: M101-NUCLEUS+H602\n",
      "   - JWST: M101-NUCLEUS+H602\n",
      "   - JWST: M101-NUCLEUS+H602\n",
      "\n",
      "üìä Total observations found: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70304/1073279971.py:59: RuntimeWarning: coroutine 'fetch_mast_observations' was never awaited\n",
      "  science_observations = asyncio.run(fetch_mast_observations())\n"
     ]
    }
   ],
   "source": [
    "# Fetch science images using MAST API\n",
    "# Note: In Jupyter notebooks, we need to handle async differently\n",
    "import asyncio\n",
    "\n",
    "async def fetch_mast_observations():\n",
    "    \"\"\"Fetch observations from MAST for target regions.\"\"\"\n",
    "    if mast_client is None:\n",
    "        print(\"‚ö†Ô∏è  MAST client not available, skipping\")\n",
    "        return []\n",
    "    \n",
    "    all_observations = []\n",
    "    \n",
    "    for region in CONFIG['target_regions'][:1]:  # Start with first region\n",
    "        print(f\"\\nüîç Querying MAST for region: {region['name']}\")\n",
    "        \n",
    "        try:\n",
    "            observations = await mast_client.query_observations_by_position(\n",
    "                ra=region['ra'],\n",
    "                dec=region['dec'],\n",
    "                radius=region['radius'],\n",
    "                missions=CONFIG['mast_missions'],\n",
    "            )\n",
    "            \n",
    "            print(f\"   Found {len(observations)} observations\")\n",
    "            \n",
    "            # Limit to max_observations\n",
    "            observations = observations[:CONFIG['max_observations']]\n",
    "            \n",
    "            for obs in observations:\n",
    "                obs['region'] = region['name']\n",
    "                all_observations.append(obs)\n",
    "                print(f\"   - {obs.get('mission', 'Unknown')}: {obs.get('target_name', 'N/A')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error querying MAST for {region['name']}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return all_observations\n",
    "\n",
    "# Run async function - Jupyter-safe\n",
    "# If nest_asyncio is installed (should be from cell 3), this will work\n",
    "try:\n",
    "    # Try using asyncio.run() - will work if nest_asyncio was applied\n",
    "    science_observations = asyncio.run(fetch_mast_observations())\n",
    "except RuntimeError as e:\n",
    "    if \"cannot be called from a running event loop\" in str(e):\n",
    "        print(\"‚ö†Ô∏è  Event loop issue detected\")\n",
    "        print(\"   Installing nest_asyncio...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nest-asyncio\", \"-q\"])\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            print(\"‚úÖ nest_asyncio installed and applied\")\n",
    "            # Try again\n",
    "            science_observations = asyncio.run(fetch_mast_observations())\n",
    "        except Exception as install_err:\n",
    "            print(f\"‚ùå Could not install nest_asyncio: {install_err}\")\n",
    "            print(\"   Please install manually: pip install nest-asyncio\")\n",
    "            print(\"   Or use: await fetch_mast_observations() in an async cell\")\n",
    "            science_observations = []\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"\\nüìä Total observations found: {len(science_observations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b4909",
   "metadata": {},
   "source": [
    "## 2.2 Download and Process Science Images\n",
    "\n",
    "Now we'll download actual FITS files from the MAST observations we found, process them, and prepare them for differencing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872405e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process actual science images from MAST observations\n",
    "# This is the real data we'll use for differencing and anomaly detection\n",
    "\n",
    "science_images = []  # Store multiple science images\n",
    "science_image_data = None  # Primary science image for processing\n",
    "\n",
    "if science_observations:\n",
    "    print(f\"üì• {len(science_observations)} science observations available\")\n",
    "    print(\"   Downloading and processing actual FITS files from MAST...\")\n",
    "    \n",
    "    async def download_and_process_observations():\n",
    "        \"\"\"Download FITS files from MAST observations.\"\"\"\n",
    "        downloaded_images = []\n",
    "        \n",
    "        for i, obs in enumerate(science_observations[:CONFIG['max_observations']], 1):\n",
    "            obs_id = obs.get('obs_id')\n",
    "            mission = obs.get('mission', 'Unknown')\n",
    "            \n",
    "            print(f\"\\n[{i}/{len(science_observations[:CONFIG['max_observations']])}] Processing {mission} observation: {obs_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Method 1: Try using astroquery.mast directly\n",
    "                try:\n",
    "                    from astroquery.mast import Observations\n",
    "                    \n",
    "                    # Get data products for this observation\n",
    "                    print(f\"   Getting data products...\")\n",
    "                    products = Observations.get_product_list(obs_id)\n",
    "                    \n",
    "                    if len(products) == 0:\n",
    "                        print(f\"   ‚ö†Ô∏è  No data products found for {obs_id}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter for science products (FITS files)\n",
    "                    science_products = products[\n",
    "                        (products['productSubGroupDescription'] == 'SCI') | \n",
    "                        (products['productSubGroupDescription'] == 'DRZ') |\n",
    "                        (products['productSubGroupDescription'].str.contains('FITS', case=False, na=False))\n",
    "                    ]\n",
    "                    \n",
    "                    if len(science_products) == 0:\n",
    "                        # Fallback: take first product\n",
    "                        science_products = products[:1]\n",
    "                        print(f\"   ‚ö†Ô∏è  No specific science products, using first available\")\n",
    "                    \n",
    "                    print(f\"   Found {len(science_products)} science products\")\n",
    "                    \n",
    "                    # Download the first science product\n",
    "                    product = science_products[0]\n",
    "                    file_size_mb = product.get('size', 0) / 1024 / 1024 if 'size' in product.columns else 0\n",
    "                    print(f\"   Downloading: {product['productFilename']} ({file_size_mb:.2f} MB)\")\n",
    "                    \n",
    "                    # Download to temporary location\n",
    "                    download_dir = dirs['science'] / 'downloads'\n",
    "                    download_dir.mkdir(exist_ok=True)\n",
    "                    \n",
    "                    manifest = Observations.download_products(\n",
    "                        product,\n",
    "                        download_dir=str(download_dir),\n",
    "                        mrp_only=False\n",
    "                    )\n",
    "                    \n",
    "                    # Find the downloaded file\n",
    "                    downloaded_file = None\n",
    "                    if 'Local Path' in manifest.columns:\n",
    "                        downloaded_file = manifest['Local Path'][0]\n",
    "                    else:\n",
    "                        # Search for the file\n",
    "                        downloaded_files = list(download_dir.glob(f\"**/{product['productFilename']}\"))\n",
    "                        if downloaded_files:\n",
    "                            downloaded_file = str(downloaded_files[0])\n",
    "                    \n",
    "                    if downloaded_file and os.path.exists(downloaded_file):\n",
    "                        print(f\"   ‚úÖ Downloaded: {os.path.basename(downloaded_file)}\")\n",
    "                        \n",
    "                        # Load and process the FITS file\n",
    "                        try:\n",
    "                            image_data, wcs_info, metadata = fits_processor.read_fits(downloaded_file)\n",
    "                            \n",
    "                            # Basic processing: normalize and resize if needed\n",
    "                            # Remove NaN/inf values\n",
    "                            image_data = np.nan_to_num(image_data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                            \n",
    "                            # Normalize to [0, 1] range\n",
    "                            if image_data.max() > image_data.min():\n",
    "                                image_data = (image_data - image_data.min()) / (image_data.max() - image_data.min())\n",
    "                            else:\n",
    "                                image_data = np.zeros_like(image_data)\n",
    "                            \n",
    "                            # Resize if needed to match reference image size\n",
    "                            if reference_image_data is not None and image_data.shape != reference_image_data.shape:\n",
    "                                from scipy.ndimage import zoom\n",
    "                                zoom_factors = (\n",
    "                                    reference_image_data.shape[0] / image_data.shape[0],\n",
    "                                    reference_image_data.shape[1] / image_data.shape[1]\n",
    "                                )\n",
    "                                image_data = zoom(image_data, zoom_factors, order=1)\n",
    "                                print(f\"   üìê Resized to match reference: {image_data.shape}\")\n",
    "                            \n",
    "                            # Save processed science image\n",
    "                            safe_obs_id = obs_id.replace('/', '_').replace(':', '_')\n",
    "                            sci_filename = f\"science_{mission}_{safe_obs_id}.fits\"\n",
    "                            sci_path = dirs['science'] / sci_filename\n",
    "                            \n",
    "                            hdu = fits.PrimaryHDU(image_data)\n",
    "                            hdu.header['OBS_ID'] = obs_id\n",
    "                            hdu.header['MISSION'] = mission\n",
    "                            hdu.header['RA'] = obs.get('ra', 0)\n",
    "                            hdu.header['DEC'] = obs.get('dec', 0)\n",
    "                            hdu.writeto(sci_path, overwrite=True)\n",
    "                            \n",
    "                            downloaded_images.append({\n",
    "                                'filename': sci_filename,\n",
    "                                'path': str(sci_path),\n",
    "                                'obs_id': obs_id,\n",
    "                                'mission': mission,\n",
    "                                'shape': image_data.shape,\n",
    "                                'metadata': metadata,\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"   üíæ Saved processed image: {sci_filename}\")\n",
    "                            \n",
    "                            # Store image data reference in metadata for later loading\n",
    "                            downloaded_images.append({\n",
    "                                'filename': sci_filename,\n",
    "                                'path': str(sci_path),\n",
    "                                'obs_id': obs_id,\n",
    "                                'mission': mission,\n",
    "                                'shape': image_data.shape,\n",
    "                                'metadata': metadata,\n",
    "                                'image_data': image_data,  # Store temporarily for primary selection\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"   ‚úÖ Ready for differencing\")\n",
    "                            \n",
    "                        except Exception as proc_err:\n",
    "                            print(f\"   ‚ùå Error processing FITS file: {proc_err}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Download file not found\")\n",
    "                        \n",
    "                except ImportError:\n",
    "                    print(f\"   ‚ö†Ô∏è  astroquery.mast not available, skipping download\")\n",
    "                    print(f\"   Install with: pip install astroquery\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error downloading {obs_id}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error processing observation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        return downloaded_images\n",
    "    \n",
    "    # Run the download\n",
    "    downloaded_science_images = asyncio.run(download_and_process_observations())\n",
    "    science_images = downloaded_science_images\n",
    "    \n",
    "    # Extract primary science image data from first downloaded image\n",
    "    if science_images and len(science_images) > 0:\n",
    "        first_image = science_images[0]\n",
    "        # Get image data if stored, otherwise load from file\n",
    "        if 'image_data' in first_image:\n",
    "            science_image_data = first_image['image_data']\n",
    "            # Remove from metadata to keep it clean\n",
    "            del first_image['image_data']\n",
    "        else:\n",
    "            # Load from file\n",
    "            first_image_path = first_image.get('path')\n",
    "            if first_image_path and os.path.exists(first_image_path):\n",
    "                try:\n",
    "                    image_data, _, _ = fits_processor.read_fits(first_image_path)\n",
    "                    science_image_data = image_data\n",
    "                    print(f\"   ‚úÖ Loaded primary science image from file\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Could not load primary image: {e}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully downloaded {len(science_images)} science images\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No science images downloaded, will create synthetic for testing\")\n",
    "        # Fall through to synthetic creation below\n",
    "\n",
    "# Fallback: Create synthetic science image if no real data available\n",
    "if science_image_data is None:\n",
    "    print(\"\\n‚ö†Ô∏è  No real science images available. Creating synthetic science image for testing...\")\n",
    "    print(\"   (This is okay for initial testing, but real data is needed for anomaly detection)\")\n",
    "    \n",
    "    if reference_image_data is not None:\n",
    "        # Create based on reference with some variation\n",
    "        science_image_data = reference_image_data.copy()\n",
    "        # Add realistic variation (simulating different observation conditions)\n",
    "        noise = np.random.normal(0, 0.02, science_image_data.shape)\n",
    "        science_image_data = science_image_data + noise\n",
    "        science_image_data = np.clip(science_image_data, 0, 1)\n",
    "        print(f\"   ‚úÖ Created synthetic science image (shape: {science_image_data.shape})\")\n",
    "    else:\n",
    "        # Create from scratch\n",
    "        science_image_data = np.random.normal(0.1, 0.05, (CONFIG['image_size_pixels'], CONFIG['image_size_pixels'])).astype(np.float32)\n",
    "        science_image_data = np.clip(science_image_data, 0, 1)\n",
    "        print(f\"   ‚úÖ Created synthetic science image (shape: {science_image_data.shape})\")\n",
    "    \n",
    "    # Save synthetic image\n",
    "    if CONFIG['save_intermediate']:\n",
    "        sci_filename = f\"science_synthetic_{target_region['name']}.fits\"\n",
    "        sci_path = dirs['science'] / sci_filename\n",
    "        hdu = fits.PrimaryHDU(science_image_data)\n",
    "        hdu.header['TYPE'] = 'SYNTHETIC'\n",
    "        hdu.writeto(sci_path, overwrite=True)\n",
    "        print(f\"   üíæ Saved synthetic image to: {sci_path}\")\n",
    "\n",
    "print(f\"\\nüìä Science image ready: {science_image_data.shape if science_image_data is not None else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31322af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved observation metadata to: /home/chris/github/AstrID/notebooks/data/exploratory/metadata/science_observations.json\n",
      "\n",
      "üìã Observation Summary:\n",
      "  1. JWST - MESSIER-101\n",
      "     RA: 210.876¬∞, Dec: 54.361¬∞\n",
      "     Filter: F115W\n",
      "  2. JWST - MESSIER-101\n",
      "     RA: 210.876¬∞, Dec: 54.361¬∞\n",
      "     Filter: F444W\n",
      "  3. JWST - MESSIER-101\n",
      "     RA: 210.763¬∞, Dec: 54.288¬∞\n",
      "     Filter: F200W\n",
      "  4. JWST - MESSIER-101\n",
      "     RA: 210.763¬∞, Dec: 54.288¬∞\n",
      "     Filter: F212N\n",
      "  5. JWST - MESSIER-101\n",
      "     RA: 210.763¬∞, Dec: 54.288¬∞\n",
      "     Filter: F300M\n"
     ]
    }
   ],
   "source": [
    "# Save observation metadata\n",
    "if science_observations:\n",
    "    obs_metadata_file = dirs['metadata'] / 'science_observations.json'\n",
    "    save_observation_metadata(science_observations, obs_metadata_file)\n",
    "    print(f\"‚úÖ Saved observation metadata to: {obs_metadata_file}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nüìã Observation Summary:\")\n",
    "    for i, obs in enumerate(science_observations[:5], 1):  # Show first 5\n",
    "        print(f\"  {i}. {obs.get('mission', 'Unknown')} - {obs.get('target_name', 'N/A')}\")\n",
    "        print(f\"     RA: {obs.get('ra', 'N/A'):.3f}¬∞, Dec: {obs.get('dec', 'N/A'):.3f}¬∞\")\n",
    "        print(f\"     Filter: {obs.get('filters', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No science observations found. You may need to:\")\n",
    "    print(\"   - Check internet connection\")\n",
    "    print(\"   - Verify MAST API access\")\n",
    "    print(\"   - Try different sky coordinates\")\n",
    "    print(\"   - Use synthetic data (see later cells)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77918a",
   "metadata": {},
   "source": [
    "## 3. Image Processing and Differencing\n",
    "\n",
    "Load FITS files, align images, and compute difference images using ZOGY algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12faf110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 13:24:03,871 - INFO - Successfully read FITS file: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits/reference/reference_ps1_M101_field_g.fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference image: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits/reference/reference_ps1_M101_field_g.fits\n",
      "‚úÖ Reference image loaded\n",
      "   Shape: (6302, 6283)\n",
      "   Data type: >f8\n",
      "   Min: 0.000, Max: 1.000\n",
      "   WCS: WCS Keywords\n",
      "\n",
      "Number of WCS axes: 2\n",
      "CTYPE : '' '' \n",
      "CRVAL : 0.0 0.0 \n",
      "CRPIX : 0.0 0.0 \n",
      "PC1_1 PC1_2  : 1.0 0.0 \n",
      "PC2_1 PC2_2  : 0.0 1.0 \n",
      "CDELT : 1.0 1.0 \n",
      "NAXIS : 6283  6302\n"
     ]
    }
   ],
   "source": [
    "# Load reference image\n",
    "reference_image_data = None\n",
    "reference_wcs = None\n",
    "reference_metadata = {}\n",
    "\n",
    "if reference_images:\n",
    "    ref_path = reference_images[0]['path']\n",
    "    print(f\"Loading reference image: {ref_path}\")\n",
    "    \n",
    "    try:\n",
    "        image_data, wcs_info, metadata = fits_processor.read_fits(ref_path)\n",
    "        reference_image_data = image_data\n",
    "        reference_wcs = wcs_info\n",
    "        reference_metadata = metadata\n",
    "        \n",
    "        print(f\"‚úÖ Reference image loaded\")\n",
    "        print(f\"   Shape: {reference_image_data.shape}\")\n",
    "        print(f\"   Data type: {reference_image_data.dtype}\")\n",
    "        print(f\"   Min: {np.nanmin(reference_image_data):.3f}, Max: {np.nanmax(reference_image_data):.3f}\")\n",
    "        \n",
    "        if reference_wcs is not None:\n",
    "            print(f\"   WCS: {reference_wcs}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading reference image: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No reference images available. Creating synthetic reference for testing...\")\n",
    "    \n",
    "    # Create a simple synthetic reference image for testing\n",
    "    reference_image_data = np.random.normal(0.1, 0.05, (CONFIG['image_size_pixels'], CONFIG['image_size_pixels'])).astype(np.float32)\n",
    "    reference_image_data = np.clip(reference_image_data, 0, 1)\n",
    "    print(f\"‚úÖ Created synthetic reference image (shape: {reference_image_data.shape})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• 10 science observations available\n",
      "   (In a full implementation, we would download and process these)\n",
      "   For now, creating a test science image...\n",
      "‚úÖ Created test science image (shape: (6302, 6283))\n",
      "üíæ Saved science image to: /home/chris/github/AstrID/notebooks/data/exploratory/source_fits/science/science_test_M101_field.fits\n"
     ]
    }
   ],
   "source": [
    "# For now, create a science image (either from fetched data or synthetic)\n",
    "# In a real scenario, you would download and process actual science images\n",
    "\n",
    "science_image_data = None\n",
    "\n",
    "if science_observations:\n",
    "    print(f\"üì• {len(science_observations)} science observations available\")\n",
    "    print(\"   (In a full implementation, we would download and process these)\")\n",
    "    print(\"   For now, creating a test science image...\")\n",
    "    \n",
    "    # Create a science image based on reference (simulating a new observation)\n",
    "    science_image_data = reference_image_data.copy() if reference_image_data is not None else None\n",
    "    \n",
    "    if science_image_data is not None:\n",
    "        # Add some variation (simulating different observation conditions)\n",
    "        noise = np.random.normal(0, 0.02, science_image_data.shape)\n",
    "        science_image_data = science_image_data + noise\n",
    "        science_image_data = np.clip(science_image_data, 0, 1)\n",
    "        print(f\"‚úÖ Created test science image (shape: {science_image_data.shape})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No science observations. Creating synthetic science image...\")\n",
    "    \n",
    "    if reference_image_data is not None:\n",
    "        science_image_data = reference_image_data.copy()\n",
    "        # Add some variation\n",
    "        noise = np.random.normal(0, 0.02, science_image_data.shape)\n",
    "        science_image_data = science_image_data + noise\n",
    "        science_image_data = np.clip(science_image_data, 0, 1)\n",
    "    else:\n",
    "        science_image_data = np.random.normal(0.1, 0.05, (CONFIG['image_size_pixels'], CONFIG['image_size_pixels'])).astype(np.float32)\n",
    "        science_image_data = np.clip(science_image_data, 0, 1)\n",
    "    \n",
    "    print(f\"‚úÖ Created synthetic science image (shape: {science_image_data.shape})\")\n",
    "\n",
    "# Save science image for reference\n",
    "if science_image_data is not None and CONFIG['save_intermediate']:\n",
    "    sci_filename = f\"science_test_{target_region['name']}.fits\"\n",
    "    sci_path = dirs['science'] / sci_filename\n",
    "    hdu = fits.PrimaryHDU(science_image_data)\n",
    "    hdu.writeto(sci_path, overwrite=True)\n",
    "    print(f\"üíæ Saved science image to: {sci_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e08cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image differencing processor initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize differencing processor\n",
    "differencing_processor = ImageDifferencingProcessor()\n",
    "\n",
    "print(\"‚úÖ Image differencing processor initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ZOGY difference image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "2025-11-04 14:29:54,244 - WARNING - nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this.\n",
      "WARNING: VerifyWarning: Keyword name 'REFERENCE' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n",
      "2025-11-04 14:29:55,502 - WARNING - VerifyWarning: Keyword name 'REFERENCE' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created.\n",
      "WARNING: VerifyWarning: Keyword name 'MTRC_MAX_SIGNIFICANCE' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n",
      "2025-11-04 14:29:55,517 - WARNING - VerifyWarning: Keyword name 'MTRC_MAX_SIGNIFICANCE' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ZOGY difference computed\n",
      "   Difference image shape: (6302, 6283)\n",
      "   Metrics:\n",
      "     max_significance: nan\n",
      "     mean_significance: nan\n",
      "     std_significance: nan\n",
      "     snr_improvement: nan\n",
      "‚ùå Error computing ZOGY difference: Floating point nan values are not allowed in FITS headers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_70304/3972455384.py\", line 41, in <module>\n",
      "    hdu.header[f'MTRC_{key.upper()}'] = value\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/github/AstrID/.venv/lib/python3.12/site-packages/astropy/io/fits/header.py\", line 222, in __setitem__\n",
      "    self._update((key, value, comment))\n",
      "  File \"/home/chris/github/AstrID/.venv/lib/python3.12/site-packages/astropy/io/fits/header.py\", line 1671, in _update\n",
      "    self.append(card)\n",
      "  File \"/home/chris/github/AstrID/.venv/lib/python3.12/site-packages/astropy/io/fits/header.py\", line 1128, in append\n",
      "    card = Card(*card)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/chris/github/AstrID/.venv/lib/python3.12/site-packages/astropy/io/fits/card.py\", line 201, in __init__\n",
      "    self.value = value\n",
      "    ^^^^^^^^^^\n",
      "  File \"/home/chris/github/AstrID/.venv/lib/python3.12/site-packages/astropy/io/fits/card.py\", line 348, in value\n",
      "    raise ValueError(\n",
      "ValueError: Floating point nan values are not allowed in FITS headers.\n"
     ]
    }
   ],
   "source": [
    "# Compute ZOGY difference image\n",
    "if science_image_data is not None and reference_image_data is not None:\n",
    "    print(\"Computing ZOGY difference image...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure images are the same shape\n",
    "        if science_image_data.shape != reference_image_data.shape:\n",
    "            print(f\"‚ö†Ô∏è  Resizing images to match...\")\n",
    "            from scipy.ndimage import zoom\n",
    "            target_shape = reference_image_data.shape\n",
    "            zoom_factors = (target_shape[0] / science_image_data.shape[0], \n",
    "                          target_shape[1] / science_image_data.shape[1])\n",
    "            science_image_data = zoom(science_image_data, zoom_factors, order=1)\n",
    "            print(f\"   Science image resized to: {science_image_data.shape}\")\n",
    "        \n",
    "        # Compute ZOGY difference\n",
    "        diff_image, diff_metrics = differencing_processor.zogy_differencing(\n",
    "            science_image=science_image_data,\n",
    "            reference_image=reference_image_data,\n",
    "            psf_science=None,  # Will be auto-generated\n",
    "            psf_reference=None,  # Will be auto-generated\n",
    "            noise_science=None,  # Will be auto-estimated\n",
    "            noise_reference=None,  # Will be auto-estimated\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ ZOGY difference computed\")\n",
    "        print(f\"   Difference image shape: {diff_image.shape}\")\n",
    "        print(f\"   Metrics:\")\n",
    "        for key, value in diff_metrics.items():\n",
    "            print(f\"     {key}: {value:.4f}\")\n",
    "        \n",
    "        # Save difference image\n",
    "        if CONFIG['save_intermediate']:\n",
    "            diff_filename = f\"diff_zogy_{target_region['name']}.fits\"\n",
    "            diff_path = dirs['zogy'] / diff_filename\n",
    "            hdu = fits.PrimaryHDU(diff_image)\n",
    "            hdu.header['METHOD'] = 'ZOGY'\n",
    "            hdu.header['SCIENCE'] = 'test_science'\n",
    "            hdu.header['REFERENCE'] = 'ps1_reference'\n",
    "            for key, value in diff_metrics.items():\n",
    "                hdu.header[f'MTRC_{key.upper()}'] = value\n",
    "            hdu.writeto(diff_path, overwrite=True)\n",
    "            print(f\"üíæ Saved ZOGY difference to: {diff_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error computing ZOGY difference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        diff_image = None\n",
    "        diff_metrics = {}\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot compute difference: missing science or reference image\")\n",
    "    diff_image = None\n",
    "    diff_metrics = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classic difference for comparison\n",
    "if science_image_data is not None and reference_image_data is not None:\n",
    "    print(\"Computing classic difference image...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure images are the same shape\n",
    "        if science_image_data.shape != reference_image_data.shape:\n",
    "            from scipy.ndimage import zoom\n",
    "            target_shape = reference_image_data.shape\n",
    "            zoom_factors = (target_shape[0] / science_image_data.shape[0], \n",
    "                          target_shape[1] / science_image_data.shape[1])\n",
    "            science_for_classic = zoom(science_image_data, zoom_factors, order=1)\n",
    "        else:\n",
    "            science_for_classic = science_image_data\n",
    "        \n",
    "        classic_diff, classic_metrics = differencing_processor.classic_differencing(\n",
    "            science_image=science_for_classic,\n",
    "            reference_image=reference_image_data\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Classic difference computed\")\n",
    "        print(f\"   Metrics:\")\n",
    "        for key, value in classic_metrics.items():\n",
    "            print(f\"     {key}: {value:.4f}\")\n",
    "        \n",
    "        # Save classic difference\n",
    "        if CONFIG['save_intermediate']:\n",
    "            classic_filename = f\"diff_classic_{target_region['name']}.fits\"\n",
    "            classic_path = dirs['classic'] / classic_filename\n",
    "            hdu = fits.PrimaryHDU(classic_diff)\n",
    "            hdu.header['METHOD'] = 'CLASSIC'\n",
    "            for key, value in classic_metrics.items():\n",
    "                hdu.header[f'MTRC_{key.upper()}'] = value\n",
    "            hdu.writeto(classic_path, overwrite=True)\n",
    "            print(f\"üíæ Saved classic difference to: {classic_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error computing classic difference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        classic_diff = None\n",
    "        classic_metrics = {}\n",
    "else:\n",
    "    classic_diff = None\n",
    "    classic_metrics = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e3e36",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Visualize the images and difference results to understand the data quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Reference image\n",
    "if reference_image_data is not None:\n",
    "    im1 = axes[0, 0].imshow(reference_image_data, origin='lower', cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, 0].set_title('Reference Image')\n",
    "    axes[0, 0].set_xlabel('X (pixels)')\n",
    "    axes[0, 0].set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No reference image', ha='center', va='center')\n",
    "    axes[0, 0].set_title('Reference Image')\n",
    "\n",
    "# Science image\n",
    "if science_image_data is not None:\n",
    "    im2 = axes[0, 1].imshow(science_image_data, origin='lower', cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, 1].set_title('Science Image')\n",
    "    axes[0, 1].set_xlabel('X (pixels)')\n",
    "    axes[0, 1].set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No science image', ha='center', va='center')\n",
    "    axes[0, 1].set_title('Science Image')\n",
    "\n",
    "# ZOGY difference\n",
    "if diff_image is not None:\n",
    "    vmax = np.percentile(np.abs(diff_image), 99)\n",
    "    vmin = -vmax\n",
    "    im3 = axes[1, 0].imshow(diff_image, origin='lower', cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "    axes[1, 0].set_title(f'ZOGY Difference (max sig: {diff_metrics.get(\"max_significance\", 0):.2f})')\n",
    "    axes[1, 0].set_xlabel('X (pixels)')\n",
    "    axes[1, 0].set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No ZOGY difference', ha='center', va='center')\n",
    "    axes[1, 0].set_title('ZOGY Difference')\n",
    "\n",
    "# Classic difference\n",
    "if classic_diff is not None:\n",
    "    vmax = np.percentile(np.abs(classic_diff), 99)\n",
    "    vmin = -vmax\n",
    "    im4 = axes[1, 1].imshow(classic_diff, origin='lower', cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "    axes[1, 1].set_title(f'Classic Difference (max: {classic_metrics.get(\"max_diff\", 0):.2f})')\n",
    "    axes[1, 1].set_xlabel('X (pixels)')\n",
    "    axes[1, 1].set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No classic difference', ha='center', va='center')\n",
    "    axes[1, 1].set_title('Classic Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15300864",
   "metadata": {},
   "source": [
    "## 5. Source Extraction and Anomaly Preparation\n",
    "\n",
    "Extract sources from difference images and prepare data for anomaly detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source extraction using SEP (Source Extractor in Python)\n",
    "# If SEP is not available, we'll use photutils as fallback\n",
    "\n",
    "try:\n",
    "    import sep\n",
    "    USE_SEP = True\n",
    "    print(\"‚úÖ SEP available for source extraction\")\n",
    "except ImportError:\n",
    "    USE_SEP = False\n",
    "    try:\n",
    "        from photutils.detection import DAOStarFinder, find_peaks\n",
    "        from photutils.segmentation import detect_sources\n",
    "        USE_PHOTUTILS = True\n",
    "        print(\"‚úÖ Using photutils for source extraction (SEP not available)\")\n",
    "    except ImportError:\n",
    "        USE_PHOTUTILS = False\n",
    "        print(\"‚ö†Ô∏è  Neither SEP nor photutils available. Source extraction will be limited.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f00ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sources from difference image\n",
    "candidates = []\n",
    "\n",
    "if diff_image is not None:\n",
    "    print(f\"Extracting sources from difference image...\")\n",
    "    print(f\"   SNR threshold: {CONFIG['source_detection_snr_threshold']}œÉ\")\n",
    "    \n",
    "    try:\n",
    "        if USE_SEP:\n",
    "            # Background estimation\n",
    "            bkg = sep.Background(diff_image.astype(np.float64))\n",
    "            \n",
    "            # Subtract background\n",
    "            data_sub = diff_image.astype(np.float64) - bkg\n",
    "            \n",
    "            # Extract sources\n",
    "            objects = sep.extract(\n",
    "                data_sub,\n",
    "                thresh=CONFIG['source_detection_snr_threshold'],\n",
    "                err=bkg.globalrms,\n",
    "                minarea=3\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(objects)} sources using SEP\")\n",
    "            \n",
    "            # Store candidates\n",
    "            for obj in objects:\n",
    "                candidates.append({\n",
    "                    'x': obj['x'],\n",
    "                    'y': obj['y'],\n",
    "                    'flux': obj['flux'],\n",
    "                    'fluxerr': obj['fluxerr'],\n",
    "                    'snr': obj['flux'] / obj['fluxerr'] if obj['fluxerr'] > 0 else 0,\n",
    "                    'a': obj['a'],  # Semi-major axis\n",
    "                    'b': obj['b'],  # Semi-minor axis\n",
    "                    'theta': obj['theta'],  # Position angle\n",
    "                    'peak': obj['peak'],\n",
    "                })\n",
    "                \n",
    "        elif USE_PHOTUTILS:\n",
    "            # Estimate background\n",
    "            from astropy.stats import sigma_clipped_stats\n",
    "            mean, median, std = sigma_clipped_stats(diff_image, sigma=3.0)\n",
    "            \n",
    "            # Find sources\n",
    "            threshold = median + (CONFIG['source_detection_snr_threshold'] * std)\n",
    "            segm = detect_sources(diff_image, threshold, npixels=3)\n",
    "            \n",
    "            print(f\"‚úÖ Found {segm.nlabels} sources using photutils\")\n",
    "            \n",
    "            # Extract properties\n",
    "            from photutils.segmentation import SourceProperties\n",
    "            props = SourceProperties(data=diff_image, segment_img=segm)\n",
    "            \n",
    "            for prop in props:\n",
    "                candidates.append({\n",
    "                    'x': prop.xcentroid.value,\n",
    "                    'y': prop.ycentroid.value,\n",
    "                    'flux': prop.source_sum.value,\n",
    "                    'fluxerr': None,\n",
    "                    'snr': prop.source_sum.value / std if std > 0 else 0,\n",
    "                    'a': prop.semimajor_axis_sigma.value if hasattr(prop, 'semimajor_axis_sigma') else None,\n",
    "                    'b': prop.semiminor_axis_sigma.value if hasattr(prop, 'semiminor_axis_sigma') else None,\n",
    "                    'theta': prop.orientation.value if hasattr(prop, 'orientation') else None,\n",
    "                    'peak': prop.max_value if hasattr(prop, 'max_value') else None,\n",
    "                })\n",
    "        else:\n",
    "            # Simple peak finding fallback\n",
    "            from scipy.ndimage import maximum_filter\n",
    "            from scipy.ndimage import binary_erosion\n",
    "            import scipy.ndimage as ndimage\n",
    "            \n",
    "            # Find local maxima\n",
    "            local_maxima = maximum_filter(diff_image, size=5) == diff_image\n",
    "            threshold = np.nanstd(diff_image) * CONFIG['source_detection_snr_threshold']\n",
    "            peaks = local_maxima & (diff_image > threshold)\n",
    "            \n",
    "            # Get peak positions\n",
    "            y_coords, x_coords = np.where(peaks)\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(x_coords)} peaks using simple method\")\n",
    "            \n",
    "            for x, y in zip(x_coords, y_coords):\n",
    "                candidates.append({\n",
    "                    'x': float(x),\n",
    "                    'y': float(y),\n",
    "                    'flux': float(diff_image[y, x]),\n",
    "                    'fluxerr': None,\n",
    "                    'snr': float(diff_image[y, x] / np.nanstd(diff_image)),\n",
    "                    'a': None,\n",
    "                    'b': None,\n",
    "                    'theta': None,\n",
    "                    'peak': float(diff_image[y, x]),\n",
    "                })\n",
    "        \n",
    "        print(f\"   Total candidates: {len(candidates)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting sources: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No difference image available for source extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected sources\n",
    "if diff_image is not None and candidates:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    \n",
    "    vmax = np.percentile(np.abs(diff_image), 99)\n",
    "    vmin = -vmax\n",
    "    im = ax.imshow(diff_image, origin='lower', cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # Plot candidate positions\n",
    "    x_coords = [c['x'] for c in candidates]\n",
    "    y_coords = [c['y'] for c in candidates]\n",
    "    snrs = [c['snr'] for c in candidates]\n",
    "    \n",
    "    scatter = ax.scatter(x_coords, y_coords, c=snrs, s=100, alpha=0.7, \n",
    "                        cmap='viridis', edgecolors='yellow', linewidths=1.5)\n",
    "    \n",
    "    ax.set_title(f'Difference Image with {len(candidates)} Detected Sources')\n",
    "    ax.set_xlabel('X (pixels)')\n",
    "    ax.set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im, ax=ax, label='Difference flux')\n",
    "    plt.colorbar(scatter, ax=ax, label='SNR')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print candidate summary\n",
    "    print(f\"\\nüìä Candidate Summary:\")\n",
    "    print(f\"   Total candidates: {len(candidates)}\")\n",
    "    if candidates:\n",
    "        snrs_list = [c['snr'] for c in candidates if c['snr'] is not None]\n",
    "        if snrs_list:\n",
    "            print(f\"   SNR range: {min(snrs_list):.2f} - {max(snrs_list):.2f}\")\n",
    "            print(f\"   Mean SNR: {np.mean(snrs_list):.2f}\")\n",
    "        \n",
    "        print(f\"\\n   Top 5 candidates by SNR:\")\n",
    "        sorted_candidates = sorted(candidates, key=lambda x: x.get('snr', 0), reverse=True)\n",
    "        for i, cand in enumerate(sorted_candidates[:5], 1):\n",
    "            print(f\"     {i}. Position: ({cand['x']:.1f}, {cand['y']:.1f}), SNR: {cand.get('snr', 'N/A'):.2f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No candidates to visualize\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe432dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save candidate catalog\n",
    "if candidates:\n",
    "    catalog_file = dirs['metadata'] / 'candidate_sources.json'\n",
    "    save_observation_metadata(candidates, catalog_file)\n",
    "    print(f\"‚úÖ Saved candidate catalog to: {catalog_file}\")\n",
    "    \n",
    "    # Also save as CSV for easy viewing\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(candidates)\n",
    "    csv_file = dirs['metadata'] / 'candidate_sources.csv'\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"‚úÖ Saved candidate catalog (CSV) to: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa6d27",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "Summary of what we've accomplished and preparation for the next notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'data_acquisition': {\n",
    "        'reference_images': len(reference_images),\n",
    "        'science_observations': len(science_observations),\n",
    "    },\n",
    "    'processing': {\n",
    "        'reference_loaded': reference_image_data is not None,\n",
    "        'science_loaded': science_image_data is not None,\n",
    "        'zogy_computed': diff_image is not None,\n",
    "        'classic_computed': classic_diff is not None,\n",
    "    },\n",
    "    'source_extraction': {\n",
    "        'candidates_found': len(candidates),\n",
    "        'snr_threshold': CONFIG['source_detection_snr_threshold'],\n",
    "    },\n",
    "    'files_created': {\n",
    "        'reference': [img['filename'] for img in reference_images],\n",
    "        'differences': {\n",
    "            'zogy': 'diff_zogy_*.fits' if diff_image is not None else None,\n",
    "            'classic': 'diff_classic_*.fits' if classic_diff is not None else None,\n",
    "        },\n",
    "        'catalogs': ['candidate_sources.json', 'candidate_sources.csv'] if candidates else [],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_file = dirs['metadata'] / 'processing_summary.json'\n",
    "save_observation_metadata(summary, summary_file)\n",
    "print(f\"‚úÖ Saved processing summary to: {summary_file}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã EXPLORATORY PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüîç Data Acquisition:\")\n",
    "print(f\"   Reference images: {summary['data_acquisition']['reference_images']}\")\n",
    "print(f\"   Science observations: {summary['data_acquisition']['science_observations']}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Processing:\")\n",
    "print(f\"   Reference image loaded: {'‚úÖ' if summary['processing']['reference_loaded'] else '‚ùå'}\")\n",
    "print(f\"   Science image loaded: {'‚úÖ' if summary['processing']['science_loaded'] else '‚ùå'}\")\n",
    "print(f\"   ZOGY difference computed: {'‚úÖ' if summary['processing']['zogy_computed'] else '‚ùå'}\")\n",
    "print(f\"   Classic difference computed: {'‚úÖ' if summary['processing']['classic_computed'] else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\nüåå Source Extraction:\")\n",
    "print(f\"   Candidates found: {summary['source_extraction']['candidates_found']}\")\n",
    "print(f\"   SNR threshold: {summary['source_extraction']['snr_threshold']}œÉ\")\n",
    "\n",
    "print(f\"\\nüìÅ Data Directory: {data_dir}\")\n",
    "print(f\"   All outputs saved to: {data_dir}\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8616889",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### For Production Use:\n",
    "1. **Download actual science images** from MAST observations\n",
    "2. **Implement proper image alignment** using WCS transformations\n",
    "3. **Add background subtraction and normalization** before differencing\n",
    "4. **Implement PSF estimation** for better ZOGY results\n",
    "5. **Add quality assessment** for difference images\n",
    "\n",
    "### For Anomaly Detection:\n",
    "1. **Extract image cutouts** around candidate sources\n",
    "2. **Prepare training dataset** with labels (real vs bogus)\n",
    "3. **Explore ZTF data sources** or synthetic anomaly generation\n",
    "4. **Create HDF5/PyTorch dataset** for next notebook\n",
    "\n",
    "### Data Sources to Explore:\n",
    "- **ZTF Public Data**: IRSA portal, Kowalski database\n",
    "- **Synthetic Anomalies**: Use `standalone_training.py` SyntheticAstronomicalDataset\n",
    "- **Hybrid Approach**: Combine synthetic training data with real ZTF validation data\n",
    "\n",
    "See `EXPLORATORY_NOTEBOOK_PLAN.md` for detailed next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121294ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
