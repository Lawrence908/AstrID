{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supernova Dataset Generation Example\n",
    "\n",
    "This notebook demonstrates how to use the YAML-based pipeline configuration system to generate mission-specific supernova training datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pipeline consists of 5 stages:\n",
    "1. **Query**: Search MAST archive for observations\n",
    "2. **Filter**: Identify same-mission pairs\n",
    "3. **Download**: Fetch FITS files with smart filtering\n",
    "4. **Organize**: Structure files for training\n",
    "5. **Differencing**: Generate difference images\n",
    "\n",
    "All stages are configured via YAML files for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /mnt/astrid/AstrID\n",
      "Python version: 3.12.3 (main, Jan  8 2026, 11:30:50) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.pipeline.config import PipelineConfig\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Configuration\n",
    "\n",
    "Let's load a configuration file and inspect its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: swift_uv_supernovae\n",
      "Description: SWIFT UVOT UV-band supernova training set with matched filter pairs\n",
      "\n",
      "Missions: ['SWIFT']\n",
      "Filters: ['uuu', 'uvw1', 'uvm2', 'uvw2']\n",
      "Year range: 2005-2020\n",
      "\n",
      "Temporal windows:\n",
      "  Reference: 1095 days before discovery\n",
      "  Science: 730 days after discovery\n"
     ]
    }
   ],
   "source": [
    "# Load SWIFT UV configuration\n",
    "config_path = project_root / \"configs\" / \"swift_uv_dataset.yaml\"\n",
    "config = PipelineConfig.from_yaml(config_path)\n",
    "\n",
    "print(f\"Dataset: {config.dataset_name}\")\n",
    "print(f\"Description: {config.description}\")\n",
    "print(f\"\\nMissions: {config.query.missions}\")\n",
    "print(f\"Filters: {config.query.filters}\")\n",
    "print(f\"Year range: {config.query.min_year}-{config.query.max_year}\")\n",
    "print(f\"\\nTemporal windows:\")\n",
    "print(f\"  Reference: {config.query.days_before} days before discovery\")\n",
    "print(f\"  Science: {config.query.days_after} days after discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validate Configuration\n",
    "\n",
    "Check for any configuration warnings or issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration is valid!\n"
     ]
    }
   ],
   "source": [
    "warnings = config.validate()\n",
    "\n",
    "if warnings:\n",
    "    print(\"⚠️  Configuration warnings:\")\n",
    "    for warning in warnings:\n",
    "        print(f\"  - {warning}\")\n",
    "else:\n",
    "    print(\"✅ Configuration is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customize Configuration (Optional)\n",
    "\n",
    "You can modify configuration parameters programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test configuration: swift_uv_test\n",
      "Will process: 10 SNe\n",
      "Output directory: /mnt/astrid/AstrID/output/datasets/swift_uv_test\n"
     ]
    }
   ],
   "source": [
    "# Example: Create a test configuration with limited scope\n",
    "test_config = PipelineConfig.from_yaml(config_path)\n",
    "\n",
    "# Limit to 10 SNe for testing\n",
    "test_config.query.limit = 10\n",
    "test_config.dataset_name = \"swift_uv_test\"\n",
    "\n",
    "# Update output paths\n",
    "base_dir = project_root / \"output\" / \"datasets\" / test_config.dataset_name\n",
    "test_config.output.query_results = base_dir / \"queries.json\"\n",
    "test_config.output.fits_downloads = base_dir / \"fits_downloads\"\n",
    "test_config.output.fits_training = base_dir / \"fits_training\"\n",
    "test_config.output.difference_images = base_dir / \"difference_images\"\n",
    "\n",
    "print(f\"Test configuration: {test_config.dataset_name}\")\n",
    "print(f\"Will process: {test_config.query.limit} SNe\")\n",
    "print(f\"Output directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Custom Configuration\n",
    "\n",
    "Save the customized configuration to a new YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved custom configuration to: /mnt/astrid/AstrID/configs/swift_uv_test.yaml\n",
      "\n",
      "Configuration content:\n",
      "dataset_name: swift_uv_test\n",
      "description: SWIFT UVOT UV-band supernova training set with matched filter pairs\n",
      "query:\n",
      "  missions:\n",
      "  - SWIFT\n",
      "  filters:\n",
      "  - uuu\n",
      "  - uvw1\n",
      "  - uvm2\n",
      "  - uvw2\n",
      "  archives: null\n",
      "  min_year: 2005\n",
      "  max_year: 2020\n",
      "  days_before: 1095\n",
      "  days_after: 730\n",
      "  radius_deg: 0.1\n",
      "  chunk_size: 250\n",
      "  start_index: 0\n",
      "  limit: 10\n",
      "download:\n",
      "  max_obs_per_type: 5\n",
      "  max_products_per_obs: 3\n",
      "  include_auxiliary: false\n",
      "  require_same_mission: true\n",
      "  verify_fits: true\n",
      "  skip_reference: false\n",
      "  skip_science: false\n",
      "quality:\n",
      "  min_overlap_fraction: 0.85\n",
      "  max_file_size_mb: 500\n",
      "  verify_wcs: true\n",
      "output:\n",
      "  query_results: /mnt/astrid/AstrID/output/datasets/swift_uv_test/queries.json\n",
      "  fits_downloads: /mnt/astrid/AstrID/output/datasets/swift_uv_test/fits_downloads\n",
      "  fits_training: /mnt/astrid/AstrID/output/datasets/swift_uv_test/fits_training\n",
      "  difference_images: /mnt/astrid/AstrID/output/datasets/swift_uv_test/difference_images\n",
      "  checkpoint: output/datasets/swift_uv/checkpoint.json\n",
      "  chunk_dir: output/datasets/swift_uv/chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert config to dictionary\n",
    "config_dict = test_config.to_dict()\n",
    "\n",
    "# Save to YAML\n",
    "custom_config_path = project_root / \"configs\" / \"swift_uv_test.yaml\"\n",
    "with open(custom_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✅ Saved custom configuration to: {custom_config_path}\")\n",
    "\n",
    "# Display the YAML content\n",
    "with open(custom_config_path) as f:\n",
    "    print(\"\\nConfiguration content:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Pipeline (Dry Run)\n",
    "\n",
    "Preview what the pipeline would execute without actually running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 05:43:55,826 - src.pipeline.config - INFO - Loading configuration from /mnt/astrid/AstrID/configs/swift_uv_test.yaml\n",
      "2026-01-16 05:43:55,832 - __main__ - INFO - Loaded configuration: swift_uv_test\n",
      "2026-01-16 05:43:55,833 - __main__ - INFO - Description: SWIFT UVOT UV-band supernova training set with matched filter pairs\n",
      "2026-01-16 05:43:55,833 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-16 05:43:55,834 - __main__ - INFO - Stage: QUERY\n",
      "2026-01-16 05:43:55,834 - __main__ - INFO - ============================================================\n",
      "2026-01-16 05:43:55,834 - __main__ - INFO - Command: /home/chris/AstrID/.venv/bin/python /mnt/astrid/AstrID/scripts/query_sn_fits_chunked.py --catalog /mnt/astrid/AstrID/resources/sncat_compiled.txt --output /mnt/astrid/AstrID/output/datasets/swift_uv_test/queries.json --chunk-size 250 --checkpoint output/datasets/swift_uv/checkpoint.json --chunk-dir output/datasets/swift_uv/chunks --days-before 1095 --days-after 730 --radius 0.1 --missions SWIFT --min-year 2005 --max-year 2020 --limit 10 --reset-checkpoint\n",
      "2026-01-16 05:43:55,834 - __main__ - INFO - [DRY RUN] Would execute query stage\n",
      "2026-01-16 05:43:55,835 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-16 05:43:55,835 - __main__ - INFO - Stage: FILTER\n",
      "2026-01-16 05:43:55,835 - __main__ - INFO - ============================================================\n",
      "2026-01-16 05:43:55,835 - __main__ - INFO - Command: /home/chris/AstrID/.venv/bin/python /mnt/astrid/AstrID/scripts/identify_same_mission_pairs.py --input /mnt/astrid/AstrID/output/datasets/swift_uv_test/queries.json --output /mnt/astrid/AstrID/output/datasets/swift_uv_test/same_mission_pairs.json\n",
      "2026-01-16 05:43:55,836 - __main__ - INFO - [DRY RUN] Would execute filter stage\n",
      "2026-01-16 05:43:55,836 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-16 05:43:55,836 - __main__ - INFO - Stage: DOWNLOAD\n",
      "2026-01-16 05:43:55,836 - __main__ - INFO - ============================================================\n",
      "2026-01-16 05:43:55,836 - __main__ - INFO - Command: /home/chris/AstrID/.venv/bin/python /mnt/astrid/AstrID/scripts/download_sn_fits.py --query-results /mnt/astrid/AstrID/output/datasets/swift_uv_test/queries.json --output-dir /mnt/astrid/AstrID/output/datasets/swift_uv_test/fits_downloads --max-obs 5 --max-products-per-obs 3 --filter-has-both --dry-run\n",
      "2026-01-16 05:43:55,837 - __main__ - INFO - [DRY RUN] Would execute download stage\n",
      "2026-01-16 05:43:55,837 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-16 05:43:55,837 - __main__ - INFO - Stage: ORGANIZE\n",
      "2026-01-16 05:43:55,837 - __main__ - INFO - ============================================================\n",
      "2026-01-16 05:43:55,837 - __main__ - INFO - Command: /home/chris/AstrID/.venv/bin/python /mnt/astrid/AstrID/scripts/organize_training_pairs.py --input-dir /mnt/astrid/AstrID/output/datasets/swift_uv_test/fits_downloads --output-dir /mnt/astrid/AstrID/output/datasets/swift_uv_test/fits_training --clean\n",
      "2026-01-16 05:43:55,838 - __main__ - INFO - [DRY RUN] Would execute organize stage\n",
      "2026-01-16 05:43:55,838 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-16 05:43:55,838 - __main__ - INFO - Stage: DIFFERENCING\n",
      "2026-01-16 05:43:55,838 - __main__ - INFO - ============================================================\n",
      "2026-01-16 05:43:55,838 - __main__ - INFO - Command: /home/chris/AstrID/.venv/bin/python /mnt/astrid/AstrID/scripts/generate_difference_images.py --input-dir /mnt/astrid/AstrID/output/datasets/swift_uv_test/fits_training --output-dir /mnt/astrid/AstrID/output/datasets/swift_uv_test/difference_images\n",
      "2026-01-16 05:43:55,839 - __main__ - INFO - [DRY RUN] Would execute differencing stage\n",
      "2026-01-16 05:43:55,839 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-16 05:43:55,839 - __main__ - INFO - ✅ Pipeline completed successfully!\n",
      "2026-01-16 05:43:55,839 - __main__ - INFO - ============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run dry-run to see commands\n",
    "!python {project_root}/scripts/run_pipeline_from_config.py \\\n",
    "    --config {custom_config_path} \\\n",
    "    --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Individual Stages\n",
    "\n",
    "You can run stages individually for testing or debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Query MAST Archive\n",
    "\n",
    "Search for observations matching the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query stage only\n",
    "# Uncomment to execute:\n",
    "# !python {project_root}/scripts/run_pipeline_from_config.py \\\n",
    "#     --config {custom_config_path} \\\n",
    "#     --stage query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Filter Same-Mission Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run filter stage only\n",
    "# Uncomment to execute:\n",
    "# !python {project_root}/scripts/run_pipeline_from_config.py \\\n",
    "#     --config {custom_config_path} \\\n",
    "#     --stage filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect Results\n",
    "\n",
    "After running stages, inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results not found. Run the query stage first.\n"
     ]
    }
   ],
   "source": [
    "# Load query results if they exist\n",
    "query_results_path = test_config.output.query_results\n",
    "\n",
    "if query_results_path.exists():\n",
    "    with open(query_results_path) as f:\n",
    "        query_data = json.load(f)\n",
    "    \n",
    "    print(f\"Total SNe queried: {len(query_data)}\")\n",
    "    \n",
    "    # Count viable pairs\n",
    "    viable = sum(\n",
    "        1 for entry in query_data \n",
    "        if entry.get('reference_observations') and entry.get('science_observations')\n",
    "    )\n",
    "    print(f\"Viable pairs (both ref & sci): {viable}\")\n",
    "    \n",
    "    # Show first entry\n",
    "    if query_data:\n",
    "        print(\"\\nFirst entry:\")\n",
    "        print(json.dumps(query_data[0], indent=2, default=str)[:500] + \"...\")\n",
    "else:\n",
    "    print(\"Query results not found. Run the query stage first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Pipeline Progress\n",
    "\n",
    "Check which stages have completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Stage Status:\n",
      "==================================================\n",
      "Query           ⏳ Pending\n",
      "Download        ⏳ Pending\n",
      "Organize        ⏳ Pending\n",
      "Differencing    ⏳ Pending\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "stages = [\n",
    "    (\"Query\", test_config.output.query_results),\n",
    "    (\"Download\", test_config.output.fits_downloads / \"download_results.json\"),\n",
    "    (\"Organize\", test_config.output.fits_training / \"training_manifest.json\"),\n",
    "    (\"Differencing\", test_config.output.difference_images / \"processing_summary.json\"),\n",
    "]\n",
    "\n",
    "print(\"Pipeline Stage Status:\")\n",
    "print(\"=\" * 50)\n",
    "for stage_name, path in stages:\n",
    "    status = \"✅ Complete\" if path.exists() else \"⏳ Pending\"\n",
    "    print(f\"{stage_name:15} {status}\")\n",
    "    if path.exists():\n",
    "        size = os.path.getsize(path) / 1024  # KB\n",
    "        print(f\"{'':15} File size: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Configurations\n",
    "\n",
    "Compare parameters across different mission configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Missions</th>\n",
       "      <th>Filters</th>\n",
       "      <th>Year Range</th>\n",
       "      <th>Days Before</th>\n",
       "      <th>Days After</th>\n",
       "      <th>Max Obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>swift_uv_supernovae</td>\n",
       "      <td>SWIFT</td>\n",
       "      <td>uuu, uvw1, uvm2, uvw2</td>\n",
       "      <td>2005-2020</td>\n",
       "      <td>1095</td>\n",
       "      <td>730</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ps1_optical_supernovae</td>\n",
       "      <td>PS1</td>\n",
       "      <td>g, r, i, z, y</td>\n",
       "      <td>2010-2020</td>\n",
       "      <td>1095</td>\n",
       "      <td>730</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>galex_uv_supernovae</td>\n",
       "      <td>GALEX</td>\n",
       "      <td>fuv, nuv</td>\n",
       "      <td>2005-2020</td>\n",
       "      <td>1095</td>\n",
       "      <td>730</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Dataset Missions                Filters Year Range  \\\n",
       "0     swift_uv_supernovae    SWIFT  uuu, uvw1, uvm2, uvw2  2005-2020   \n",
       "1  ps1_optical_supernovae      PS1          g, r, i, z, y  2010-2020   \n",
       "2     galex_uv_supernovae    GALEX               fuv, nuv  2005-2020   \n",
       "\n",
       "   Days Before  Days After  Max Obs  \n",
       "0         1095         730        5  \n",
       "1         1095         730        5  \n",
       "2         1095         730        5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all configurations\n",
    "configs_dir = project_root / \"configs\"\n",
    "config_files = [\n",
    "    \"swift_uv_dataset.yaml\",\n",
    "    \"ps1_optical_dataset.yaml\",\n",
    "    \"galex_uv_dataset.yaml\",\n",
    "]\n",
    "\n",
    "comparison_data = []\n",
    "for config_file in config_files:\n",
    "    cfg = PipelineConfig.from_yaml(configs_dir / config_file)\n",
    "    comparison_data.append({\n",
    "        \"Dataset\": cfg.dataset_name,\n",
    "        \"Missions\": \", \".join(cfg.query.missions),\n",
    "        \"Filters\": \", \".join(cfg.query.filters) if cfg.query.filters else \"All\",\n",
    "        \"Year Range\": f\"{cfg.query.min_year}-{cfg.query.max_year or 'present'}\",\n",
    "        \"Days Before\": cfg.query.days_before,\n",
    "        \"Days After\": cfg.query.days_after,\n",
    "        \"Max Obs\": cfg.download.max_obs_per_type,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Complete Pipeline\n",
    "\n",
    "Execute all stages in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline\n",
    "# WARNING: This will take several hours for a full dataset\n",
    "# Uncomment to execute:\n",
    "\n",
    "# !python {project_root}/scripts/run_pipeline_from_config.py \\\n",
    "#     --config {custom_config_path} \\\n",
    "#     --visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyze Results\n",
    "\n",
    "After pipeline completion, analyze the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summary not found. Run the complete pipeline first.\n"
     ]
    }
   ],
   "source": [
    "# Load processing summary\n",
    "summary_path = test_config.output.difference_images / \"processing_summary.json\"\n",
    "\n",
    "if summary_path.exists():\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print(f\"Pipeline version: {summary.get('pipeline_version')}\")\n",
    "    print(f\"SNe processed: {summary.get('n_processed')}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    results = summary.get('results', [])\n",
    "    if results:\n",
    "        missions = {}\n",
    "        filters = {}\n",
    "        overlaps = []\n",
    "        \n",
    "        for r in results:\n",
    "            mission = r.get('mission_name', 'Unknown')\n",
    "            missions[mission] = missions.get(mission, 0) + 1\n",
    "            \n",
    "            filt = r.get('filter_name', 'Unknown')\n",
    "            filters[filt] = filters.get(filt, 0) + 1\n",
    "            \n",
    "            overlaps.append(r.get('overlap_fraction', 0))\n",
    "        \n",
    "        print(\"\\nMission breakdown:\")\n",
    "        for mission, count in sorted(missions.items()):\n",
    "            print(f\"  {mission}: {count} pairs\")\n",
    "        \n",
    "        print(\"\\nFilter breakdown:\")\n",
    "        for filt, count in sorted(filters.items()):\n",
    "            print(f\"  {filt}: {count} pairs\")\n",
    "        \n",
    "        print(f\"\\nOverlap statistics:\")\n",
    "        print(f\"  Mean: {sum(overlaps)/len(overlaps):.1f}%\")\n",
    "        print(f\"  Min: {min(overlaps):.1f}%\")\n",
    "        print(f\"  Max: {max(overlaps):.1f}%\")\n",
    "else:\n",
    "    print(\"Processing summary not found. Run the complete pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Training Manifest\n",
    "\n",
    "Generate a manifest for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training manifest not found. Run the organize stage first.\n"
     ]
    }
   ],
   "source": [
    "# Load training manifest\n",
    "manifest_path = test_config.output.fits_training / \"training_manifest.json\"\n",
    "\n",
    "if manifest_path.exists():\n",
    "    with open(manifest_path) as f:\n",
    "        manifest = json.load(f)\n",
    "    \n",
    "    print(f\"Total SNe in training set: {len(manifest)}\")\n",
    "    \n",
    "    # Count files\n",
    "    total_ref = sum(len(entry.get('reference_files', [])) for entry in manifest)\n",
    "    total_sci = sum(len(entry.get('science_files', [])) for entry in manifest)\n",
    "    \n",
    "    print(f\"Total reference files: {total_ref}\")\n",
    "    print(f\"Total science files: {total_sci}\")\n",
    "    print(f\"Total files: {total_ref + total_sci}\")\n",
    "    \n",
    "    # Create simplified manifest for ML training\n",
    "    ml_manifest = []\n",
    "    for entry in manifest:\n",
    "        sn_name = entry['sn_name']\n",
    "        for ref_file in entry.get('reference_files', []):\n",
    "            for sci_file in entry.get('science_files', []):\n",
    "                ml_manifest.append({\n",
    "                    'sn_name': sn_name,\n",
    "                    'reference': str(test_config.output.fits_training / ref_file),\n",
    "                    'science': str(test_config.output.fits_training / sci_file),\n",
    "                })\n",
    "    \n",
    "    # Save ML manifest\n",
    "    ml_manifest_path = test_config.output.fits_training / \"ml_training_manifest.json\"\n",
    "    with open(ml_manifest_path, 'w') as f:\n",
    "        json.dump(ml_manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ ML training manifest saved to: {ml_manifest_path}\")\n",
    "    print(f\"   Total training pairs: {len(ml_manifest)}\")\n",
    "else:\n",
    "    print(\"Training manifest not found. Run the organize stage first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After generating your dataset:\n",
    "\n",
    "1. **Quality Check**: Inspect a few difference images to verify quality\n",
    "2. **Training Data Preparation**: Generate image triplets (reference, science, difference)\n",
    "3. **Labeling**: Create ground truth labels at known SN positions\n",
    "4. **Model Training**: Train CNN classifier on the dataset\n",
    "5. **Evaluation**: Test on held-out data\n",
    "\n",
    "See the main project documentation for more details on each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Data Pipeline Documentation](../docs/research/DATA_PIPELINE.md)\n",
    "- [Configuration Files](../configs/)\n",
    "- [Pipeline Scripts](../scripts/)\n",
    "- [Midterm Notes](../docs/research/MIDTERM_NOTES.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
